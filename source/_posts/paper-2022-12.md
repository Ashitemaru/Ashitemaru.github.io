---
title: 2022 年 12 月论文笔记
date: 2022-12-06 13:17:26
category:
    - 【论文笔记】计算机
mathjax: true
---

12 月开始就得准备做毕设了，先要充分阅读文献，确定自己要做的方向有什么要解决的问题，以及目前已经有的探索和目前还存在的困难。

<!-- more -->

# Learning Tailored Adaptive Bitrate Algorithms to Heterogeneous Network Conditions - A Domain-Specific Priors and Meta-Reinforcement Learning Approach

黄老师的文章，作为起手还是需要好好看看的。

这篇文章的基本概念是，基于学习的 ABR 已经能够实现一个平均而言优秀的 ABR 算法，但是对于部分特殊的网络环境其依然表现不佳，而这也就是所谓的个性化。然后这篇文章使用了 meta RL 的方式实现了 AABR（其实应当是 $A^2BR$，但是我真的不想打公式了，AABR 表示 Adaptation of ABR）。

AABR 首先离线学习一个元算法，之后再在在线的个性化环境下学习一个针对当前网络环境的个性化算法。另外，还需要不断优化离线学习来保证元算法的优势。

## Introduction

首先对 ABR 算法做了总结，启发式算法根据当前网络情况和一部分状态变量给出决策，但是只有在选定较好的算法参数的时候这种算法才能表现较好。基于学习的算法则根据网络情况学习这些参数。然而总而言之，这些算法都不会自动调节自己的算法参数，尤其在面对变化较多的网络环境下时，这些算法表现较差。

之后是通过实验说明了现在的网络状况不仅是多样性（diverse）的，更多的是独一无二（unique）的。基于这一点，这篇文章认为做用户个性化是一个重要的提升。

所以基于 Input-driven Markov Decision Process (IMDP)，本文提出了 AABR。分为离线和在线两步，具体的训练方式见后续。

总结一下这篇文章的工作就是确认了当今网络环境的独特性，并通过 meta RL 解决了 ABR 的个性化问题。

## Background & Motivation

关于 ABR 之类的介绍这里忽略，主要看这篇文章如何论证当今网络的多样性。

多样性分为三个方向去论证，即不同用户间差异、不同场景间差异以及已有的部分 ABR 算法在这些网络环境下的表现。

用户间差异和场景间差异可以关注下图：

![](/uploads/paper-2022-12/1.png)

可以首先注意到网络吞吐率和 RTT 的变动是相当大的，而如果观察具体的用户，也可以发现用户之间的网络状况差异也很大，部分用户网络稳定（整体带宽 CDF 陡峭）而部分用户的网络波动较大，而且各个用户的网络指标平均值也差异较大。

对于场景，也可以注意到步行、地铁、公交、汽车的网络情况也各有不同，并不能一概而论。

如果从 ABR 的角度出发，这篇文章在两个不同环境下测试了若干个 ABR 算法的性能，得到的结果是：

![](/uploads/paper-2022-12/2.png)

显然可以注意到已有的算法都很难兼顾两种情况，常常顾此失彼。这就意味着不稳定的网络情况很容易导致这些算法失效，从而也难以实现用户个性化。

## Methods

### Input-driven Markov Decision Process

基本而言是在正常的 MDP 中引入了 input process $\mathcal{Z}$，这个时候环境的转移概率定义为：

$$
\mathbb{T}[s' \mid s, a, z] := \mathbb{P}[s_{t + 1} = s' \mid s_t = s, a_t = a, z_t = z]
$$

此外，决策依然仅仅和 $s_t \in \mathcal{S}$ 有关，也就是说具体行为 $a_t \in \mathcal{A}$ 与 $z_t \in \mathcal{Z}$ 是独立的。

进一步的价值函数定义为：

$$
Q(s, a, z) := \sum_{s' \in \mathcal{S}} \mathbb{T}[s' \mid s, a, z](r(s, a, z) + \gamma V(s', z'))
$$

在 IMDP 下，最优策略也就定义为：

$$
\pi^*(s, z) := \mathop{\rm argmax}_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} \mathbb{T}[s', z' \mid s, a, z]\left[r(s, a, z) + \gamma \max_{a' \in \mathcal{A}} Q(s', a', z')\right]
$$

可以注意到，如果两个 agent 运行相同的 policy $\pi$，其不同之处仅仅是 input process $\mathcal{Z}_{1, 2}$ 不同。可以注意到这两个 agent 将会始终保持相同的决策，所以其价值函数相同当且仅当 $\mathcal{Z}_1 = \mathcal{Z}_2$。

如果在完全得知 input process $\mathcal{Z}$ 的条件下，求解最优策略相当于一般的强化学习问题，可以通过大量的强化学习方法完成。但是我们现在需要假定 input process 是完全不可感知的，在这种条件下，一般强化学习方法学习到的最优策略 $\hat\pi^*$ 的价值函数实际上是与 input process 无关的，其价值函数用 $Q(s', a')$ 表示，其满足：

$$
\max_{a' \in \mathcal{A}} Q(s', a') = \mathbb{E}_{z'} \max_{a' \in \mathcal{A}} Q(s', a', z')
$$

所以现在解决 input process 未知条件下的最优策略求解问题就是最重要的。

### Meta RL with Domain Knowledge