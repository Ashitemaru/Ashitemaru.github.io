---
title: 2022 年 12 月论文笔记
date: 2022-12-06 13:17:26
category:
    - 【论文笔记】计算机
mathjax: true
---

12 月开始就得准备做毕设了，先要充分阅读文献，确定自己要做的方向有什么要解决的问题，以及目前已经有的探索和目前还存在的困难。

$$
\newcommand{\b}{\boldsymbol}
$$

<!-- more -->

# Learning Tailored Adaptive Bitrate Algorithms to Heterogeneous Network Conditions - A Domain-Specific Priors and Meta-Reinforcement Learning Approach

黄老师的文章，作为起手还是需要好好看看的。

这篇文章的基本概念是，基于学习的 ABR 已经能够实现一个平均而言优秀的 ABR 算法，但是对于部分特殊的网络环境其依然表现不佳，而这也就是所谓的个性化。然后这篇文章使用了 meta RL 的方式实现了 AABR（其实应当是 $A^2BR$，但是我真的不想打公式了，AABR 表示 Adaptation of ABR）。

AABR 首先离线学习一个元算法，之后再在在线的个性化环境下学习一个针对当前网络环境的个性化算法。另外，还需要不断优化离线学习来保证元算法的优势。

## Introduction

首先对 ABR 算法做了总结，启发式算法根据当前网络情况和一部分状态变量给出决策，但是只有在选定较好的算法参数的时候这种算法才能表现较好。基于学习的算法则根据网络情况学习这些参数。然而总而言之，这些算法都不会自动调节自己的算法参数，尤其在面对变化较多的网络环境下时，这些算法表现较差。

之后是通过实验说明了现在的网络状况不仅是多样性（diverse）的，更多的是独一无二（unique）的。基于这一点，这篇文章认为做用户个性化是一个重要的提升。

所以基于 Input-driven Markov Decision Process (IMDP)，本文提出了 AABR。分为离线和在线两步，具体的训练方式见后续。

总结一下这篇文章的工作就是确认了当今网络环境的独特性，并通过 meta RL 解决了 ABR 的个性化问题。

## Background & Motivation

关于 ABR 之类的介绍这里忽略，主要看这篇文章如何论证当今网络的多样性。

多样性分为三个方向去论证，即不同用户间差异、不同场景间差异以及已有的部分 ABR 算法在这些网络环境下的表现。

用户间差异和场景间差异可以关注下图：

![](/uploads/paper-2022-12/1.png)

可以首先注意到网络吞吐率和 RTT 的变动是相当大的，而如果观察具体的用户，也可以发现用户之间的网络状况差异也很大，部分用户网络稳定（整体带宽 CDF 陡峭）而部分用户的网络波动较大，而且各个用户的网络指标平均值也差异较大。

对于场景，也可以注意到步行、地铁、公交、汽车的网络情况也各有不同，并不能一概而论。

如果从 ABR 的角度出发，这篇文章在两个不同环境下测试了若干个 ABR 算法的性能，得到的结果是：

![](/uploads/paper-2022-12/2.png)

显然可以注意到已有的算法都很难兼顾两种情况，常常顾此失彼。这就意味着不稳定的网络情况很容易导致这些算法失效，从而也难以实现用户个性化。

## Methods

### Input-driven Markov Decision Process

基本而言是在正常的 MDP 中引入了 input process $\mathcal{Z}$，这个时候环境的转移概率定义为：

$$
\mathbb{T}[s' \mid s, a, z] := \mathbb{P}[s_{t + 1} = s' \mid s_t = s, a_t = a, z_t = z]
$$

此外，决策依然仅仅和 $s_t \in \mathcal{S}$ 有关，也就是说具体行为 $a_t \in \mathcal{A}$ 与 $z_t \in \mathcal{Z}$ 是独立的。

进一步的价值函数定义为：

$$
Q(s, a, z) := \sum_{s' \in \mathcal{S}} \mathbb{T}[s' \mid s, a, z](r(s, a, z) + \gamma V(s', z'))
$$

在 IMDP 下，最优策略也就定义为：

$$
\pi^*(s, z) := \mathop{\rm argmax}_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} \mathbb{T}[s', z' \mid s, a, z]\left[r(s, a, z) + \gamma \max_{a' \in \mathcal{A}} Q(s', a', z')\right]
$$

可以注意到，如果两个 agent 运行相同的 policy $\pi$，其不同之处仅仅是 input process $\mathcal{Z}_{1, 2}$ 不同。可以注意到这两个 agent 将会始终保持相同的决策，所以其价值函数相同当且仅当 $\mathcal{Z}_1 = \mathcal{Z}_2$。

如果在完全得知 input process $\mathcal{Z}$ 的条件下，求解最优策略相当于一般的强化学习问题，可以通过大量的强化学习方法完成。但是我们现在需要假定 input process 是完全不可感知的，在这种条件下，一般强化学习方法学习到的最优策略 $\hat\pi^*$ 的价值函数实际上是与 input process 无关的，其价值函数用 $Q(s', a')$ 表示，其满足：

$$
\max_{a' \in \mathcal{A}} Q(s', a') = \mathbb{E}_{z'} \max_{a' \in \mathcal{A}} Q(s', a', z')
$$

所以现在解决 input process 未知条件下的最优策略求解问题就是最重要的。

### Meta RL with Domain Knowledge

总之就是又阐述了一边 AABR 分为离线和在线两个训练过程，离线训练一个元算法之后在线训练一个个性化 ABR 算法。那么两个问题就是：

{% note info no-icon %}
i) How to obtain a good parameter initialization for fast-learning?

ii) How to efficiently learn tailor-made ABR algorithms online?
{% endnote %}

### Model Agnostic Meta-Learning

总之就是选定了 MAML 作为 AABR 的核心算法，MAML 的论文后面慢慢推着看。

不过这一部分也阐释了为何 MAML 适合用于这个场景，原文是：

{% note info no-icon %}
We find that model agnostic meta learning (MAML) is quite suitable in personalized ABR scenarios where the network traces on each user are quite limited.

Specifically, MAML consists of an inner loop and an outer loop.
{% endnote %}

也就是说从数据集的角度和训练流程的角度上说，MAML 和这个实验场景的契合度比较高。

### Leveraging Domain Knowledge

总体的含义是设计了一个模拟器用于生成数据以及相关的启发式算法，一方面补充了现实世界中数据量不足的问题，另外一方面其代表的启发式算法可以在 policy 无法处理某些状态的时候作为 backup 使用。

**注：**这一部分可能需要结合下面的 overview 仔细学一下，然后对这里做一些扩充，因为这里模拟器的细节应该是做数据扩充的重要部分。

## AABR Overview

系统的大致结构为：

![](/uploads/paper-2022-12/3.png)

### Basic Training Algorithm

在设计网络的输入的时候首先需要考虑的是什么才是真正需要考虑的状态，这篇文章采用的方式是首先将所有可以使用的状态都作为输入以训练一个 teacher network，之后采用类似模仿学习的方式，采用类似决策树等较为轻量级的模型去模仿神经网络的策略，之后再使用决策树剪枝的方式去获取真正有意义的非平凡状态。

{% note success no-icon %}
这个方法似乎是比较好玩的一个方法，用于筛选真正有用的输入状态。
{% endnote %}

这里剪枝的目的也是为了压缩最后的成本。

最后筛选出来的状态包括：

- 上一次选定的视频块质量 $q_t$
- 目前的缓冲区占用率 $b_t$
- 前 $k$ 个视频块传输时网络吞吐率 $C_t$
- 前 $k$ 个视频块的下载时间 $D_t$
- 前 $k$ 个视频块的应答时间 $P_t$

这里为了控制成本，实验中选取 $k = 5$。

另外，为了保证模型的泛化性，这里需要将参数正则化，这样至少能保证模型面对完全没有经历过的网络环境不至于将其认为非法输入。

模型的输出和正常的 AC 网络没有很大的差别，都是 Actor 网络输出一个 softmax 后的概率张量表示各个行为的决策概率，而 Critic 网络输出一个标量表示对当前状态的价值评估。

#### Maximum Entropy PPO (ME-PPO)

这里的训练策略是 ME-PPO，其与 PPO 的差别在于最优行为的定义里纳入了熵：

$$
a_t^* := \mathop{\rm argmax}_a \hat{\mathbb{E}}_t\left[\sum_t \gamma^t(r_t + \lambda H^{\pi_\theta}(s_t))\right]
$$

这里定义熵为：

$$
H^{\pi_\theta}(s_t) = -\sum_{a \in \mathcal{A}} \pi_\theta(a \mid s_t) \ln\pi_\theta(a \mid s_t)
$$

元学习要求在离线环境下更重视 exploration 而在线环境下更重视 exploitation。这里将熵纳入考虑，如果策略 $\pi_\theta$ 对动作空间 $\mathcal{A}$ 中的所有动作给出的概率约趋向于平均，则熵越高，从而越会支持这样的选择。

ME-PPO 的具体策略是基于 Dual-PPO 的 double clip 策略，具体如下：

$$
\begin{aligned}
\mathcal{L}^{\rm PPO} &:= \min\left(\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\rm old}}(a_t \mid s_t)}(\theta) \hat A_t, {\rm clip}\left(\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\rm old}}(a_t \mid s_t)}(\theta), 1 - \varepsilon, 1 + \varepsilon\right)\hat A_t\right) \\
\mathcal{L}^{\rm Actor} &:= \begin{cases}
\hat{\mathbb{E}}_t[\max(\mathcal{L}^{\rm PPO}, c\hat A_t)] & \hat A_t < 0 \\
\hat{\mathbb{E}}_t[\mathcal{L}^{\rm PPO}] & \hat A_t \geq 0 \\
\end{cases}
\end{aligned}
$$

这里 $\hat A_t$ 是优势函数，定义为：

$$
\hat A_t := r_t + \gamma(V^{\pi_\theta}(s_{t + 1}) + \lambda H^{\pi_\theta}(s_{t + 1})) - V^{\pi_\theta}(s_t)
$$

这里 $c, \varepsilon$ 是用来控制梯度的超参数。

AABR 中的 Critic 网络的训练策略就是最小化优势函数的均方误差：

$$
\mathcal{L}^{\rm Critic} := \frac12 \hat{\mathbb{E}}_t[A_t]^2
$$

从而整体的 loss 可以总结为：

$$
\nabla\mathcal{L}^{\rm MEPPO} := -\nabla_\theta \mathcal{L}^{\rm Actor}(\pi_\theta, \hat A_t) + \nabla_{\theta_v} \mathcal{L}^{\rm Critic}
$$

---

此外，由于算法对 $\lambda$ 较为敏感，我们需要在训练过程中不断调节 $\lambda$，调节方式较为简单：

$$
\lambda \leftarrow \lambda + \alpha[H^{\pi_\theta}(s_t) - H^{\rm Target}]
$$

这个目标在于熵逐步接近目标的时候衰减 $\lambda$ 以减缓接近速度。

### Meta-Learned Policies for Offline Stage

**注：**具体的训练方式后续补充。

# Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks

这是 MAML 训练方法的原始论文，看情况有没有时间做一个复现。

文章的主要贡献就是一个元学习方式，并且似乎泛用性很广：

{% note info no-icon %}
In this work, we propose a meta-learning algorithm that is general and model-agnostic, in the sense that it can be directly applied to any learning problem and model that is trained with a gradient descent procedure.
{% endnote %}

元学习的核心在于训练好的模型能够迅速通过少量数据学习到新的任务，并且其能够应用到大量的下游任务中。这篇文章将训练模型理解为在模型内部建立起适用于某种任务的特征表示，如果这种表示能够适合于多种任务，那么我们只需要进行合适的微调就可以迅速适应新的任务。如果从动力系统的角度理解，我们需要提高需要完成的任务的 loss 对参数的敏感度，这个敏感度越高，参数的变化对 loss 的提升的影响越大。

## Model-Agnostic Meta-Learning

### Meta-Learning Problem Set-Up

这里严格定义了元学习需要解决的问题。

首先我们需要形式化定义什么是任务，这篇文章定义为：

$$
\mathcal{T} := \{\mathcal{L}(\b x_1, \b a_1, \cdots, \b x_H, \b a_H), q(\b x_1), q(\b x_{t + 1} \mid \b x_t, \b a_t), H\}
$$

这里 $\mathcal{L}$ 表示 loss，$q(\b x_1)$ 是初始状态的分布，$q(\b x_{t + 1} \mid \b x_t, \b a_t)$ 是状态转移的概率分布，$H$ 是 episode 的长度。这个定义同时涵盖了普通的强化学习和监督学习，其中监督学习中 $H = 1$。

模型本身描述为一个函数 $f$，其接受当前状态 $\b x$ 并给出输出 $\b a$，即 $\b a = f(\b x)$。模型不断在该任务中给定 $\b a_1, \b a_2, \cdots, \b a_H$，而 loss $\mathcal{L}$ 则给出了实际的反馈，这种反馈可以是分类问题中的失配损失或者 Markov 决策过程中的负收益。

在元学习中我们需要考虑一系列的任务的分布，这里记作 $p(\mathcal{T})$。如果我们要求 $K$ shot learning，对于每一个从 $p(\mathcal{T})$ 抽取的具体任务 $\mathcal{T}_i$，我们需要再通过 $q_i$ 抽取 $K$ 个样本，并且考虑 $\mathcal{L}_{\mathcal{T}_i}$ 的反馈。

之后直接搬一个论文原句，也应该是论文后面算法设计的主要出发点：

{% note info no-icon %}
The model $f$ is then improved by considering how the *test* error on new data from $q_i$ changes with respect to the parameters.
{% endnote %}

### A Model-Agnostic Meta-Learning Algorithm

这里提供了算法设计，一个关键词就是之前提到的**敏感度**。

考虑用参数 $\theta$ 表示的模型 $f_\theta$。在学习具体的任务 $\mathcal{T}_i$ 的时候，参数的更新方式为：

$$
\theta \leftarrow \theta - \alpha\nabla_\theta\mathcal{L}_{\mathcal{T}_i}(f_\theta)
$$

为了简化记号，我们假设在元学习之后学习具体任务仅用了一次梯度更新，此时任务 $\mathcal{T}_i$ 的最优参数就是 $\theta_i' := \theta - \alpha\nabla_\theta\mathcal{L}_{\mathcal{T}_i}(f_\theta)$，那么元学习的目标是：

$$
\min_\theta \sum_{\mathcal{T}_i \in p(\mathcal{T})} \mathcal{L}_{\mathcal{T}_i}(f_{\theta_i'}) = \min_\theta \sum_{\mathcal{T}_i \in p(\mathcal{T})} \mathcal{L}_{\mathcal{T}_i}(f_{\theta - \alpha\nabla_\theta\mathcal{L}_{\mathcal{T}_i}(f_\theta)})
$$

那么还是使用简单的梯度下降，元学习中的参数更新就是：

$$
\theta \leftarrow \theta - \beta\nabla_\theta \sum_{\mathcal{T}_i \in p(\mathcal{T})} \mathcal{L}_{\mathcal{T}_i}(f_{\theta_i'})
$$

在这里我们注意到出现了二阶梯度，部分深度学习框架能够支持这一操作，但本文也同样指出作一阶近似（即舍去二阶及以上的高阶项）是可行的。

基于此，MAML 的整体框架为：

![](/uploads/paper-2022-12/4.png)

## Species of MAML

本文这里举出了监督学习和强化学习中运用 MAML 的例子，这里我们只关注强化学习。

强化学习特化的 MAML 主要关注的是 loss 的选定，这里选定的就是最经典的负收益。另外，此时 $q_{\mathcal{T}_i}$ 实际上就是环境中的状态转移概率分布：

$$
\mathcal{L}_{\mathcal{T}_i}(f_\phi) = -\mathbb{E}_{(\b x_t, \b a_t) \sim (f_\phi, q_{\mathcal{T}_i})} \left[\sum_{t = 1}^H R(\b x_t, \b a_t)\right]
$$

从而，强化学习特化的 MAML 算法为：

![](/uploads/paper-2022-12/5.png)

这之后就是一些实验结果了，感觉没啥意思了。

# Learning to Adapt in Dynamic, Real-World Environments through Meta Reinforcement Learning

## Abstract & Introduction

需要解决的问题依然是生成数据很困难以及测试环境中无法预料到的数据可能导致所训练的策略失效，为了让模型能够快速适应测试环境中的新任务