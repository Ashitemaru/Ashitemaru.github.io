---
title: 2022 年 12 月论文笔记
date: 2022-12-06 13:17:26
category:
    - 【论文笔记】计算机
mathjax: true
---

12 月开始就得准备做毕设了，先要充分阅读文献，确定自己要做的方向有什么要解决的问题，以及目前已经有的探索和目前还存在的困难。

<!-- more -->

# Learning Tailored Adaptive Bitrate Algorithms to Heterogeneous Network Conditions - A Domain-Specific Priors and Meta-Reinforcement Learning Approach

黄老师的文章，作为起手还是需要好好看看的。

这篇文章的基本概念是，基于学习的 ABR 已经能够实现一个平均而言优秀的 ABR 算法，但是对于部分特殊的网络环境其依然表现不佳，而这也就是所谓的个性化。然后这篇文章使用了 meta RL 的方式实现了 AABR（其实应当是 $A^2BR$，但是我真的不想打公式了，AABR 表示 Adaptation of ABR）。

AABR 首先离线学习一个元算法，之后再在在线的个性化环境下学习一个针对当前网络环境的个性化算法。另外，还需要不断优化离线学习来保证元算法的优势。

## Introduction

首先对 ABR 算法做了总结，启发式算法根据当前网络情况和一部分状态变量给出决策，但是只有在选定较好的算法参数的时候这种算法才能表现较好。基于学习的算法则根据网络情况学习这些参数。然而总而言之，这些算法都不会自动调节自己的算法参数，尤其在面对变化较多的网络环境下时，这些算法表现较差。

之后是通过实验说明了现在的网络状况不仅是多样性（diverse）的，更多的是独一无二（unique）的。基于这一点，这篇文章认为做用户个性化是一个重要的提升。

所以基于 Input-driven Markov Decision Process (IMDP)，本文提出了 AABR。分为离线和在线两步，具体的训练方式见后续。

总结一下这篇文章的工作就是确认了当今网络环境的独特性，并通过 meta RL 解决了 ABR 的个性化问题。

## Background & Motivation

关于 ABR 之类的介绍这里忽略，主要看这篇文章如何论证当今网络的多样性。

多样性分为三个方向去论证，即不同用户间差异、不同场景间差异以及已有的部分 ABR 算法在这些网络环境下的表现。

用户间差异和场景间差异可以关注下图：

![](/uploads/paper-2022-12/1.png)

可以首先注意到网络吞吐率和 RTT 的变动是相当大的，而如果观察具体的用户，也可以发现用户之间的网络状况差异也很大，部分用户网络稳定（整体带宽 CDF 陡峭）而部分用户的网络波动较大，而且各个用户的网络指标平均值也差异较大。

对于场景，也可以注意到步行、地铁、公交、汽车的网络情况也各有不同，并不能一概而论。

如果从 ABR 的角度出发，这篇文章在两个不同环境下测试了若干个 ABR 算法的性能，得到的结果是：

![](/uploads/paper-2022-12/2.png)

显然可以注意到已有的算法都很难兼顾两种情况，常常顾此失彼。这就意味着不稳定的网络情况很容易导致这些算法失效，从而也难以实现用户个性化。

## Methods

### Input-driven Markov Decision Process

基本而言是在正常的 MDP 中引入了 input process $\mathcal{Z}$，这个时候环境的转移概率定义为：

$$
\mathbb{T}[s' \mid s, a, z] := \mathbb{P}[s_{t + 1} = s' \mid s_t = s, a_t = a, z_t = z]
$$

此外，决策依然仅仅和 $s_t \in \mathcal{S}$ 有关，也就是说具体行为 $a_t \in \mathcal{A}$ 与 $z_t \in \mathcal{Z}$ 是独立的。

进一步的价值函数定义为：

$$
Q(s, a, z) := \sum_{s' \in \mathcal{S}} \mathbb{T}[s' \mid s, a, z](r(s, a, z) + \gamma V(s', z'))
$$

在 IMDP 下，最优策略也就定义为：

$$
\pi^*(s, z) := \mathop{\rm argmax}_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} \mathbb{T}[s', z' \mid s, a, z]\left[r(s, a, z) + \gamma \max_{a' \in \mathcal{A}} Q(s', a', z')\right]
$$

可以注意到，如果两个 agent 运行相同的 policy $\pi$，其不同之处仅仅是 input process $\mathcal{Z}_{1, 2}$ 不同。可以注意到这两个 agent 将会始终保持相同的决策，所以其价值函数相同当且仅当 $\mathcal{Z}_1 = \mathcal{Z}_2$。

如果在完全得知 input process $\mathcal{Z}$ 的条件下，求解最优策略相当于一般的强化学习问题，可以通过大量的强化学习方法完成。但是我们现在需要假定 input process 是完全不可感知的，在这种条件下，一般强化学习方法学习到的最优策略 $\hat\pi^*$ 的价值函数实际上是与 input process 无关的，其价值函数用 $Q(s', a')$ 表示，其满足：

$$
\max_{a' \in \mathcal{A}} Q(s', a') = \mathbb{E}_{z'} \max_{a' \in \mathcal{A}} Q(s', a', z')
$$

所以现在解决 input process 未知条件下的最优策略求解问题就是最重要的。

### Meta RL with Domain Knowledge

总之就是又阐述了一边 AABR 分为离线和在线两个训练过程，离线训练一个元算法之后在线训练一个个性化 ABR 算法。那么两个问题就是：

{% note info no-icon %}
i) How to obtain a good parameter initialization for fast-learning?

ii) How to efficiently learn tailor-made ABR algorithms online?
{% endnote %}

### Model Agnostic Meta-Learning

总之就是选定了 MAML 作为 AABR 的核心算法，MAML 的论文后面慢慢推着看。

不过这一部分也阐释了为何 MAML 适合用于这个场景，原文是：

{% note info no-icon %}
We find that model agnostic meta learning (MAML) is quite suitable in personalized ABR scenarios where the network traces on each user are quite limited.

Specifically, MAML consists of an inner loop and an outer loop.
{% endnote %}

也就是说从数据集的角度和训练流程的角度上说，MAML 和这个实验场景的契合度比较高。

### Leveraging Domain Knowledge

总体的含义是设计了一个模拟器用于生成数据以及相关的启发式算法，一方面补充了现实世界中数据量不足的问题，另外一方面其代表的启发式算法可以在 policy 无法处理某些状态的时候作为 backup 使用。

**注：**这一部分可能需要结合下面的 overview 仔细学一下，然后对这里做一些扩充，因为这里模拟器的细节应该是做数据扩充的重要部分。

## AABR Overview

系统的大致结构为：

![](/uploads/paper-2022-12/3.png)

### Basic Training Algorithm

在设计网络的输入的时候首先需要考虑的是什么才是真正需要考虑的状态，这篇文章采用的方式是首先将所有可以使用的状态都作为输入以训练一个 teacher network，之后采用类似模仿学习的方式，采用类似决策树等较为轻量级的模型去模仿神经网络的策略，之后再使用决策树剪枝的方式去获取真正有意义的非平凡状态。

{% note success no-icon %}
这个方法似乎是比较好玩的一个方法，用于筛选真正有用的输入状态。
{% endnote %}

这里剪枝的目的也是为了压缩最后的成本。

最后筛选出来的状态包括：

- 上一次选定的视频块质量 $q_t$
- 目前的缓冲区占用率 $b_t$
- 前 $k$ 个视频块传输时网络吞吐率 $C_t$
- 前 $k$ 个视频块的下载时间 $D_t$
- 前 $k$ 个视频块的应答时间 $P_t$

这里为了控制成本，实验中选取 $k = 5$。

另外，为了保证模型的泛化性，这里需要将参数正则化，这样至少能保证模型面对完全没有经历过的网络环境不至于将其认为非法输入。

模型的输出和正常的 AC 网络没有很大的差别，都是 Actor 网络输出一个 softmax 后的概率张量表示各个行为的决策概率，而 Critic 网络输出一个标量表示对当前状态的价值评估。

#### Maximum Entropy PPO (ME-PPO)

这里的训练策略是 ME-PPO，其与 PPO 的差别在于最优行为的定义里纳入了熵：

$$
a_t^* := \mathop{\rm argmax}_a \hat{\mathbb{E}}_t\left[\sum_t \gamma^t(r_t + \lambda H^{\pi_\theta}(s_t))\right]
$$

这里定义熵为：

$$
H^{\pi_\theta}(s_t) = -\sum_{a \in \mathcal{A}} \pi_\theta(a \mid s_t) \ln\pi_\theta(a \mid s_t)
$$

元学习要求在离线环境下更重视 exploration 而在线环境下更重视 exploitation。这里将熵纳入考虑，如果策略 $\pi_\theta$ 对动作空间 $\mathcal{A}$ 中的所有动作给出的概率约趋向于平均，则熵越高，从而越会支持这样的选择。

ME-PPO 的具体策略是基于 Dual-PPO 的 double clip 策略，具体如下：

$$
\begin{aligned}
\mathcal{L}^{\rm PPO} &:= \min\left(\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\rm old}}(a_t \mid s_t)}(\theta) \hat A_t, {\rm clip}\left(\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\rm old}}(a_t \mid s_t)}(\theta), 1 - \varepsilon, 1 + \varepsilon\right)\hat A_t\right) \\
\mathcal{L}^{\rm Actor} &:= \begin{cases}
\hat{\mathbb{E}}_t[\max(\mathcal{L}^{\rm PPO}, c\hat A_t)] & \hat A_t < 0 \\
\hat{\mathbb{E}}_t[\mathcal{L}^{\rm PPO}] & \hat A_t \geq 0 \\
\end{cases}
\end{aligned}
$$

这里 $\hat A_t$ 是优势函数，定义为：

$$
\hat A_t := r_t + \gamma(V^{\pi_\theta}(s_{t + 1}) + \lambda H^{\pi_\theta}(s_{t + 1})) - V^{\pi_\theta}(s_t)
$$

这里 $c, \varepsilon$ 是用来控制梯度的超参数。

AABR 中的 Critic 网络的训练策略就是最小化优势函数的均方误差：

$$
\mathcal{L}^{\rm Critic} := \frac12 \hat{\mathbb{E}}_t[A_t]^2
$$

从而整体的 loss 可以总结为：

$$
\nabla\mathcal{L}^{\rm MEPPO} := -\nabla_\theta \mathcal{L}^{\rm Actor}(\pi_\theta, \hat A_t) + \nabla_{\theta_v} \mathcal{L}^{\rm Critic}
$$

---

此外，由于算法对 $\lambda$ 较为敏感，我们需要在训练过程中不断调节 $\lambda$，调节方式较为简单：

$$
\lambda \leftarrow \lambda + \alpha[H^{\pi_\theta}(s_t) - H^{\rm Target}]
$$

这个目标在于熵逐步接近目标的时候衰减 $\lambda$ 以减缓接近速度。

### Meta-Learned Policies for Offline Stage

**注：**具体的训练方式后续补充。