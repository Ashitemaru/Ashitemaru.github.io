---
title: 《Neural Adaptive Video Streaming with Pensieve》论文笔记
date: 2022-07-07 14:03:15
category:
    - 【论文笔记】计算机
mathjax: true
---

打算用 PyTorch 写一遍 Pensieve，也算是尝试学学实际场景之中的强化学习的应用。不过在此之前还是好好阅读一下别人的论文看一下这个算法到底在干什么。

<!-- more -->

# Introduction

现行的 ABR 算法在选择下一个视频块的码率的时候往往面临下述四个问题：

- 网络吞吐量的不稳定
- 部分 QoE 指标内存在相互矛盾的要求（如高质量和尽可能减少缓冲）
- 当前码率选择可能对未来的选择产生影响
- ABR 的决策可能是粗粒度的

现行的大部分 ABR 方法都依赖于对网络吞吐的估计或者对缓冲区大小的估计，所以其难以应对网络吞吐的不稳定。比如说 SOTA 方法 MPC 依赖于对网络吞吐量的预估，原文：

{% note info no-icon %}
However, MPC’s performance relies on an accurate model of the system dynamics - particularly, a forecast of future network throughput.
{% endnote %}

这样的限制使得 MPC 对预测结果的偏差十分敏感。

从而本文提出了 Pensieve，该方法会自动学习 ABR 算法，而不需要任何的预训练的控制规则或者对运行环境的假设条件。Pensieve 是基于强化学习的，其完全通过其与环境交互的经验学习。Pensieve 的核心为一个神经网络，该神经网络代表了 Pensieve 所采取的针对网络环境的策略。该网络接受的输入为各种对网络环境的原始观测数据（吞吐量采样、回放缓冲区占用率、视频块尺寸等），而其输出则为下一个视频块应当采取的码率。该网络的训练方式为 A3C。

Pensieve 的训练方式的核心则是一个模拟器，用于将大规模的网络跟踪信息数据库回放给 Pensieve，其划分粒度可以是块级别的，也可以是包级别的或者其他。Pensieve 的评测方式为将算法部署在一个 ABR 服务器上，令算法不断给出下一个视频块应当采取的码率，这样做可以避免在客户端上进行繁重的神经网络计算。

最后 Pensieve 在几乎所有的网络环境和 QoE 指标上持平或超过现有最佳的 ABR 算法。

# Background

当前占主导地位的视频分发策略为 HTTP-based adaptive streaming，而其标准化技术为 DASH，即 Dynamic Adaptive Streaming over HTTP。而 DASH 的流程可以大致描述为下图：

![](/uploads/paper-4/1.png)

- 首先播放器向服务器获得认证
- 服务器应答一个保存有实际视频数据的 CDN 地址，并且列出所有可选码率
- 播放器使用 ABR 算法逐个请求视频块，视频块下载完成后渲染到屏幕上播放

这里 ABR 算法几乎决定了整体的视频传输性能，其面临的若干挑战已经在 Introduction 部分给出了基本的介绍，这里稍加详细叙述。

1. ABR 算法并不能仅考虑网络情况，因为在类似蜂窝移动网络等不稳定网络条件下，我们难以预测未来的网络吞吐，最后往往仅能够获取到保守估计导致性能浪费，或者过高估计导致传输延迟。从而 ABR 也应当考虑缓冲区占有率等更为稳定的参数。

2. QoE 指标可能是相互冲突的。在带宽有限的网络环境中持续以最高码率请求视频可以保证视频质量，但是会升高缓冲率。相反，在不稳定的网络环境中，在任何时候选取当前网络环境能提供的最高码率会导致视频播放不平滑。

3. 当前视频块的选择会对后续视频块的码率选择产生影响。其一，高码率视频块常常会导致后续视频块难以维持该高码率，因为可能导致缓冲。其二，为了保证视频播放平滑，ABR 算法不倾向于突然改变码率，所以后续视频块的码率并不会在当前码率基础上改变很大。

4. 由于 ABR 算法能选择的码率范围被服务器给出的 Manifest file 限制，所以其选择往往是粗粒度的。有时网络环境会恰好落在某一个低码率过于浪费资源而稍高的码率就会导致缓冲的临界区域。

# Learning ABR Algorithms

其首先将 Pensieve 和 Robust MPC 进行了对比，指出了 Robust MPC 的部分缺陷（基本还是先前部分提到的缺陷，只是使用了实验数据做了支持）。

这一部分似乎并没有写出很多新东西，还是先前部分的重复。

# Design

## Training Methodology

训练策略的核心是一个模拟视频下载流程的模拟器。其对于每一个视频块，模拟器会根据该视频块的码率以及网络状况生成下载耗时。在这一段时间，模拟器会缓慢消耗缓冲区内已经存在的视频块以模拟视频播放，之后再将该视频块压入缓冲区。模拟器会跟踪所有可能导致重新缓冲的事件，比如缓冲区剩余视频块长度小于正在下载的视频块需要的时间（这会导致已经缓冲的视频播放完成后新的视频块还未下载完毕）。另外一方面，如果当前缓冲区无法容纳新的视频块，那么模拟器会暂停下载 500 毫秒，之后重新开始请求。

在完成下载之后，模拟器会给出若干参数，比如当前缓冲区占有率、重新缓冲时间、视频块下载时间、（在所有可选码率下）下一个视频块的大小、该视频剩余的视频块数量等。这些参数的具体使用方式见后。

基于该模拟器，Pensieve 可以在很短的时间内学习一个本应占用长时间的视频下载流程。

虽然上述基于应用层行为的建模简单直接，但是实际上由于传输层复杂的行为，上述建模有可能并不精确。例如，播放器在缓冲区已满的条件下可能并不会在视频块下载完成后立刻请求下一个视频块，这可能导致传输层 TCP 回退到慢启动状态，这样可能并不能充分利用带宽。该问题实际上让 ABR 算法需要考虑的方面增多了，因为通常意义上快速填满缓冲区的策略会导致 TCP 慢启动从而造成事实上的低效率。

为了了解 TCP 慢启动造成的影响，设计的实验为在带宽 6 Mbps 的条件下不断以某一个恒定的码率请求视频块。得出的结论为，在启用 TCP 慢启动的条件下请求的码率越低带宽利用率越低，而不启用 TCP 慢启动的条件下所有码率均充分利用了带宽。

所以为了让模拟贴合现实，我们可以关闭服务器的 TCP 慢启动。虽然这可能导致流量的突变，但目前可以有其他的解决方案解决这一问题而不需要 Pensieve 来处理。

事实上上述模拟依然是不精确的，但是我们发现 Pensieve 依然可以通过上述不完整的模拟学习到高质量的 ABR 算法，这可能是由于 Pensieve 强大的泛用能力。

## Basic Training Algorithm

