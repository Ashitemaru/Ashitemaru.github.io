---
title: 《Reinforcement Learning - An Introduction》学习笔记
date: 2022-08-14 14:59:43
category:
    - 【学习笔记】计算机
mathjax: true
---

Sutton 编写的强化学习理论入门书，先前在学完基本的强化学习知识之后和几位学长做了一些交流，结果发现自己的理论还是有很多并不充足的部分，于是想着暑假还有一段时间，不如来深入学习一下强化学习理论，期间同时推进 Pensieve Torch 的项目。

<!-- more -->

# 强化学习引论

## 强化学习方法的定义和独特性

强化学习是研究**智能体（Agent）**如何和**环境（Environment）**交互的机器学习方式，其重要的两个特征为 **Trial-and-error Search** 与 **Delayed reward**，即我们需要智能体通过尝试来探索环境如何响应其动作，并且智能体做出的动作不仅会影响当下的收益，还可能影响后续行为的收益。

强化学习和有监督学习的差别在于，有监督学习的智能体会从一个打标签后的数据集中学习，该数据集来源于外部的全知全能的监督，我们希望有监督学习的智能体能够学习每一条数据和其标签之间的关系，以此去预测未出现在训练集中的数据的标签。有监督学习并不能在交互之中学习，因为为某一种情况打上固定的标签，并且该标签在任何条件下都表示智能体应当采取的最正确的行为，寻找这样的标签并不实际。我们需要让智能体在未知的环境中通过自己的经验学习。

强化学习虽然不依赖于监督数据，但是也并不是无监督学习，因为无监督学习的主要任务是发现无标签数据内部的结构关系。相比较于无监督学习，强化学习的目标是令智能体能够最大化收益，即使发现数据集、环境内部的结构逻辑可能辅助完成该目标。

此外，强化学习具有一个独特的特征，即强化学习方法需要处理 exploration 和 exploitation 的平衡问题。exploration 可以让智能体更为了解环境，更能确定何种行为更有利。而 exploitation 则可以让智能体根据目前的知识采取能获取最大收益的行为。不充足的 exploration 可能导致智能体无法正确判定最佳行为，不充足的 exploitation 则可能收获不到尽可能高的收益。所以强化学习方法需要平衡这两者以达到最佳的学习效果。

另外一方面，强化学习会整体地考虑问题，思考如何让一个以明确目标为导向的智能体和完全不确定的环境交互。而其他的机器学习方法往往仅仅考虑当前问题的某些子问题，并不会论述该子问题该如何嵌入最后的整体问题。

我们需要注意的是，强化学习中的智能体并不仅仅指的是机器人或者类似的独立决策个体，智能体也可以代指这些系统中的某一个部分，其通过和系统其余部分交互来间接影响这个系统所在的环境。比如机器人的电量监测模块就可以通过告知机器人当前电量状况来影响整个机器人对环境的响应。

综上所述，强化学习的基本框架大致如下：

{% note info no-icon %}
强化学习往往包含一个活跃的，和环境产生**交互**的决策智能体以及其所处的环境。环境对智能体而言是**不确定的**，而智能体需要尝试做出能够达到其既定**目标**的行动。智能体的行为会对环境产生影响，从而会影响下一步智能体的决策。所以正确的决策需要考虑到当前行为的延迟影响并且智能体需要有一定的前瞻和规划。

此外，环境对智能体的行为的响应并不能精确预测，所以智能体需要不断观测环境。但另一方面，智能体可以通过环境的响应来优化后续的行为决策。
{% endnote %}

## 强化学习的要素

具体而言，强化学习方法往往包含四个子元素，即**策略（Policy）**、**收益信号（Reward signal）**、**价值函数（Value function）**以及表示环境的**模型（Model）**，表示环境的模型可以不具有。

策略是根据当前状态决定智能体需要做出的行为的函数，其可能是简单的查找表，也可以是涉及重型计算的搜索流程。策略是强化学习方法的核心，我们的目标即为学习最优策略。另外，策略除了直接给出当前需要采取的行为之外，还可以给出智能体作出各种行为的概率分布。

收益信号则会在每次智能体做出决策后由环境给出，收益信号实际上定义了智能体行为的目标，即最大化整个流程中收益信号的总和。如果智能体在行为过程中收到了较低的收益信号，则其往往会更改其决策以尝试获得更高的收益信号。同样，收益信号也有可能是一个概率分布。

相比于作为即时收益的收益信号，价值函数会从长远的角度评价一个状态，其代指了从该状态出发在长期的行为下智能体能期望获得的总收益。价值函数是收益总和的期望，但是依然可能发生从高价值状态出发依然收获低收益信号这样的情况。我们在决策的时候一般更为重视价值函数而非即时的收益信号，因为这样能够保证我们在长远上获取更高的收益。但是决定价值函数的取值远困难于决定收益信号，因为收益信号往往可以直接通过环境获取，而价值函数需要通过智能体在整个学习过程中去探索。可以说强化学习方法的核心就是如何高效地估计价值函数。

此外，部分强化学习系统则包含用于模拟智能体所在的环境的模型，使用模型模拟环境的强化学习方法为 model-based 方法，而不使用模型，直接通过不断尝试进行学习的 model-free 方法。

## 强化学习方法示例

我们使用基本的井字棋作为例子简单讲解强化学习方法。事实上，训练一个能够在井字棋中尽可能获胜的智能体并非容易。传统方法，例如常见的优化方法需要我们完全了解对手的行为模式，并对此加以学习，但是现实问题中更常见的，是我们并不知晓对手的行为模式，这个时候需要要求智能体通过实际上去和该对手对战以获取经验，并从经验中学习。

使用强化学习方法时，我们首先需要确定状态空间 $\mathcal{S}$，该状态空间即井字棋所有可能棋子排布形式构成的集合。之后设立价值函数 $V: \mathcal{S} \to \mathbb{R}$，对于状态 $s \in \mathcal{S}$，$V(s)$ 的含义为最近的对状态 $s$ 的价值估计。

假设我们执 X 子，那么在算法开始之前，我们需要把所有三 X 子连珠的局面 $s_X$ 的价值函数 $V(s_X)$ 定义为 $1$，相应地我们需要把所有三 O 子连珠的局面 $s_O$ 的价值函数 $V(s_O)$ 定义为 $0$。而其他的所有状态 $s$，则设定 $V(s) := 0.5$，表明在无经验条件下，我们从任何中间状态开始的获胜概率估计为百分之五十。

那么我们只需要不断按照某一个行为方式对弈，并根据对弈经验修改价值函数 $V$ 的值。这里我们的修改策略为，逆向遍历转移链，对于某一个转移行为，比如说我们在 $t$ 时刻从状态 $s_t$ 转移到状态 $s_{t + 1}$，我们可以将前者的价值向后者的价值的方向前进一步：

$$
V(s_t) \leftarrow V(s_t) + \alpha[V(s_{t + 1}) - V(s_t)]
$$

这样，我们在获得一整个对局的基础上，就可以从游戏的最终情形倒推回起始，并在这一过程中逐步更新各个状态的评价。这样就实现了将 $V(s_X), V(s_O)$ 两个值传播到整个状态空间上。

公式里的 $\alpha > 0$ 一般是一个小常数，称为**学习率（Learning rate）**或者**步长参数（Step-size parameter）**。

上述策略实际上是 **Temporal-difference learning** 的一个示例，因为上述学习依赖于差值 $V(s_{t + 1}) - V(s_t)$，其是两个在时间上先后关系的状态的价值之差。

上述策略和基于评价的学习方法有相当大的差别。基于评价的学习方法的原理为，固定一个策略，不断和给定的对手对弈，那么智能体的获胜频率给出了该策略获胜概率的无偏估计，该估计会为下一次策略选取提供指导。但是这种方法仅仅会关注结果，其完全忽视对局内部的每一次行动的具体影响，相比于考虑每个状态价值的强化学习方法，基于评价的学习方法往往会给一些毫无作用甚至反作用的行为给予高评价。