---
title: 《Reinforcement Learning - An Introduction》学习笔记
date: 2022-08-14 14:59:43
category:
    - 【学习笔记】计算机
mathjax: true
---

Sutton 编写的强化学习理论入门书，先前在学完基本的强化学习知识之后和几位学长做了一些交流，结果发现自己的理论还是有很多并不充足的部分，于是想着暑假还有一段时间，不如来深入学习一下强化学习理论，期间同时推进 Pensieve Torch 的项目。

<!-- more -->

# 强化学习引论

强化学习是研究 Agent 如何和环境交互的机器学习方式，其重要的两个特征为 **Trial-and-error Search** 与 **Delayed reward**，即我们需要 Agent 通过尝试来探索环境如何响应其动作，并且 Agent 做出的动作不仅会影响当下的收益，还可能影响后续行为的收益。

强化学习和有监督学习的差别在于，有监督学习的 Agent 会从一个打标签后的数据集中学习，该数据集来源于外部的全知全能的监督，我们希望有监督学习的 Agent 能够学习每一条数据和其标签之间的关系，以此去预测未出现在训练集中的数据的标签。有监督学习并不能在交互之中学习，因为为某一种情况打上固定的标签，并且该标签在任何条件下都表示 Agent 应当采取的最正确的行为，寻找这样的标签并不实际。我们需要让 Agent 在未知的环境中通过自己的经验学习。

强化学习虽然不依赖于监督数据，但是也并不是无监督学习，因为无监督学习的主要任务是发现无标签数据内部的结构关系。相比较于无监督学习，强化学习的目标是令 Agent 能够最大化 reward signal，即使发现数据集、环境内部的结构逻辑可能辅助完成该目标。

此外，强化学习具有一个独特的特征，即强化学习方法需要处理 exploration 和 exploitation 的平衡问题。exploration 可以让 Agent 更为了解环境，更能确定何种行为更有利。而 exploitation 则可以让 Agent 根据目前的知识采取能获取最大收益的行为。不充足的 exploration 可能导致 Agent 无法正确判定最佳行为，不充足的 exploitation 则可能收获不到尽可能高的收益。所以强化学习方法需要平衡这两者以达到最佳的学习效果。

另外一方面，强化学习会整体地考虑问题，思考如何让一个以明确目标为导向的 Agent 和完全不确定的环境交互。而其他的机器学习方法往往仅仅考虑当前问题的某些子问题，并不会论述该子问题该如何嵌入最后的整体问题。

我们需要注意的是，强化学习中的 Agent 并不仅仅指的是机器人或者类似的独立决策个体，Agent 也可以代指这些系统中的某一个部分，其通过和系统其余部分交互来间接影响这个系统所在的环境。比如机器人的电量监测模块就可以通过告知机器人当前电量状况来影响整个机器人对环境的响应。

综上所述，强化学习的基本框架大致如下：

{% note info no-icon %}
强化学习往往包含一个活跃的，和环境产生**交互**的决策 Agent 以及其所处的环境。环境对 Agent 而言是**不确定的**，而 Agent 需要尝试做出能够达到其既定**目标**的行动。Agent 的行为会对环境产生影响，从而会影响下一步 Agent 的决策。所以正确的决策需要考虑到当前行为的延迟影响并且 Agent 需要有一定的前瞻和规划。

此外，环境对 Agent 的行为的响应并不能精确预测，所以 Agent 需要不断观测环境。但另一方面，Agent 可以通过环境的响应来优化后续的行为决策。
{% endnote %}

## 更具体的强化学习定义

