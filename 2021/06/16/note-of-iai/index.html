<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic%7CMS+PMincho:300,300italic,400,400italic,700,700italic%7CFira+Code:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext" referrerpolicy="no-referrer">

<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://unpkg.com/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;ashitemaru.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Muse&quot;,&quot;version&quot;:&quot;8.5.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;width&quot;:320,&quot;display&quot;:&quot;always&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:true,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:&quot;disqusjs&quot;,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;manual&quot;,&quot;top_n_per_article&quot;:1,&quot;unescape&quot;:false,&quot;preload&quot;:false}}</script><script src="/js/config.js"></script>
<meta name="description" content="这个文档就是一时兴起想要好好学学人工智能背后的玩意创建的。主要还是记录《人工智能导论》课程的笔记。">
<meta property="og:type" content="article">
<meta property="og:title" content="《人工智能导论》学习笔记">
<meta property="og:url" content="https://ashitemaru.github.io/2021/06/16/note-of-iai/index.html">
<meta property="og:site_name" content="Ashitemaru">
<meta property="og:description" content="这个文档就是一时兴起想要好好学学人工智能背后的玩意创建的。主要还是记录《人工智能导论》课程的笔记。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-06-16T20:27:16.000Z">
<meta property="article:modified_time" content="2021-06-16T20:27:16.000Z">
<meta property="article:author" content="Ashitemaru">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://ashitemaru.github.io/2021/06/16/note-of-iai/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;ashitemaru.github.io&#x2F;2021&#x2F;06&#x2F;16&#x2F;note-of-iai&#x2F;&quot;,&quot;path&quot;:&quot;2021&#x2F;06&#x2F;16&#x2F;note-of-iai&#x2F;&quot;,&quot;title&quot;:&quot;《人工智能导论》学习笔记&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>《人工智能导论》学习笔记 | Ashitemaru</title>
  



<link rel="stylesheet" href="https://www.unpkg.com/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://www.unpkg.com/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://www.unpkg.com/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
<script src="https://www.unpkg.com/twikoo@1.4.1/dist/twikoo.all.min.js"></script>
<script src="/js/jquery.min.js"></script>
<script>
function getURL(e) {
    var http = e.slice(0, 4)
    var https = e.slice(0, 5)
    if (http == "http" || https == "https") {
        return e
    } else if (e == "" || e == null || e == undefined) {
        return e
    } else {
        e = 'http://' + e
        return e
    }
}

function newComment() {
    twikoo.getRecentComments({
        envId: 'https://vercel-deploy-two.vercel.app',
        pageSize: 10,
        includeReply: false
    }).then(function (res) {
        var hotComments = $("#hot-comments");
        for (var i = 0; i < res.length; i++) {
            var nick = res[i].nick;
            var content = res[i].commentText;
            var newcontent = content.substring(0, 50);
            var url = res[i].url;
            var avatar = res[i].avatar;
            var link = getURL(res[i].link);
            var updatedAt = res[i].relativeTime;
            var commentId = '#' + res[i].id;
            hotComments.append(
                `<li class="px1 pb2 flex items-center">
                    <img style="width:40px;height:40px" class="circle mx1 listavatar" src="${avatar}">
                    <div style="display:flex;flex-direction:column;width:100%;">
                        <div style="display:flex;justify-content:space-between;flex-direction:row;align-items:center;">
                            <div class="h5 listauthor overflow-hidden" title="${nick}">
                                <a target="_blank" rel="noopener external nofollow noreferrer" href="${link}">${nick}</a>
                            </div>
                            <div class="h6 mr1 listdate wenzi hang1" style="color:#777777;">${updatedAt}</div>
                        </div>
                        <div style="display:flex;flex-direction:row;width:100%;">
                            <a class="h5 list-comcontent" style="overflow:hidden;display:flex;border-bottom:0px;text-overflow:ellipsis;line-height:1.5;text-align:left" href="${url}${commentId}">${newcontent}</a>
                        </div>
                    </div>
                </li>`
            );
        }
    }).catch(function (err) {
        console.error(err);
    });
}

function replaceRuby() {
    $('code')
        .filter((_, node) => {
            var list = $(node).text().split(' ');
            return list.length === 3 && list[0] === "@";
        })
        .replaceWith((_, text) => {
            var list = text.split(' ');
            var written = list[1];
            var read = list[2];
            return $(`<ruby>${written}<rp>(</rp><rt>${read}</rt><rp>)</rp></ruby>`);
        });
}

$(function () {
    newComment();
    replaceRuby();
});
</script>

<!-- CSS -->
<link href="/css/app.min.css" rel="stylesheet">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Ashitemaru</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">11</span></a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">40</span></a></li>
        <li class="menu-item menu-item-skill-docs"><a href="https://sast-skill-docers.github.io/sast-skill-docs/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i>技能引导文档</a></li>
        <li class="menu-item menu-item-thuinfo"><a href="https://www.thuinfo.net/" rel="noopener" target="_blank"><i class="fa fa-sitemap fa-fw"></i>THUInfo</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98"><span class="nav-number">1.</span> <span class="nav-text">搜索问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B2%E7%9B%AE%E6%90%9C%E7%B4%A2bfs-dfs"><span class="nav-number">1.1.</span> <span class="nav-text">盲目搜索（BFS &#x2F; DFS）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8F%91%E5%BC%8F%E6%90%9C%E7%B4%A2a-a"><span class="nav-number">1.2.</span> <span class="nav-text">启发式搜索（A &#x2F; A*）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%B9%E8%BF%9B-a-%E7%AE%97%E6%B3%95"><span class="nav-number">1.3.</span> <span class="nav-text">改进 A* 算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%9A%E5%BC%88%E9%97%AE%E9%A2%98%E5%AF%B9%E6%8A%97%E6%90%9C%E7%B4%A2"><span class="nav-number">2.</span> <span class="nav-text">博弈问题（对抗搜索）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#alpha-beta-%E5%89%AA%E6%9E%9D"><span class="nav-number">2.1.</span> <span class="nav-text">\(\alpha-\beta\) 剪枝</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%96%B9%E6%B3%95"><span class="nav-number">2.2.</span> <span class="nav-text">蒙特卡罗方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%AB%98%E7%BA%A7%E6%90%9C%E7%B4%A2"><span class="nav-number">3.</span> <span class="nav-text">高级搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%80%E9%83%A8%E6%90%9C%E7%B4%A2"><span class="nav-number">3.1.</span> <span class="nav-text">局部搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95"><span class="nav-number">3.2.</span> <span class="nav-text">模拟退火算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95"><span class="nav-number">3.3.</span> <span class="nav-text">遗传算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="nav-number">4.</span> <span class="nav-text">统计机器学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">4.1.</span> <span class="nav-text">基本概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%B4%E7%B4%A0-bayes-%E6%B3%95"><span class="nav-number">4.2.</span> <span class="nav-text">朴素 Bayes 法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm"><span class="nav-number">4.3.</span> <span class="nav-text">支持向量机（SVM）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-number">4.4.</span> <span class="nav-text">决策树</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="nav-number">5.</span> <span class="nav-text">神经网络与深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E7%A5%9E%E7%BB%8F%E5%85%83%E7%BB%93%E6%9E%84"><span class="nav-number">5.1.</span> <span class="nav-text">基本神经元结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95bp"><span class="nav-number">5.2.</span> <span class="nav-text">反向传播算法（BP）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">5.3.</span> <span class="nav-text">过拟合问题与正则化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Ccnn"><span class="nav-number">5.4.</span> <span class="nav-text">卷积神经网络（CNN）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn"><span class="nav-number">5.5.</span> <span class="nav-text">循环神经网络（RNN）</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ashitemaru"
      src="/uploads/avatar.png">
  <p class="site-author-name" itemprop="name">Ashitemaru</p>
  <div class="site-description" itemprop="description">A cat that likes Sakana.</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">40</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Ashitemaru" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Ashitemaru" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:qhd19@mails.tsinghua.edu.cn" title="E-Mail → mailto:qhd19@mails.tsinghua.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/Ashitemaru" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;Ashitemaru" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Friends
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.unidy.cn/" title="https:&#x2F;&#x2F;www.unidy.cn" rel="noopener" target="_blank">UNIDY</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.dilant.cn/" title="https:&#x2F;&#x2F;www.dilant.cn" rel="noopener" target="_blank">Dilant</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://zcy.moe/" title="https:&#x2F;&#x2F;zcy.moe" rel="noopener" target="_blank">猫猫</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://c7w.tech/" title="https:&#x2F;&#x2F;c7w.tech" rel="noopener" target="_blank">c7w</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.leenldk.top/" title="http:&#x2F;&#x2F;www.leenldk.top&#x2F;" rel="noopener" target="_blank">leenldk (20)</a>
        </li>
    </ul>
  </div>
<div class="sidebar-1 mybox relative">
    <div class="p2">
        <i class="fab fa-facebook-messenger mr1"></i>
        Latest Comments
    </div>
    <div id="hot-comments"></div>
</div>
          </div>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/Ashitemaru" class="github-corner" title="Come here for fun." aria-label="Come here for fun." rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ashitemaru.github.io/2021/06/16/note-of-iai/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.png">
      <meta itemprop="name" content="Ashitemaru">
      <meta itemprop="description" content="A cat that likes Sakana.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ashitemaru">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《人工智能导论》学习笔记
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-16 20:27:16" itemprop="dateCreated datePublished" datetime="2021-06-16T20:27:16+00:00">2021-06-16</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-06-16 20:27:16" itemprop="dateModified" datetime="2021-06-16T20:27:16+00:00">2021-06-16</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%AE%A1%E7%AE%97%E6%9C%BA/" itemprop="url" rel="index"><span itemprop="name">【学习笔记】计算机</span></a>
        </span>
    </span>

  
    <span id="/2021/06/16/note-of-iai/" class="post-meta-item twikoo_visitors" data-flag-title="《人工智能导论》学习笔记" title="阅读次数">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="twikoo_visitors"></span>
    </span>
  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>15k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>14 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>这个文档就是一时兴起想要好好学学人工智能背后的玩意创建的。主要还是记录《人工智能导论》课程的笔记。</p>
<span id="more"></span>
<p>大概随着这个文档越来越饱满，我应该就能看懂<code>paper.md</code>这个诡异的超纲文档了。</p>
<h2 id="搜索问题">搜索问题</h2>
<h3 id="盲目搜索bfs-dfs">盲目搜索（BFS / DFS）</h3>
<p>DFS 优先扩展深度深的节点，BFS 优先扩展深度浅的节点。由于 DFS 往往存在深度限制，所以 DFS 是有可能找不到最优解的，并且最坏情况下 DFS 等价于枚举。而 BFS 在<strong>单位耗散值的有解问题</strong>上一定能找到最优解。</p>
<p>但有的时候搜索树上的相邻节点之间距离并非一致，所以出现了 Dijkstra 算法。这个算法是最短路算法之中较为重要的一个。在 Dijkstra 算法之中，原则是优先扩展<strong>距离起点最近的节点</strong>。</p>
<p>Dijkstra 算法考虑了当前节点与起点的距离，但是实则没有对距离终点的距离进行评估。所以之后可以引入启发式搜索。</p>
<h3 id="启发式搜索a-a">启发式搜索（A / A*）</h3>
<p>所谓启发式，就是引入<strong>启发知识</strong>，也就是对当前节点与目标之间的距离的评估。</p>
<p>在 A 算法之中，我们引入对节点 <span class="math inline">\(n\)</span> 的<strong>评估函数</strong> <span class="math inline">\(f(n)\)</span>：</p>
<p><span class="math display">\[
f(n) = g(n) + h(n)
\]</span></p>
<p>这里 <span class="math inline">\(h(n)\)</span> 就是对当前节点与目标之间距离的评估，也被称为<strong>启发函数</strong>。</p>
<p>上述函数均为<strong>估计值</strong>。我们把相应的实际值（也就是最短路所对应的）分别标记为 <span class="math inline">\(f^*(n), g^*(n), h^*(n)\)</span>。</p>
<p>A 算法的核心为，优先扩展 <span class="math inline">\(f(n)\)</span> 最小的节点。</p>
<p>在算法的具体实现过程之中，定义了<code>OPEN</code>表以及<code>CLOSE</code>表，分别代表<strong>当前考虑扩展的节点</strong>以及<strong>当前暂时不考虑扩展的节点</strong>。一份伪代码实现为：</p>
<figure class="highlight python"><figcaption><span>Python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">OPEN = [<span class="string">&#x27;s&#x27;</span>] <span class="comment"># &#x27;s&#x27; is the starting node</span></span><br><span class="line">CLOSE = []</span><br><span class="line"><span class="keyword">while</span> <span class="keyword">not</span> OPEN.empty():</span><br><span class="line">    <span class="comment"># Select the node with minimum f value</span></span><br><span class="line">    n = OPEN.node_with_min_f()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># n is the expected node</span></span><br><span class="line">    <span class="keyword">if</span> expected(n):</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    </span><br><span class="line">    OPEN.remove(n)</span><br><span class="line">    CLOSE.add(n)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Traverse the neighbors</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> n.neighbor():</span><br><span class="line">        path_len = g(n) + distance(n, m)</span><br><span class="line">        <span class="comment"># Expand the list</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">not</span> m <span class="keyword">in</span> OPEN) <span class="keyword">and</span> (<span class="keyword">not</span> m <span class="keyword">in</span> CLOSE):</span><br><span class="line">            OPEN.add(m)</span><br><span class="line">            n.<span class="built_in">next</span> = m</span><br><span class="line">        <span class="comment"># m has been explored before, but it needs update</span></span><br><span class="line">        <span class="keyword">elif</span> (m <span class="keyword">in</span> OPEN) <span class="keyword">and</span> (path_len &lt; g(m)):</span><br><span class="line">            g(m) = path_len</span><br><span class="line">            n.<span class="built_in">next</span> = m</span><br><span class="line">        <span class="comment"># m has been discarded, but it can be reused now</span></span><br><span class="line">        <span class="keyword">elif</span> (m <span class="keyword">in</span> CLOSE) <span class="keyword">and</span> (path_len &lt; g(m)):</span><br><span class="line">            CLOSE.remove(m)</span><br><span class="line">            OPEN.add(m)</span><br><span class="line">            n.<span class="built_in">next</span> = m</span><br><span class="line"><span class="comment"># Fail</span></span><br><span class="line"><span class="keyword">return</span> FAIL</span><br></pre></td></tr></table></figure>
<p>在 A 算法之中，如果满足：</p>
<p><span class="math display">\[
h(n) \leq h^*(n)
\]</span></p>
<p>那么 A 算法就称为 A* 算法。A* 算法的特点就在于<strong>只要初始节点到目标节点有路径，那么算法永远能找到最优解</strong>，而 A 算法并不保证有最优解。</p>
<p>另外给出一个定理：</p>
<blockquote>
<p>对于两个 A* 算法 <span class="math inline">\(A_1, A_2\)</span>，如果对于非目标节点均满足：</p>
<p><span class="math display">\[
h_2(n) &gt; h_1(n)
\]</span></p>
<p>那么 <span class="math inline">\(A_1\)</span> 所扩展的节点数不小于 <span class="math inline">\(A_2\)</span> 所扩展的节点数。</p>
</blockquote>
<h3 id="改进-a-算法">改进 A* 算法</h3>
<p>A* 算法面临的一个问题在于<strong>可能会多次扩展同一个节点</strong>，这就导致了算法的低效，而实际上多次扩展某一个节点的原因在于扩展该节点所使用的路径并非是最短的。</p>
<p>我们可以认为可以使用<strong>单调的</strong>启发函数来解决这一问题，启发函数的单调性指的是三角形法则：</p>
<p><span class="math display">\[
\begin{cases}
    h(n_i) - h(n_j) \leq {\rm distance}(n_i, n_j) \\
    h(t) = 0
\end{cases}
\]</span></p>
<p>这里 <span class="math inline">\(n_j\)</span> 为 <span class="math inline">\(n_i\)</span> 的子节点。</p>
<p>我们可以证明如果 <span class="math inline">\(h\)</span> 是单调的，那么使用这个启发函数的 A* 算法一旦扩展到了某一个节点 <span class="math inline">\(n\)</span>，就已经找到了从起点到这个节点的最短路，也就是说 <span class="math inline">\(g(n) = g^*(n)\)</span>。</p>
<p>实际上还有一个结论，就是满足单调性的 <span class="math inline">\(h\)</span> 必然满足 A* 条件。</p>
<p>基于此改进 A* 算法如下（伪代码表示）：</p>
<figure class="highlight python"><figcaption><span>Python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">OPEN = [<span class="string">&#x27;s&#x27;</span>] <span class="comment"># &#x27;s&#x27; is the starting node</span></span><br><span class="line">CLOSE = []</span><br><span class="line">f_max = <span class="number">0</span> <span class="comment"># The maximum f value that has been found till now</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">not</span> OPEN.empty():</span><br><span class="line">    <span class="comment"># Find the nodes in OPEN with f value less than f_max</span></span><br><span class="line">    NEST = OPEN.with_f_less_than(f_max)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Select the node</span></span><br><span class="line">    <span class="comment"># When the f value has been small enough to be selected into NEST</span></span><br><span class="line">    <span class="comment"># We just need to select the minimum g value now</span></span><br><span class="line">    n = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> NEST.empty():</span><br><span class="line">        n = OPEN.node_with_min_f()</span><br><span class="line">        f_max = f(n)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        n = NEST.node_with_min_g()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># n is the expected node</span></span><br><span class="line">    <span class="keyword">if</span> expected(n):</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    </span><br><span class="line">    OPEN.remove(n)</span><br><span class="line">    CLOSE.add(n)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Traverse the neighbors</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> n.neighbor():</span><br><span class="line">        path_len = g(n) + distance(n, m)</span><br><span class="line">        <span class="comment"># Expand the list</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">not</span> m <span class="keyword">in</span> OPEN) <span class="keyword">and</span> (<span class="keyword">not</span> m <span class="keyword">in</span> CLOSE):</span><br><span class="line">            OPEN.add(m)</span><br><span class="line">            n.<span class="built_in">next</span> = m</span><br><span class="line">        <span class="comment"># m has been explored before, but it needs update</span></span><br><span class="line">        <span class="keyword">elif</span> (m <span class="keyword">in</span> OPEN) <span class="keyword">and</span> (path_len &lt; g(m)):</span><br><span class="line">            g(m) = path_len</span><br><span class="line">            n.<span class="built_in">next</span> = m</span><br><span class="line">        <span class="comment"># m has been discarded, but it can be reused now</span></span><br><span class="line">        <span class="keyword">elif</span> (m <span class="keyword">in</span> CLOSE) <span class="keyword">and</span> (path_len &lt; g(m)):</span><br><span class="line">            CLOSE.remove(m)</span><br><span class="line">            OPEN.add(m)</span><br><span class="line">            n.<span class="built_in">next</span> = m</span><br><span class="line"><span class="comment"># Fail</span></span><br><span class="line"><span class="keyword">return</span> FAIL</span><br></pre></td></tr></table></figure>
<h2 id="博弈问题对抗搜索">博弈问题（对抗搜索）</h2>
<h3 id="alpha-beta-剪枝"><span class="math inline">\(\alpha-\beta\)</span> 剪枝</h3>
<p>我们在面临双方博弈，需要我方决策的时候，我们可以先搜索一遍所有可能的双方情况并使用专家知识对局面进行评估。由于对方有利就是我方不利，所以在搜索到最底部回溯得到估值的时候，要注意并不是始终取孩子节点的最大值，而是按层数交替取最大值和最小值。</p>
<p>一般而言，根节点要取最大的儿子节点（一般用方形节点代表<strong>极大过程</strong>），下一层要取最小的儿子节点（一般用圆形节点代表<strong>极小过程</strong>），以此类推。</p>
<p>所谓剪枝，就是目前遍历到此，发现继续遍历子节点都不会更新这个节点的估值了，此时就可以剪枝。注意，判定是否剪枝的时候是要和<strong>当前节点的所有祖先</strong>都进行比较的，不能只比较父节点。</p>
<p>不过要注意，在最后给出根节点估值后确定最后决策的时候，<strong>只能沿着决策树走一步</strong>。这是因为你做出决策后决策树可能会改变形态，不能保证后面的决策的估值情况还保证如此。</p>
<h3 id="蒙特卡罗方法">蒙特卡罗方法</h3>
<p><span class="math inline">\(\alpha-\beta\)</span> 剪枝依赖于大量的专家知识，所以在实际应用上也有所局限。其基本的思路是在给定的时限之内不断通过模拟对弈的方式扩展搜索树，最后再给定走步方式。</p>
<p>基本的循环包括以下几个部分：</p>
<ul>
<li>根据前期评估结果选取最有希望的叶子节点</li>
<li>在该叶子节点处随机进行一步，扩展出新叶子节点</li>
<li>从新叶子节点开始随机模拟对弈直到判定出胜负</li>
<li>根据模拟的结果反向更新决策树上各个节点的评估值</li>
</ul>
<p>一般而言，节点的评估值包含两个方面：</p>
<ul>
<li>当前信息下节点是有希望的</li>
<li>深度较浅的节点具有更大的探索可能</li>
</ul>
<p>一般而言可以选择这样的评估方式：</p>
<p><span class="math display">\[
I_j = \overline{X_j} + c\sqrt{\frac{2\ln n}{T_j(n)}}
\]</span></p>
<p>这里 <span class="math inline">\(\overline{X_j}\)</span> 表示经过当前节点的决策路径的胜率，这是一个守成的选项。</p>
<p><span class="math inline">\(n\)</span> 表示当前搜索总次数，<span class="math inline">\(T_j(n)\)</span> 表示当前节点访问次数。</p>
<h2 id="高级搜索">高级搜索</h2>
<h3 id="局部搜索">局部搜索</h3>
<p>局部搜索的意思就是一直往更好的地方走，具体过程为：</p>
<ul>
<li>选择初始点 <span class="math inline">\(x_0\)</span>，以及其相邻可考察点集 <span class="math inline">\(P\)</span></li>
<li>只要不满足退出条件，进行下面循环：
<ul>
<li>选择 <span class="math inline">\(P&#39; \subset P\)</span>，取 <span class="math inline">\(P&#39;\)</span> 之中最优解 <span class="math inline">\(x_b\)</span></li>
<li>如果 <span class="math inline">\(x_b\)</span> 更优，则切换为考察 <span class="math inline">\(x_b\)</span>，<span class="math inline">\(P\)</span> 修改为 <span class="math inline">\(x_b\)</span> 的相邻考察点集</li>
<li>否则 <span class="math inline">\(P = P - P&#39;\)</span></li>
</ul></li>
<li>输出结果</li>
</ul>
<h3 id="模拟退火算法">模拟退火算法</h3>
<p>模拟退火实际上是对局部搜索的一个优化。其基本原理来自于液态金属凝固的时候如果降温过快则有可能由于分子没有充分时间排列为结晶态而导致凝固不工整（没有达到最低能状态），而实际工艺会让温度缓慢下降，并且有回升温度的可能，保证大概率凝固为结晶态。</p>
<p>迁移到局部搜索上，为了防止落入局部最小值，我们可以让“温度回升”是一个有概率发生的事情。对于评估函数 <span class="math inline">\(E(i)\)</span>，以及两个状态 <span class="math inline">\(p,q\)</span>，给出从 <span class="math inline">\(p\)</span> 迁移到 <span class="math inline">\(q\)</span> 的概率：</p>
<p><span class="math display">\[
P(p \rightarrow q) = \begin{cases}
1 &amp; E(q) \leq E(p) \\
\exp\left(\dfrac{E(p) - E(q)}{kT}\right) &amp; E(q) &gt; E(p) \\
\end{cases}
\]</span></p>
<p>这里 <span class="math inline">\(k\)</span> 是一个常量，<span class="math inline">\(T\)</span> 是“温度”，指的是优化问题之中的控制参数。</p>
<p>根据物理定律（Boltzmann 分布）实际上我们可以给出定律：</p>
<ul>
<li>同一温度下，物体处于低能量状态的概率高于高能量状态</li>
<li>温度无限高的时候，物体等概率处于任何状态</li>
<li>温度无限低的时候，物体等概率处于任何最低能状态</li>
<li>温度下降的时候，物体进入低能量状态概率上升，进入高能量状态概率下降</li>
</ul>
<p>所以我们使用模拟退火方法的时候需要尽可能保证：</p>
<ul>
<li>初始能量足够高</li>
<li>每个温度下状态交换足够充分</li>
<li>温度的下降足够缓慢</li>
</ul>
<p>所以算法的基本思路就是：</p>
<ul>
<li>选定初始状态</li>
<li>随机选定初始状态的某一个相邻状态，考察它和初始状态的评估函数差：
<ul>
<li>如果新状态更优，直接迁移</li>
<li>如果原状态更优，则按照概率迁移</li>
</ul></li>
<li>迁移完毕后降温，重复上述迁移步骤直到寻找到满意的状态</li>
</ul>
<p>这里有些细节的问题需要考量。</p>
<p>首先是<strong>什么是满意的状态</strong>，一般而言最简单的就是设定温度阈值或者降温次数阈值，另外一个是如果多次降温都没有能够让评估函数的变化超过阈值就可以认为稳定了。</p>
<p>其次是<strong>什么时机降温</strong>，上述算法框架之中每次迁移完毕就会降温，但实际上有的时候会进行若干次迁移后才会降温。一般而言我们可以设定迁移次数阈值。</p>
<p>之后是<strong>怎么降温</strong>。等比例降温是最简单的，另外还有一个较为常用的：</p>
<p><span class="math display">\[
t_{k + 1} = \frac{t_k}{1 + \frac{t_k\ln(1 + \delta)}{3\sigma_{t_k}}}
\]</span></p>
<p>最后是<strong>初始温度如何设定</strong>。一般而言较高就可以了。</p>
<h3 id="遗传算法">遗传算法</h3>
<p>遗传算法的本质就是模拟生物进化的过程，通过引入交叉、变异等干扰因素尝试在若干代迭代后获得满意的结果。不过我们首先关注最为重要的<strong>选优</strong>过程的模拟。</p>
<p>考虑一个包含 <span class="math inline">\(N\)</span> 个个体的群体，其中第 <span class="math inline">\(i\)</span> 个个体的适应值为 <span class="math inline">\(F(x_i)\)</span>，那么在进化过程中其被选中的概率为：</p>
<p><span class="math display">\[
p(x_i) = \frac{F(x_i)}{\sum_{j = 1}^N F(x_j)}
\]</span></p>
<p>之后就可以模拟选优，过程为：</p>
<ul>
<li>从 <span class="math inline">\(x_1\)</span> 开始，以 <span class="math inline">\(p(x_1)\)</span> 的概率选择 <span class="math inline">\(x_1\)</span>。若选中，结束模拟，否则继续</li>
<li>转到 <span class="math inline">\(x_2\)</span>，以 <span class="math inline">\(p(x_1) + p(x_2)\)</span> 的概率选择 <span class="math inline">\(x_2\)</span>。若选中，结束模拟，否则继续</li>
<li>......</li>
<li>结束模拟</li>
</ul>
<p>这是选出一个染色体的方式，下面介绍如何选出一个群体：</p>
<p>对于每一个个体 <span class="math inline">\(x_i\)</span>，我们用上述方法在 <span class="math inline">\(N\)</span> 个个体之中选取 <span class="math inline">\(\lfloor p(x_i)N\rfloor\)</span> 次。之后按照 <span class="math inline">\(p(x_i)N - \lfloor p(x_i)N\rfloor\)</span> 从大到小排序群体，再取若干个让选出的群体恰好有 <span class="math inline">\(N\)</span> 个个体。</p>
<p>这种方法就是模拟了群体的一次进化，高适应的个体就有高可能得到繁殖（被多次取到）。</p>
<p>另外，交叉和变异则是对表示状态的二进制（或者十进制）数字串进行一定处理。</p>
<p>从而就有了遗传算法的基本框架：</p>
<ul>
<li>给定群体规模 <span class="math inline">\(N\)</span>，交叉概率 <span class="math inline">\(p_c\)</span> 以及变异概率 <span class="math inline">\(p_m\)</span></li>
<li>随机生成 <span class="math inline">\(N\)</span> 个染色体作为初始群体，并计算适应值</li>
<li>下面不断重复循环直到找到合适的解：
<ul>
<li>选优出 <span class="math inline">\(N\)</span> 个染色体成为扩展群体</li>
<li>按照 <span class="math inline">\(p_c,p_m\)</span> 进行交叉和变异，未变化的染色体保留，形成新群体</li>
</ul></li>
<li>选取整个进化过程中最适应的染色体作为最后输出</li>
</ul>
<p>不过我们注意，我们一定要<strong>选取合适的编码方式</strong>，否则可能会因为问题的状态难以描述而导致编码串有过多无用位，从而导致无效变异。</p>
<p>二进制的交叉和变异是简单的，十进制的交叉可以如此：</p>
<ul>
<li>子代 1 的交叉位之后的基因从父代 2 的所有基因之中按顺序取出尚未在子代 1 之中基因</li>
<li>指定若干的欠缺位，让父代 2 这些位置留空，之后按照父代1的顺序把原来的数字填回去形成子代 2</li>
<li>指定一个一一对应的映射，以生成子代</li>
</ul>
<p>变异则可以：</p>
<ul>
<li>指定两个位置，将后面的数字移到前面的指定位置之前</li>
<li>指定两个位置，交换两个位置上的基因</li>
<li>随意打乱某一区间。</li>
</ul>
<h2 id="统计机器学习">统计机器学习</h2>
<h3 id="基本概念">基本概念</h3>
<p>现实中的预测问题都可以归纳为在函数空间 <span class="math inline">\(H = \{f \mid f: X \rightarrow Y\}\)</span> 之中寻找最优的预测函数。这里 <span class="math inline">\(X\)</span> 是输入集，其中的元素一般表示预测的基础（已知信息）。而 <span class="math inline">\(Y\)</span> 是输出集，其中的元素一般表示需要预测的结果。</p>
<p>我们假设 <span class="math inline">\(f \in H\)</span> 为最优函数，也就是我们要寻找的。一般这个函数不会已知，但我们可以知道若干对输入输出组 <span class="math inline">\((x_i, y_i)\)</span> 满足 <span class="math inline">\(f(x_i) = y_i \pm {\rm noise}\)</span>（<span class="math inline">\({\rm noise}\)</span> 指的是可能出现的数据扰动）。这些输入输出组构成<strong>训练集</strong>。</p>
<p>统计机器学习的目标就是根据训练集，按照某种算法尝试找一个 <span class="math inline">\(g \in H\)</span>，让 <span class="math inline">\(g\)</span> 尽可能表现类似 <span class="math inline">\(f\)</span>。</p>
<p>统计机器学习可以按照监督的介入分为：</p>
<ul>
<li>监督学习</li>
<li>无监督学习</li>
<li>半 / 弱监督学习</li>
</ul>
<p>监督学习事实上更贴近我们上述的统计机器学习定义，其又可以被分为<strong>回归</strong>（如线性回归、二次回归）以及<strong>分类</strong>。而无监督学习实际上常常被称为<strong>聚类</strong>。</p>
<p>有无监督的差别可以理解为训练集之中的输入输出对 <span class="math inline">\((x_i, y_i)\)</span> 的 <span class="math inline">\(y_i\)</span> 是否明确。如果 <span class="math inline">\(y_i\)</span> 明确，那么这个学习就有监督，否则无监督。</p>
<p>下面解释为何无监督又被称为聚类。由于 <span class="math inline">\(y_i\)</span> 是不给定的，所以我们不能很明确给出一个已知局面具体属于何种类别，但我们对于给定的两个已知局面，我们可以判断其相似性（断定两者是否大概率属于同一类别），进而将类似的输入输出对聚合在一类里面，不类似的尽可能分开，这就是<strong>聚类（clustering）</strong>。所以尽管学习目标都是寻找 <span class="math inline">\(g\)</span> 让模型的预测尽可能准确，但是两者的学习方式是不同的。</p>
<p>之后我们给出两个概念，就是<strong>过拟合</strong>以及<strong>泛化能力</strong>。</p>
<p>过拟合是机器学习要处理的一个重要问题，其描述的就是算法给出了一个相当复杂的函数 <span class="math inline">\(g\)</span>，其在训练集上能够相当准确率地和 <span class="math inline">\(f\)</span> 类似，但是在其余输入上却不够好。泛化能力描述的是这个模型在面对未知输入的时候是否能够做出合理输出的能力。</p>
<h3 id="朴素-bayes-法">朴素 Bayes 法</h3>
<p>这个方法应用在多分类问题上。我们可以认为输入空间为若干 <span class="math inline">\(n\)</span> 维向量的集合 <span class="math inline">\(\boldsymbol X \subset \mathbb R^n\)</span>，输出空间则是若干类别（标签）的集合 <span class="math inline">\(\boldsymbol Y = \{c_1, c_2, \cdots, c_k\}\)</span>。所以说输入 <span class="math inline">\(X\)</span> 是 <span class="math inline">\(\boldsymbol X\)</span> 上的随机变量，输出 <span class="math inline">\(Y\)</span> 是 <span class="math inline">\(\boldsymbol Y\)</span> 上的随机变量。考虑 <span class="math inline">\(X, Y\)</span> 的联合分布 <span class="math inline">\(f(x, y)\)</span>。</p>
<p>我们得出：</p>
<p><span class="math display">\[
P(Y = c_k \mid X = x) = \frac{
    P(X = x \mid Y = c_k)P(Y = c_k)
} {
    \sum_k P(X = x \mid Y = c_k)P(Y = c_k)
}
\]</span></p>
<p>这个概率实际上是在给定输入 <span class="math inline">\(X = x\)</span> 的时候，输出 <span class="math inline">\(Y = c_k\)</span> 的后验概率。所有的先验概率来源于训练集，也就是用频率近似概率。</p>
<p>我们确定最后的输出的方式是寻找令后验概率最大的 <span class="math inline">\(c_k\)</span>。由于上述分母保持常数，所以输出：</p>
<p><span class="math display">\[
y = \arg\max_{c_k} P(X = x \mid Y = c_k)P(Y = c_k)
\]</span></p>
<p>但是我们这里要注意到先验概率之中的 <span class="math inline">\(P(X = x \mid Y = c_k)\)</span> 项，其复杂度随着 <span class="math inline">\(n\)</span> 的上升而指数上升。但是我们可以假设 <span class="math inline">\(X\)</span> 的各个维度是独立的，所以我们可以得到：</p>
<p><span class="math display">\[
\begin{aligned}
P(X = x \mid Y = c_k) &amp;= P\left(X^{(1)} = x^{(1)}, X^{(2)} = x^{(2)}, \cdots, X^{(n)} = x^{(n)} \ \middle| \  Y = c_k\right) \\
&amp;= \prod_{j = 1}^n P\left(X^{(j)} = x^{(j)} \ \middle| \ Y = c_k\right)
\end{aligned}
\]</span></p>
<p>最终给出的 <strong>Bayes 分类器</strong>为： <span class="math display">\[
y = \arg\max_{c_k} P(Y = c_k)\prod_{j = 1}^n P\left(X^{(j)} = x^{(j)} \ \middle| \ Y = c_k\right)
\]</span></p>
<p>这里给出<strong>平滑</strong>的概念。也就是如果训练集中如果没有出现过某一个 case，这个时候 case 的频率为 <span class="math inline">\(0\)</span>，但是显然我们不能把概率估计为 <span class="math inline">\(0\)</span>，这个时候会引入平滑。这里给出一个例子：</p>
<p><span class="math display">\[
P_\lambda\left(X^{(j)} = a_{jl} \ \middle| \ Y = c_k\right) = \frac{
    \sum_{i = 1}^N {\rm idx}\left(x_i^{(j)} = a_{jl}, y_i = c_k\right) + \lambda
} {
    \sum_{i = 1}^N {\rm idx}(y_i = c_k) + S_j\lambda
}
\]</span></p>
<p><span class="math inline">\({\rm idx}\)</span> 函数为<strong>示性函数</strong>，其参数为布尔表达式，为真的时候函数返回 <span class="math inline">\(1\)</span>，否则返回 <span class="math inline">\(0\)</span>。</p>
<p>这里 <span class="math inline">\(S_j\)</span> 为 <span class="math inline">\(x^{(j)}\)</span>（输入第 <span class="math inline">\(j\)</span> 维）的取值集合 <span class="math inline">\(\{a_{j1}, a_{j2}, \cdots, a_{jS_j}\}\)</span> 的大小。</p>
<p>这里 <span class="math inline">\(\lambda\)</span> 是平滑系数，一般取 <span class="math inline">\(\lambda = 1\)</span>，此时平滑成为 <strong>Laplace 平滑</strong>。</p>
<h3 id="支持向量机svm">支持向量机（SVM）</h3>
<p>SVM 是一个适用于二分类问题的计算模型。</p>
<p>给定线性可分训练集 <span class="math inline">\(T = \{(\boldsymbol x_1, y_1), (\boldsymbol x_2, y_2), \cdots, (\boldsymbol x_N, y_N)\}\)</span>，这里 <span class="math inline">\(\boldsymbol x_i \in X = \mathbb R^n\)</span> 以及 <span class="math inline">\(y_i \in Y = \{1, -1\}\)</span>。这里我们称 <span class="math inline">\(1\)</span> 为正类，<span class="math inline">\(-1\)</span> 为负类。</p>
<p>我们希望寻找一个超平面 <span class="math inline">\(\boldsymbol w^{*T}\boldsymbol x + b^* = 0\)</span>，给定决策函数：</p>
<p><span class="math display">\[
f(\boldsymbol x) = {\rm sgn}(\boldsymbol w^{*T}\boldsymbol x + b^*)
\]</span></p>
<p>这就是<strong>线性可分支持向量机</strong>。</p>
<p>为了评估一个超平面 <span class="math inline">\(\boldsymbol w^T\boldsymbol x + b = 0\)</span>，我们给定 <span class="math inline">\(T\)</span> 之中的一个样本点 <span class="math inline">\((x_i, y_i)\)</span>，定义<strong>函数间隔</strong>：</p>
<p><span class="math display">\[
\hat\gamma_i = y_i(\boldsymbol w^T\boldsymbol x_i + b)
\]</span></p>
<p>定义<strong>几何间隔</strong>：</p>
<p><span class="math display">\[
\gamma_i = y_i\left(\frac{\boldsymbol w^T}{\|\boldsymbol w\|}\boldsymbol x_i + \frac{b}{\|\boldsymbol w\|}\right)
\]</span></p>
<p>上述定义是针对单个样本点的，所以对于整个训练集 <span class="math inline">\(T\)</span>，定义：</p>
<p><span class="math display">\[
\begin{cases}
    \hat\gamma = \min_i \hat\gamma_i \\
    \gamma = \min_i \gamma_i \\
\end{cases}
\]</span></p>
<p>这也就是训练集和超平面的函数间隔以及几何间隔。这两种间隔之间相差 <span class="math inline">\(\|\boldsymbol w\|\)</span> 倍。</p>
<p>我们选择超平面的标准就是<strong>最大化超平面和训练集的间隔</strong>，也就是求取 <span class="math inline">\(\max_{\boldsymbol w, b}\gamma\)</span>。由于 <span class="math inline">\(\boldsymbol w, b\)</span> 可以成比例缩放，所以说我们完全可以假设 <span class="math inline">\(\hat\gamma = 1\)</span>，从而最优化问题转化为最大化 <span class="math inline">\(1 / \|\boldsymbol w\|\)</span>，等价于最小化 <span class="math inline">\(1 / 2\|\boldsymbol w\|^2\)</span>。所以说问题就是求解 <span class="math inline">\(\min_{\boldsymbol w, b} 1 / 2\|\boldsymbol w\|^2\)</span>。</p>
<p>这里由于 <span class="math inline">\(\hat\gamma = 1\)</span>，所以说总是存在 <span class="math inline">\((\boldsymbol x_i, y_i)\)</span> 满足 <span class="math inline">\(y_i(\boldsymbol w^T\boldsymbol x_i + b) = \hat\gamma = 1\)</span>，这个向量就是<strong>支持向量</strong>。</p>
<p>SVM 的学习过程，可以先定义 Lagrange 函数：</p>
<p><span class="math display">\[
L(\boldsymbol w, b, \boldsymbol\alpha) = \frac{1}{2}\|\boldsymbol w\|^2 + \sum_{i = 1}^N \alpha_i[1 - y_i(\boldsymbol w^T\boldsymbol x_i + b)]
\]</span></p>
<p>这里 <span class="math inline">\(\alpha_i \geq 0\)</span>，并且 <span class="math inline">\(\boldsymbol\alpha = (\alpha_1, \alpha_2, \cdots, \alpha_N)^T\)</span> 为 Lagrange 乘子向量。</p>
<p>我们知道：</p>
<p><span class="math display">\[
\max_\boldsymbol\alpha L(\boldsymbol w, b, \boldsymbol\alpha) =
\begin{cases}
    \dfrac{1}{2}\|\boldsymbol w\|^2 &amp; {\rm when\ some\ requirements\ are\ satisfied} \\
    \infty &amp; {\rm otherwise}
\end{cases}
\]</span></p>
<p>所以说 <span class="math inline">\(\min_{\boldsymbol w, b}\max_\alpha L(\boldsymbol w, b, \boldsymbol\alpha)\)</span> 与原问题等价。</p>
<p>另外，我们断定：</p>
<p><span class="math display">\[
\min_{\boldsymbol w, b} L(\boldsymbol w, b, \boldsymbol\alpha) \leq L(\boldsymbol w, b, \boldsymbol\alpha) \leq \max_\boldsymbol\alpha L(\boldsymbol w, b, \boldsymbol\alpha)
\]</span></p>
<p>所以我们有：</p>
<p><span class="math display">\[
\max_\boldsymbol\alpha\min_{\boldsymbol w, b} L(\boldsymbol w, b, \boldsymbol\alpha) \leq \min_{\boldsymbol w, b}\max_\boldsymbol\alpha L(\boldsymbol w, b, \boldsymbol\alpha)
\]</span></p>
<p>这个等号成立的条件为 <strong>KKT 条件</strong>。所以我们将问题转化为求 <span class="math inline">\(\max_\boldsymbol\alpha\min_{\boldsymbol w, b} L(\boldsymbol w, b, \boldsymbol\alpha)\)</span>。</p>
<p>我们令 <span class="math inline">\(L(\boldsymbol w, b, \boldsymbol\alpha)\)</span> 对 <span class="math inline">\(\boldsymbol w, b\)</span> 偏导为 <span class="math inline">\(0\)</span> 并代入就将问题转化为：</p>
<p><span class="math display">\[
\max_\boldsymbol\alpha\left[-\frac{1}{2}\sum_{i = 1}^N \sum_{j = 1}^N \alpha_i\alpha_jy_iy_j(\boldsymbol x_i^T\boldsymbol x_j) + \sum_{i = 1}^N \alpha_i\right]
\]</span></p>
<p>这里的约束条件为：</p>
<p><span class="math display">\[
\sum_{i = 1}^N \alpha_iy_i = 0\ (\alpha_i \geq 0)
\]</span></p>
<p>我们据此获得 <span class="math inline">\(\alpha^*\)</span>，从而我们可以计算：</p>
<p><span class="math display">\[
\begin{cases}
    \boldsymbol w^* = \sum_{i = 1}^N \alpha_i^*y_i\boldsymbol x_i \\
    b^* = y_j - \sum_{i = 1}^N \alpha_i^*y_i(\boldsymbol x_i^T\boldsymbol x_j)\ (\alpha_j^* \neq 0)
\end{cases}
\]</span></p>
<p>但是有的时候这些点并不能被线性超平面完全分隔，所以说 <span class="math inline">\(y_i(\boldsymbol w^T\boldsymbol x_i + b) \geq 1\)</span> 并不能处处满足，所以需要引入<strong>松弛变量</strong>：</p>
<p><span class="math display">\[
y_i(\boldsymbol w^T\boldsymbol x_i + b) \geq 1 - \xi_i
\]</span></p>
<p>为了尽量减小 <span class="math inline">\(\xi_i\)</span> 的影响，所以我们可以把优化目标改为：</p>
<p><span class="math display">\[
\min_{\boldsymbol w, b, \xi}\left(\frac{1}{2}\|\boldsymbol w\|^2 + C\sum_{i = 1}^N \xi_i\right)
\]</span></p>
<p>这种处理方式称为<strong>软间隔最大化</strong>，这里 <span class="math inline">\(C &gt; 0\)</span> 为惩罚参数，<span class="math inline">\(C\)</span> 越大惩罚力度越大。</p>
<p>按照线性可分 SVM 的方法，问题转化为：</p>
<p><span class="math display">\[
\max_\alpha\left[-\frac{1}{2}\sum_{i = 1}^N \sum_{j = 1}^N \alpha_i\alpha_jy_iy_j(\boldsymbol x_i^T\boldsymbol x_j) + \sum_{i = 1}^N \alpha_i\right]
\]</span></p>
<p>这里的约束条件为：</p>
<p><span class="math display">\[
\sum_{i = 1}^N \alpha_iy_i = 0\ (0 \leq \alpha_i \leq C)
\]</span></p>
<p>我们据此获得 <span class="math inline">\(\alpha^*\)</span>，从而我们可以计算：</p>
<p><span class="math display">\[
\begin{cases}
    \boldsymbol w^* = \sum_{i = 1}^N \alpha_i^*y_i\boldsymbol x_i \\
    b^* = y_j - \sum_{i = 1}^N \alpha_i^*y_i(\boldsymbol x_i^T\boldsymbol x_j)\ (0 &lt; \alpha_j^* &lt; C)
\end{cases}
\]</span></p>
<p><span class="math inline">\(\alpha_i^* &gt; 0\)</span> 对应的 <span class="math inline">\(\boldsymbol x_i\)</span> 是支持向量。</p>
<ul>
<li>若 <span class="math inline">\(\alpha_i^* = 0\)</span>，则不是支持向量（对应曲线外侧的点）</li>
<li>若 <span class="math inline">\(0 &lt; \alpha_i^* &lt; C\)</span>，则 <span class="math inline">\(\xi_i = 0\)</span>，那么 <span class="math inline">\(\boldsymbol x_i\)</span> 在间隔边界上，是支持向量（对应两条虚线上的点）</li>
<li>若 <span class="math inline">\(\alpha_i^* = C\)</span>，那么 <span class="math inline">\(\boldsymbol x_i\)</span> 也是支持向量
<ul>
<li>若 <span class="math inline">\(0 &lt; \xi_i &lt; 1\)</span>，则分类正确（对应在己方虚线和实线之间的点）</li>
<li>若 <span class="math inline">\(\xi_i = 1\)</span>，则在超平面上（对应在实线上的点）</li>
<li>若 <span class="math inline">\(\xi_i &gt; 1\)</span>，则被误分（对应在实线和对方虚线之间的点）</li>
</ul></li>
</ul>
<p>这里给出一个结论，也就是样本点到软间隔边界（虚线边界）的距离为 <span class="math inline">\(\xi_i / \|\boldsymbol w\|\)</span>。</p>
<p>另外一方面，我们考虑使用非线性的方式分割数据点。事实上就是尝试建立一个非线性映射将原空间的数据点映射到新空间上，这些数据点在新空间上线性可分。</p>
<p>考虑使用映射 <span class="math inline">\(\phi: X \rightarrow H\)</span> 将输入 <span class="math inline">\(\boldsymbol x \in X\)</span> 映射到新空间 <span class="math inline">\(H\)</span> 上，这里把 <span class="math inline">\(K(\boldsymbol x, \boldsymbol y) := \phi(\boldsymbol x)^T\phi(\boldsymbol y)\)</span> 称为<strong>核函数</strong>。这个时候所有的与内积相关的运算均应该使用核函数进行。</p>
<p>按照线性可分 SVM 的方法，问题转化为：</p>
<p><span class="math display">\[
\max_\alpha\left[-\frac{1}{2}\sum_{i = 1}^N \sum_{j = 1}^N \alpha_i\alpha_jy_iy_jK(\boldsymbol x_i, \boldsymbol x_j) + \sum_{i = 1}^N \alpha_i\right]
\]</span></p>
<p>这里的约束条件为：</p>
<p><span class="math display">\[
\sum_{i = 1}^N \alpha_iy_i = 0\ (0 \leq \alpha_i \leq C)
\]</span></p>
<p>我们据此获得 <span class="math inline">\(\alpha^*\)</span>，从而我们可以计算：</p>
<p><span class="math display">\[
b^* = y_j - \sum_{i = 1}^N \alpha_i^*y_iK(\boldsymbol x_i, \boldsymbol x_j)\ (0 &lt; \alpha_j^* &lt; C)
\]</span></p>
<p>原有的决策函数之中含有内积，所以也需要将其修改为：</p>
<p><span class="math display">\[
f(\boldsymbol x) = {\rm sgn}\left(\sum_{i = 1}^N \alpha_i^*y_iK(\boldsymbol x_i, \boldsymbol x) + b^*\right)
\]</span></p>
<p>常用的核函数包括<strong>多项式核函数</strong>：</p>
<p><span class="math display">\[
K(\boldsymbol x, \boldsymbol y) = (\boldsymbol x^T\boldsymbol y + 1)^p
\]</span></p>
<p>以及<strong>高斯核函数</strong>：</p>
<p><span class="math display">\[
K(\boldsymbol x, \boldsymbol y) = \exp\left(-\frac{\|\boldsymbol x - \boldsymbol y\|^2}{2\sigma^2}\right)
\]</span></p>
<h3 id="决策树">决策树</h3>
<p>我们定义随机变量 <span class="math inline">\(X\)</span> 的熵为：</p>
<p><span class="math display">\[
H(X) = -\sum_{i = 1}^n p_i\log p_i
\]</span></p>
<p>当概率由训练集 <span class="math inline">\(D\)</span> 给出，该熵可以标记为 <span class="math inline">\(H(D)\)</span>。</p>
<p>之后给出条件熵：</p>
<p><span class="math display">\[
H(Y \mid X) = \sum_{i = 1}^n P(X = x_i)H(Y \mid X = x_i)
\]</span></p>
<p>这表示的是已经知道 <span class="math inline">\(X\)</span> 的时候 <span class="math inline">\(Y\)</span> 的不确定性。</p>
<p>之后定义特征 <span class="math inline">\(A\)</span> 对数据集 <span class="math inline">\(D\)</span> 的<strong>信息增益</strong>为：</p>
<p><span class="math display">\[
g(D, A) = H(D) - H(D \mid A)
\]</span></p>
<p>这实际上表示的是给定特征 <span class="math inline">\(A\)</span> 的时候数据集 <span class="math inline">\(D\)</span> 不确定性减少的程度。</p>
<p>假设有训练集 <span class="math inline">\(D\)</span>，有 <span class="math inline">\(K\)</span> 个类 <span class="math inline">\(C_k\)</span>，特征 <span class="math inline">\(A\)</span> 有 <span class="math inline">\(n\)</span> 个取值 <span class="math inline">\(a_i\)</span>，特征 <span class="math inline">\(A\)</span> 的不同取值将 <span class="math inline">\(D\)</span> 划分为 <span class="math inline">\(n\)</span> 个子集 <span class="math inline">\(D_i\)</span>，记 <span class="math inline">\(D_i\)</span> 中属于 <span class="math inline">\(C_k\)</span> 类的样本集合为 <span class="math inline">\(D_{ik}\)</span>，那么：</p>
<p><span class="math display">\[
\begin{aligned}
    g(D, A) &amp;= H(D) - H(D \mid A) \\
    &amp;= -\sum_{k = 1}^K \frac{|C_k|}{|D|}\log\frac{|C_k|}{|D|} + \sum_{i = 1}^n \frac{|D_i|}{|D|}\left(\sum_{k = 1}^K \frac{|D_{ik}|}{|D_i|}\log\frac{|D_{ik}|}{|D_i|}\right)
\end{aligned}
\]</span></p>
<p>下面介绍 ID3 算法，这个算法可以用于生成决策树：</p>
<ul>
<li>如果 <span class="math inline">\(D\)</span> 之中所有例子属于同一类或者没有用于判断的特征，则返回单节点树，类标记为实例数最多的类</li>
<li>选择信息增益最大的特征 <span class="math inline">\(A_g\)</span>，其信息增益为 <span class="math inline">\(\delta_A\)</span></li>
<li>如果 <span class="math inline">\(\delta_A\)</span> 小于阈值 <span class="math inline">\(\varepsilon\)</span>，那么置单节点决策树，类标记为实例数最多的类</li>
<li>否则按照特征取值分割训练集，如果某个分割后的块为空，那么构建单节点子树，类标记为 <span class="math inline">\(D\)</span> 实例数最多的类。若非空则构建子树，递归上述过程</li>
</ul>
<p>这样的算法倾向于选择分支比较多的属性。</p>
<p>所以定义<strong>信息增益比</strong>：</p>
<p><span class="math display">\[
g_R(D, A) = \frac{g(D, A)}{H_A(D)}
\]</span></p>
<p>将 ID3 算法之中的信息增益换成信息增益比则得到 C4.5 算法。</p>
<p>这种方法生成的决策树可能产生过拟合，所以需要一定程度上的剪枝。剪枝的基本流程在于找到一个父节点，剪取其下一代后将其作为新的叶子节点，其类型标记为其原来对应的子树中量最大的类。</p>
<p>在数据量足够大的时候，可以使用验证集进行剪枝，也就是不断使用验证集剪枝直到性能下降。如果数据量不够，则使用训练集，从下往上缩节点，直到损失函数回升。损失函数定义：</p>
<p><span class="math display">\[
C_a(T) = \sum_{t = 1}^{|T|}N_tH_t(T) + a|T|
\]</span></p>
<p>这里经验熵定义为：</p>
<p><span class="math display">\[
H_t(T) = -\sum_k \frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}
\]</span></p>
<p>这里某一个节点 <span class="math inline">\(t\)</span> 的样本数为 <span class="math inline">\(N_t\)</span>，其中 <span class="math inline">\(k\)</span> 类的样本数为 <span class="math inline">\(N_{tk}\)</span>。</p>
<h2 id="神经网络与深度学习">神经网络与深度学习</h2>
<h3 id="基本神经元结构">基本神经元结构</h3>
<p>一个神经网络中的神经元一般接受多个输出并产生一个输出，一般而言其数学表达为：</p>
<p><span class="math display">\[
y = g\left(\sum_{i = 1}^n w_ix_i + b\right)
\]</span></p>
<p>这里函数 <span class="math inline">\(g\)</span> 一般是非线性的，称为<strong>激活函数</strong>。激活函数常见的有 sigmoid 等。</p>
<h3 id="反向传播算法bp">反向传播算法（BP）</h3>
<p>训练一个多层神经网络可以分为两步，首先是正向从输入计算出输出，和标准输出对比，算出损失函数（衡量实际输出和标准输出的差别），这是<strong>正向传播</strong>。之后就是根据差别的大小，计算出网络中各个权重对最终输出的偏导数，从而更新权重值，这是<strong>反向传播</strong>。</p>
<p>一般而言我们常常使用均方误差评价输出，也就是说对于某一层神经元以及某一个样本 <span class="math inline">\(d\)</span>，标记其中第 <span class="math inline">\(j\)</span> 个神经元的实际输出为 <span class="math inline">\(o_j\)</span>，理想输出为 <span class="math inline">\(t_j\)</span>，那么损失函数为：</p>
<p><span class="math display">\[
E_d(\boldsymbol w) = \frac{1}{2}\sum_j (t_j - o_j)^2
\]</span></p>
<p>根据损失函数，权重 <span class="math inline">\(w_{ji}\)</span>（第 <span class="math inline">\(j\)</span> 个神经元对第 <span class="math inline">\(i\)</span> 个输入的权重）的更新量为：</p>
<p><span class="math display">\[
\Delta w_{ji} = -\eta\frac{\partial E_d}{\partial w_{ji}}
\]</span></p>
<p>这里 <span class="math inline">\(0 &lt; \eta &lt; 1\)</span> 称为<strong>学习率</strong>。</p>
<p>我们设置一个中间量：</p>
<p><span class="math display">\[
n_j := \sum_iw_{ji}x_{ji} + b_j
\]</span></p>
<p>也就是未激活的神经元输出。那么我们知道：</p>
<p><span class="math display">\[
\frac{\partial E_d}{\partial w_{ji}} = \frac{\partial E_d}{\partial n_j}\frac{\partial n_j}{\partial w_{ji}} = \frac{\partial E_d}{\partial n_j}x_{ji}
\]</span></p>
<p>下面我们对输出层和隐含层分别计算损失函数对未激活输出的偏导数。</p>
<p>如果该层为输出层，那么我们进一步展开：</p>
<p><span class="math display">\[
\frac{\partial E_d}{\partial n_j} = \frac{\partial E_d}{\partial o_j}\frac{\partial o_j}{\partial n_j} = \frac{\partial}{\partial o_j}\left(\frac{1}{2}\sum_k(t_k - o_k)^2\right)\frac{\partial \sigma(n_j)}{\partial n_j} = -(t_j - o_j)o_j(1-o_j)
\]</span></p>
<p>这里默认激活函数为 sigmoid 函数。</p>
<p>如果该层为隐藏层，我们记其下游层的神经元构成集合 <span class="math inline">\(D\)</span>。作如下展开：</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial E_d}{\partial n_j} &amp;= \sum_{k \in D}\frac{\partial E_d}{\partial n_k}\frac{\partial n_k}{\partial o_j}\frac{\partial o_j}{\partial n_j} \\
&amp;= \sum_{k \in D}\frac{\partial E_d}{\partial n_k}\frac{\partial}{\partial o_j}\left(\sum_l w_{kj}o_j + b\right)\frac{\partial\sigma(n_j)}{\partial n_j} \\
&amp;= \sum_{k \in D}\frac{\partial E_d}{\partial n_k}w_{kj}o_j(1 - o_j) \\
&amp;= o_j(1 - o_j)\sum_{k \in D}\frac{\partial E_d}{\partial n_k}w_{kj}
\end{aligned}
\]</span></p>
<p>这里还有一项偏导数实则可以继续按照上述的方式递推计算，直到计算到输出层。</p>
<p>这里额外提一下，均方误差只是一个选择，在分类问题中，更常用的是<strong>交叉熵误差</strong>：</p>
<p><span class="math display">\[
H_\boldsymbol t(\boldsymbol o) = -\sum_i t_i\log(o_i)
\]</span></p>
<h3 id="过拟合问题与正则化">过拟合问题与正则化</h3>
<p>过拟合问题在之前有过说明，在深度学习之中减少过拟合的一个方法就是在损失函数之中加入正则项：</p>
<p><span class="math display">\[
E_d(\boldsymbol w) = \frac{1}{2}\sum_k(t_k - o_k)^2 + \|\boldsymbol w\|
\]</span></p>
<p>另外还有引入 Dropout 以及引入验证集的方式，这里均不展开。</p>
<h3 id="卷积神经网络cnn">卷积神经网络（CNN）</h3>
<p><code>TODO</code></p>
<h3 id="循环神经网络rnn">循环神经网络（RNN）</h3>
<p><code>TODO</code></p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/06/16/hello-world/" rel="prev" title="Hello world">
                  <i class="fa fa-chevron-left"></i> Hello world
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/06/18/ja-1/" rel="next" title="日语单词变形手册">
                  日语单词变形手册 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments"><div id="twikoo-comments"></div></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ashitemaru</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">299k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">4:31</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://unpkg.com/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{&quot;enable&quot;:true,&quot;theme&quot;:&quot;default&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;unpkg.com&#x2F;mermaid@8.9.3&#x2F;dist&#x2F;mermaid.min.js&quot;,&quot;integrity&quot;:&quot;sha256-OyJHvRcZHaRR6Ig73ppxF4QXk8HzvfgTprRWkulCkfY&#x3D;&quot;}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;unpkg.com&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;,&quot;integrity&quot;:&quot;sha256-ncNI9OXOS5Ek4tzVYiOMmN&#x2F;KKCPZ6V0Cpv2P&#x2F;zHntiA&#x3D;&quot;}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="twikoo" type="application/json">{&quot;enable&quot;:true,&quot;visitor&quot;:true,&quot;envId&quot;:&quot;https:&#x2F;&#x2F;vercel-deploy-two.vercel.app&quot;,&quot;el&quot;:&quot;#twikoo-comments&quot;}</script>
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.twikoo.el)
    .then(() => NexT.utils.getScript(
      'https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js',
      { condition: window.twikoo }
    ))
    .then(() => {
      twikoo.init(CONFIG.twikoo);
    });
});
</script>

</body>
</html>
