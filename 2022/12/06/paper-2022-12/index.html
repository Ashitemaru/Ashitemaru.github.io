<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic%7CNoto+Serif+KR:300,300italic,400,400italic,700,700italic%7CMS+PMincho:300,300italic,400,400italic,700,700italic%7CFira+Code:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext" referrerpolicy="no-referrer">

<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://unpkg.com/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;ashitemaru.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Muse&quot;,&quot;version&quot;:&quot;8.5.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;width&quot;:320,&quot;display&quot;:&quot;always&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:true,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:&quot;disqusjs&quot;,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;manual&quot;,&quot;top_n_per_article&quot;:1,&quot;unescape&quot;:false,&quot;preload&quot;:false}}</script><script src="/js/config.js"></script>
<meta name="description" content="12 月开始就得准备做毕设了，先要充分阅读文献，确定自己要做的方向有什么要解决的问题，以及目前已经有的探索和目前还存在的困难。 \[ \newcommand{\b}{\boldsymbol} \]">
<meta property="og:type" content="article">
<meta property="og:title" content="2022 年 12 月论文笔记">
<meta property="og:url" content="https://ashitemaru.github.io/2022/12/06/paper-2022-12/index.html">
<meta property="og:site_name" content="Ashitemaru">
<meta property="og:description" content="12 月开始就得准备做毕设了，先要充分阅读文献，确定自己要做的方向有什么要解决的问题，以及目前已经有的探索和目前还存在的困难。 \[ \newcommand{\b}{\boldsymbol} \]">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/paper-2022-12/1.png">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/paper-2022-12/2.png">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/paper-2022-12/3.png">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/paper-2022-12/4.png">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/paper-2022-12/5.png">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/paper-2022-12/6.png">
<meta property="article:published_time" content="2022-12-06T13:17:26.000Z">
<meta property="article:modified_time" content="2022-12-06T13:17:26.000Z">
<meta property="article:author" content="Ashitemaru">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ashitemaru.github.io/uploads/paper-2022-12/1.png">


<link rel="canonical" href="https://ashitemaru.github.io/2022/12/06/paper-2022-12/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;ashitemaru.github.io&#x2F;2022&#x2F;12&#x2F;06&#x2F;paper-2022-12&#x2F;&quot;,&quot;path&quot;:&quot;2022&#x2F;12&#x2F;06&#x2F;paper-2022-12&#x2F;&quot;,&quot;title&quot;:&quot;2022 年 12 月论文笔记&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>2022 年 12 月论文笔记 | Ashitemaru</title>
  



<link rel="stylesheet" href="https://www.unpkg.com/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://www.unpkg.com/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://www.unpkg.com/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
<script src="https://www.unpkg.com/twikoo@1.4.1/dist/twikoo.all.min.js"></script>
<script src="/js/jquery.min.js"></script>
<script>
function getURL(e) {
    var http = e.slice(0, 4)
    var https = e.slice(0, 5)
    if (http == "http" || https == "https") {
        return e
    } else if (e == "" || e == null || e == undefined) {
        return e
    } else {
        e = 'http://' + e
        return e
    }
}

function newComment() {
    twikoo.getRecentComments({
        envId: 'https://vercel-deploy-two.vercel.app',
        pageSize: 10,
        includeReply: false
    }).then(function (res) {
        var hotComments = $("#hot-comments");
        for (var i = 0; i < res.length; i++) {
            var nick = res[i].nick;
            var content = res[i].commentText;
            var newcontent = content.substring(0, 50);
            var url = res[i].url;
            var avatar = res[i].avatar;
            var link = getURL(res[i].link);
            var updatedAt = res[i].relativeTime;
            var commentId = '#' + res[i].id;
            hotComments.append(
                `<li class="px1 pb2 flex items-center">
                    <img style="width:40px;height:40px" class="circle mx1 listavatar" src="${avatar}">
                    <div style="display:flex;flex-direction:column;width:100%;">
                        <div style="display:flex;justify-content:space-between;flex-direction:row;align-items:center;">
                            <div class="h5 listauthor overflow-hidden" title="${nick}">
                                <a target="_blank" rel="noopener external nofollow noreferrer" href="${link}">${nick}</a>
                            </div>
                            <div class="h6 mr1 listdate wenzi hang1" style="color:#777777;">${updatedAt}</div>
                        </div>
                        <div style="display:flex;flex-direction:row;width:100%;">
                            <a class="h5 list-comcontent" style="overflow:hidden;display:flex;border-bottom:0px;text-overflow:ellipsis;line-height:1.5;text-align:left" href="${url}${commentId}">${newcontent}</a>
                        </div>
                    </div>
                </li>`
            );
        }
    }).catch(function (err) {
        console.error(err);
    });
}

function replaceRuby() {
    $('code')
        .filter((_, node) => {
            var list = $(node).text().split(' ');
            return list.length === 3 && list[0] === "@";
        })
        .replaceWith((_, text) => {
            var list = text.split(' ');
            var written = list[1];
            var read = list[2];
            return $(`<ruby>${written}<rp>(</rp><rt>${read}</rt><rp>)</rp></ruby>`);
        });
}

$(function () {
    newComment();
    replaceRuby();
});
</script>

<!-- CSS -->
<link href="/css/app.min.css" rel="stylesheet">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Ashitemaru</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">12</span></a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">49</span></a></li>
        <li class="menu-item menu-item-skill-docs"><a href="https://sast-skill-docers.github.io/sast-skill-docs/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i>技能引导文档</a></li>
        <li class="menu-item menu-item-thuinfo"><a href="https://www.thuinfo.net/" rel="noopener" target="_blank"><i class="fa fa-sitemap fa-fw"></i>THUInfo</a></li>
        <li class="menu-item menu-item-se-index"><a href="https://thuse-course.github.io/course-index/" rel="noopener" target="_blank"><i class="fa fa-calendar fa-fw"></i>软工课程主页</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#learning-tailored-adaptive-bitrate-algorithms-to-heterogeneous-network-conditions---a-domain-specific-priors-and-meta-reinforcement-learning-approach"><span class="nav-number">1.</span> <span class="nav-text">Learning Tailored Adaptive Bitrate Algorithms to Heterogeneous Network Conditions - A Domain-Specific Priors and Meta-Reinforcement Learning Approach</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction"><span class="nav-number">1.1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#background-motivation"><span class="nav-number">1.2.</span> <span class="nav-text">Background &amp; Motivation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#methods"><span class="nav-number">1.3.</span> <span class="nav-text">Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#input-driven-markov-decision-process"><span class="nav-number">1.3.1.</span> <span class="nav-text">Input-driven Markov Decision Process</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#meta-rl-with-domain-knowledge"><span class="nav-number">1.3.2.</span> <span class="nav-text">Meta RL with Domain Knowledge</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model-agnostic-meta-learning"><span class="nav-number">1.3.3.</span> <span class="nav-text">Model Agnostic Meta-Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#leveraging-domain-knowledge"><span class="nav-number">1.3.4.</span> <span class="nav-text">Leveraging Domain Knowledge</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#aabr-overview"><span class="nav-number">1.4.</span> <span class="nav-text">AABR Overview</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#basic-training-algorithm"><span class="nav-number">1.4.1.</span> <span class="nav-text">Basic Training Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#maximum-entropy-ppo-me-ppo"><span class="nav-number">1.4.1.1.</span> <span class="nav-text">Maximum Entropy PPO (ME-PPO)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#meta-learned-policies-for-offline-stage"><span class="nav-number">1.4.2.</span> <span class="nav-text">Meta-Learned Policies for Offline Stage</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks"><span class="nav-number">2.</span> <span class="nav-text">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#model-agnostic-meta-learning-1"><span class="nav-number">2.1.</span> <span class="nav-text">Model-Agnostic Meta-Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#meta-learning-problem-set-up"><span class="nav-number">2.1.1.</span> <span class="nav-text">Meta-Learning Problem Set-Up</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#a-model-agnostic-meta-learning-algorithm"><span class="nav-number">2.1.2.</span> <span class="nav-text">A Model-Agnostic Meta-Learning Algorithm</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#species-of-maml"><span class="nav-number">2.2.</span> <span class="nav-text">Species of MAML</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#learning-to-adapt-in-dynamic-real-world-environments-through-meta-reinforcement-learning"><span class="nav-number">3.</span> <span class="nav-text">Learning to Adapt in Dynamic, Real-World Environments through Meta Reinforcement Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#abstract-introduction"><span class="nav-number">3.1.</span> <span class="nav-text">Abstract &amp; Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#related-works"><span class="nav-number">3.2.</span> <span class="nav-text">Related Works</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#preliminaries"><span class="nav-number">3.3.</span> <span class="nav-text">Preliminaries</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#model-based-reinforcement-learning"><span class="nav-number">3.3.1.</span> <span class="nav-text">Model-Based Reinforcement Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#meta-learning"><span class="nav-number">3.3.2.</span> <span class="nav-text">Meta Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#gradient-based-meta-learning"><span class="nav-number">3.3.2.1.</span> <span class="nav-text">Gradient Based Meta Learning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#recurrence-based-meta-learning"><span class="nav-number">3.3.2.2.</span> <span class="nav-text">Recurrence Based Meta Learning</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#meta-learning-for-online-model-adaptation"><span class="nav-number">3.4.</span> <span class="nav-text">Meta Learning for Online Model Adaptation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#model-based-meta-reinforcement-learning"><span class="nav-number">3.5.</span> <span class="nav-text">Model Based Meta Reinforcement Learning</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ashitemaru"
      src="/uploads/avatar.png">
  <p class="site-author-name" itemprop="name">Ashitemaru</p>
  <div class="site-description" itemprop="description">人は自分の全部を把握することない。</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">49</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Ashitemaru" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Ashitemaru" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:qhd19@mails.tsinghua.edu.cn" title="E-Mail → mailto:qhd19@mails.tsinghua.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/Ashitemaru" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;Ashitemaru" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Friends
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.unidy.cn/" title="https:&#x2F;&#x2F;www.unidy.cn" rel="noopener" target="_blank">UNIDY</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.dilant.cn/" title="https:&#x2F;&#x2F;www.dilant.cn" rel="noopener" target="_blank">Dilant</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://zcy.moe/" title="https:&#x2F;&#x2F;zcy.moe" rel="noopener" target="_blank">猫猫</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://c7w.tech/" title="https:&#x2F;&#x2F;c7w.tech" rel="noopener" target="_blank">c7w</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.leenldk.top/" title="http:&#x2F;&#x2F;www.leenldk.top&#x2F;" rel="noopener" target="_blank">leenldk (20)</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://sun80449.github.io/" title="https:&#x2F;&#x2F;sun80449.github.io" rel="noopener" target="_blank">lcr</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://pwe.cat/" title="https:&#x2F;&#x2F;pwe.cat" rel="noopener" target="_blank">索尔</a>
        </li>
    </ul>
  </div>
<div class="sidebar-1 mybox relative">
    <div class="p2">
        <i class="fab fa-facebook-messenger mr1"></i>
        Latest Comments
    </div>
    <div id="hot-comments"></div>
</div>
          </div>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/Ashitemaru" class="github-corner" title="Come here for fun." aria-label="Come here for fun." rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ashitemaru.github.io/2022/12/06/paper-2022-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.png">
      <meta itemprop="name" content="Ashitemaru">
      <meta itemprop="description" content="人は自分の全部を把握することない。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ashitemaru">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          2022 年 12 月论文笔记
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-12-06 13:17:26" itemprop="dateCreated datePublished" datetime="2022-12-06T13:17:26+00:00">2022-12-06</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2022-12-06 13:17:26" itemprop="dateModified" datetime="2022-12-06T13:17:26+00:00">2022-12-06</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91%E8%AE%A1%E7%AE%97%E6%9C%BA/" itemprop="url" rel="index"><span itemprop="name">【论文笔记】计算机</span></a>
        </span>
    </span>

  
    <span id="/2022/12/06/paper-2022-12/" class="post-meta-item twikoo_visitors" data-flag-title="2022 年 12 月论文笔记" title="阅读次数">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="twikoo_visitors"></span>
    </span>
  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>12 月开始就得准备做毕设了，先要充分阅读文献，确定自己要做的方向有什么要解决的问题，以及目前已经有的探索和目前还存在的困难。</p>
<p><span class="math display">\[
\newcommand{\b}{\boldsymbol}
\]</span></p>
<span id="more"></span>
<h1 id="learning-tailored-adaptive-bitrate-algorithms-to-heterogeneous-network-conditions---a-domain-specific-priors-and-meta-reinforcement-learning-approach">Learning Tailored Adaptive Bitrate Algorithms to Heterogeneous Network Conditions - A Domain-Specific Priors and Meta-Reinforcement Learning Approach</h1>
<p>黄老师的文章，作为起手还是需要好好看看的。</p>
<p>这篇文章的基本概念是，基于学习的 ABR 已经能够实现一个平均而言优秀的 ABR 算法，但是对于部分特殊的网络环境其依然表现不佳，而这也就是所谓的个性化。然后这篇文章使用了 meta RL 的方式实现了 AABR（其实应当是 <span class="math inline">\(A^2BR\)</span>，但是我真的不想打公式了，AABR 表示 Adaptation of ABR）。</p>
<p>AABR 首先离线学习一个元算法，之后再在在线的个性化环境下学习一个针对当前网络环境的个性化算法。另外，还需要不断优化离线学习来保证元算法的优势。</p>
<h2 id="introduction">Introduction</h2>
<p>首先对 ABR 算法做了总结，启发式算法根据当前网络情况和一部分状态变量给出决策，但是只有在选定较好的算法参数的时候这种算法才能表现较好。基于学习的算法则根据网络情况学习这些参数。然而总而言之，这些算法都不会自动调节自己的算法参数，尤其在面对变化较多的网络环境下时，这些算法表现较差。</p>
<p>之后是通过实验说明了现在的网络状况不仅是多样性（diverse）的，更多的是独一无二（unique）的。基于这一点，这篇文章认为做用户个性化是一个重要的提升。</p>
<p>所以基于 Input-driven Markov Decision Process (IMDP)，本文提出了 AABR。分为离线和在线两步，具体的训练方式见后续。</p>
<p>总结一下这篇文章的工作就是确认了当今网络环境的独特性，并通过 meta RL 解决了 ABR 的个性化问题。</p>
<h2 id="background-motivation">Background &amp; Motivation</h2>
<p>关于 ABR 之类的介绍这里忽略，主要看这篇文章如何论证当今网络的多样性。</p>
<p>多样性分为三个方向去论证，即不同用户间差异、不同场景间差异以及已有的部分 ABR 算法在这些网络环境下的表现。</p>
<p>用户间差异和场景间差异可以关注下图：</p>
<p><img src="/uploads/paper-2022-12/1.png" /></p>
<p>可以首先注意到网络吞吐率和 RTT 的变动是相当大的，而如果观察具体的用户，也可以发现用户之间的网络状况差异也很大，部分用户网络稳定（整体带宽 CDF 陡峭）而部分用户的网络波动较大，而且各个用户的网络指标平均值也差异较大。</p>
<p>对于场景，也可以注意到步行、地铁、公交、汽车的网络情况也各有不同，并不能一概而论。</p>
<p>如果从 ABR 的角度出发，这篇文章在两个不同环境下测试了若干个 ABR 算法的性能，得到的结果是：</p>
<p><img src="/uploads/paper-2022-12/2.png" /></p>
<p>显然可以注意到已有的算法都很难兼顾两种情况，常常顾此失彼。这就意味着不稳定的网络情况很容易导致这些算法失效，从而也难以实现用户个性化。</p>
<h2 id="methods">Methods</h2>
<h3 id="input-driven-markov-decision-process">Input-driven Markov Decision Process</h3>
<p>基本而言是在正常的 MDP 中引入了 input process <span class="math inline">\(\mathcal{Z}\)</span>，这个时候环境的转移概率定义为：</p>
<p><span class="math display">\[
\mathbb{T}[s&#39; \mid s, a, z] := \mathbb{P}[s_{t + 1} = s&#39; \mid s_t = s, a_t = a, z_t = z]
\]</span></p>
<p>此外，决策依然仅仅和 <span class="math inline">\(s_t \in \mathcal{S}\)</span> 有关，也就是说具体行为 <span class="math inline">\(a_t \in \mathcal{A}\)</span> 与 <span class="math inline">\(z_t \in \mathcal{Z}\)</span> 是独立的。</p>
<p>进一步的价值函数定义为：</p>
<p><span class="math display">\[
Q(s, a, z) := \sum_{s&#39; \in \mathcal{S}} \mathbb{T}[s&#39; \mid s, a, z](r(s, a, z) + \gamma V(s&#39;, z&#39;))
\]</span></p>
<p>在 IMDP 下，最优策略也就定义为：</p>
<p><span class="math display">\[
\pi^*(s, z) := \mathop{\rm argmax}_{a \in \mathcal{A}} \sum_{s&#39; \in \mathcal{S}} \mathbb{T}[s&#39;, z&#39; \mid s, a, z]\left[r(s, a, z) + \gamma \max_{a&#39; \in \mathcal{A}} Q(s&#39;, a&#39;, z&#39;)\right]
\]</span></p>
<p>可以注意到，如果两个 agent 运行相同的 policy <span class="math inline">\(\pi\)</span>，其不同之处仅仅是 input process <span class="math inline">\(\mathcal{Z}_{1, 2}\)</span> 不同。可以注意到这两个 agent 将会始终保持相同的决策，所以其价值函数相同当且仅当 <span class="math inline">\(\mathcal{Z}_1 = \mathcal{Z}_2\)</span>。</p>
<p>如果在完全得知 input process <span class="math inline">\(\mathcal{Z}\)</span> 的条件下，求解最优策略相当于一般的强化学习问题，可以通过大量的强化学习方法完成。但是我们现在需要假定 input process 是完全不可感知的，在这种条件下，一般强化学习方法学习到的最优策略 <span class="math inline">\(\hat\pi^*\)</span> 的价值函数实际上是与 input process 无关的，其价值函数用 <span class="math inline">\(Q(s&#39;, a&#39;)\)</span> 表示，其满足：</p>
<p><span class="math display">\[
\max_{a&#39; \in \mathcal{A}} Q(s&#39;, a&#39;) = \mathbb{E}_{z&#39;} \max_{a&#39; \in \mathcal{A}} Q(s&#39;, a&#39;, z&#39;)
\]</span></p>
<p>所以现在解决 input process 未知条件下的最优策略求解问题就是最重要的。</p>
<h3 id="meta-rl-with-domain-knowledge">Meta RL with Domain Knowledge</h3>
<p>总之就是又阐述了一边 AABR 分为离线和在线两个训练过程，离线训练一个元算法之后在线训练一个个性化 ABR 算法。那么两个问题就是：</p>
<div class="note info no-icon"><ol type="i">
<li><p>How to obtain a good parameter initialization for fast-learning?</p></li>
<li><p>How to efficiently learn tailor-made ABR algorithms online?</p></li>
</ol>
</div>
<h3 id="model-agnostic-meta-learning">Model Agnostic Meta-Learning</h3>
<p>总之就是选定了 MAML 作为 AABR 的核心算法，MAML 的论文后面慢慢推着看。</p>
<p>不过这一部分也阐释了为何 MAML 适合用于这个场景，原文是：</p>
<div class="note info no-icon"><p>We find that model agnostic meta learning (MAML) is quite suitable in personalized ABR scenarios where the network traces on each user are quite limited.</p>
<p>Specifically, MAML consists of an inner loop and an outer loop.</p>
</div>
<p>也就是说从数据集的角度和训练流程的角度上说，MAML 和这个实验场景的契合度比较高。</p>
<h3 id="leveraging-domain-knowledge">Leveraging Domain Knowledge</h3>
<p>总体的含义是设计了一个模拟器用于生成数据以及相关的启发式算法，一方面补充了现实世界中数据量不足的问题，另外一方面其代表的启发式算法可以在 policy 无法处理某些状态的时候作为 backup 使用。</p>
<p><strong>注：</strong>这一部分可能需要结合下面的 overview 仔细学一下，然后对这里做一些扩充，因为这里模拟器的细节应该是做数据扩充的重要部分。</p>
<h2 id="aabr-overview">AABR Overview</h2>
<p>系统的大致结构为：</p>
<p><img src="/uploads/paper-2022-12/3.png" /></p>
<h3 id="basic-training-algorithm">Basic Training Algorithm</h3>
<p>在设计网络的输入的时候首先需要考虑的是什么才是真正需要考虑的状态，这篇文章采用的方式是首先将所有可以使用的状态都作为输入以训练一个 teacher network，之后采用类似模仿学习的方式，采用类似决策树等较为轻量级的模型去模仿神经网络的策略，之后再使用决策树剪枝的方式去获取真正有意义的非平凡状态。</p>
<div class="note success no-icon"><p>这个方法似乎是比较好玩的一个方法，用于筛选真正有用的输入状态。</p>
</div>
<p>这里剪枝的目的也是为了压缩最后的成本。</p>
<p>最后筛选出来的状态包括：</p>
<ul>
<li>上一次选定的视频块质量 <span class="math inline">\(q_t\)</span></li>
<li>目前的缓冲区占用率 <span class="math inline">\(b_t\)</span></li>
<li>前 <span class="math inline">\(k\)</span> 个视频块传输时网络吞吐率 <span class="math inline">\(C_t\)</span></li>
<li>前 <span class="math inline">\(k\)</span> 个视频块的下载时间 <span class="math inline">\(D_t\)</span></li>
<li>前 <span class="math inline">\(k\)</span> 个视频块的应答时间 <span class="math inline">\(P_t\)</span></li>
</ul>
<p>这里为了控制成本，实验中选取 <span class="math inline">\(k = 5\)</span>。</p>
<p>另外，为了保证模型的泛化性，这里需要将参数正则化，这样至少能保证模型面对完全没有经历过的网络环境不至于将其认为非法输入。</p>
<p>模型的输出和正常的 AC 网络没有很大的差别，都是 Actor 网络输出一个 softmax 后的概率张量表示各个行为的决策概率，而 Critic 网络输出一个标量表示对当前状态的价值评估。</p>
<h4 id="maximum-entropy-ppo-me-ppo">Maximum Entropy PPO (ME-PPO)</h4>
<p>这里的训练策略是 ME-PPO，其与 PPO 的差别在于最优行为的定义里纳入了熵：</p>
<p><span class="math display">\[
a_t^* := \mathop{\rm argmax}_a \hat{\mathbb{E}}_t\left[\sum_t \gamma^t(r_t + \lambda H^{\pi_\theta}(s_t))\right]
\]</span></p>
<p>这里定义熵为：</p>
<p><span class="math display">\[
H^{\pi_\theta}(s_t) = -\sum_{a \in \mathcal{A}} \pi_\theta(a \mid s_t) \ln\pi_\theta(a \mid s_t)
\]</span></p>
<p>元学习要求在离线环境下更重视 exploration 而在线环境下更重视 exploitation。这里将熵纳入考虑，如果策略 <span class="math inline">\(\pi_\theta\)</span> 对动作空间 <span class="math inline">\(\mathcal{A}\)</span> 中的所有动作给出的概率约趋向于平均，则熵越高，从而越会支持这样的选择。</p>
<p>ME-PPO 的具体策略是基于 Dual-PPO 的 double clip 策略，具体如下：</p>
<p><span class="math display">\[
\begin{aligned}
\mathcal{L}^{\rm PPO} &amp;:= \min\left(\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\rm old}}(a_t \mid s_t)}(\theta) \hat A_t, {\rm clip}\left(\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\rm old}}(a_t \mid s_t)}(\theta), 1 - \varepsilon, 1 + \varepsilon\right)\hat A_t\right) \\
\mathcal{L}^{\rm Actor} &amp;:= \begin{cases}
\hat{\mathbb{E}}_t[\max(\mathcal{L}^{\rm PPO}, c\hat A_t)] &amp; \hat A_t &lt; 0 \\
\hat{\mathbb{E}}_t[\mathcal{L}^{\rm PPO}] &amp; \hat A_t \geq 0 \\
\end{cases}
\end{aligned}
\]</span></p>
<p>这里 <span class="math inline">\(\hat A_t\)</span> 是优势函数，定义为：</p>
<p><span class="math display">\[
\hat A_t := r_t + \gamma(V^{\pi_\theta}(s_{t + 1}) + \lambda H^{\pi_\theta}(s_{t + 1})) - V^{\pi_\theta}(s_t)
\]</span></p>
<p>这里 <span class="math inline">\(c, \varepsilon\)</span> 是用来控制梯度的超参数。</p>
<p>AABR 中的 Critic 网络的训练策略就是最小化优势函数的均方误差：</p>
<p><span class="math display">\[
\mathcal{L}^{\rm Critic} := \frac12 \hat{\mathbb{E}}_t[A_t]^2
\]</span></p>
<p>从而整体的 loss 可以总结为：</p>
<p><span class="math display">\[
\nabla\mathcal{L}^{\rm MEPPO} := -\nabla_\theta \mathcal{L}^{\rm Actor}(\pi_\theta, \hat A_t) + \nabla_{\theta_v} \mathcal{L}^{\rm Critic}
\]</span></p>
<hr />
<p>此外，由于算法对 <span class="math inline">\(\lambda\)</span> 较为敏感，我们需要在训练过程中不断调节 <span class="math inline">\(\lambda\)</span>，调节方式较为简单：</p>
<p><span class="math display">\[
\lambda \leftarrow \lambda + \alpha[H^{\pi_\theta}(s_t) - H^{\rm Target}]
\]</span></p>
<p>这个目标在于熵逐步接近目标的时候衰减 <span class="math inline">\(\lambda\)</span> 以减缓接近速度。</p>
<h3 id="meta-learned-policies-for-offline-stage">Meta-Learned Policies for Offline Stage</h3>
<p><strong>注：</strong>具体的训练方式后续补充。</p>
<h1 id="model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</h1>
<p>这是 MAML 训练方法的原始论文，看情况有没有时间做一个复现。</p>
<p>文章的主要贡献就是一个元学习方式，并且似乎泛用性很广：</p>
<div class="note info no-icon"><p>In this work, we propose a meta-learning algorithm that is general and model-agnostic, in the sense that it can be directly applied to any learning problem and model that is trained with a gradient descent procedure.</p>
</div>
<p>元学习的核心在于训练好的模型能够迅速通过少量数据学习到新的任务，并且其能够应用到大量的下游任务中。这篇文章将训练模型理解为在模型内部建立起适用于某种任务的特征表示，如果这种表示能够适合于多种任务，那么我们只需要进行合适的微调就可以迅速适应新的任务。如果从动力系统的角度理解，我们需要提高需要完成的任务的 loss 对参数的敏感度，这个敏感度越高，参数的变化对 loss 的提升的影响越大。</p>
<h2 id="model-agnostic-meta-learning-1">Model-Agnostic Meta-Learning</h2>
<h3 id="meta-learning-problem-set-up">Meta-Learning Problem Set-Up</h3>
<p>这里严格定义了元学习需要解决的问题。</p>
<p>首先我们需要形式化定义什么是任务，这篇文章定义为：</p>
<p><span class="math display">\[
\mathcal{T} := \{\mathcal{L}(\b x_1, \b a_1, \cdots, \b x_H, \b a_H), q(\b x_1), q(\b x_{t + 1} \mid \b x_t, \b a_t), H\}
\]</span></p>
<p>这里 <span class="math inline">\(\mathcal{L}\)</span> 表示 loss，<span class="math inline">\(q(\b x_1)\)</span> 是初始状态的分布，<span class="math inline">\(q(\b x_{t + 1} \mid \b x_t, \b a_t)\)</span> 是状态转移的概率分布，<span class="math inline">\(H\)</span> 是 episode 的长度。这个定义同时涵盖了普通的强化学习和监督学习，其中监督学习中 <span class="math inline">\(H = 1\)</span>。</p>
<p>模型本身描述为一个函数 <span class="math inline">\(f\)</span>，其接受当前状态 <span class="math inline">\(\b x\)</span> 并给出输出 <span class="math inline">\(\b a\)</span>，即 <span class="math inline">\(\b a = f(\b x)\)</span>。模型不断在该任务中给定 <span class="math inline">\(\b a_1, \b a_2, \cdots, \b a_H\)</span>，而 loss <span class="math inline">\(\mathcal{L}\)</span> 则给出了实际的反馈，这种反馈可以是分类问题中的失配损失或者 Markov 决策过程中的负收益。</p>
<p>在元学习中我们需要考虑一系列的任务的分布，这里记作 <span class="math inline">\(p(\mathcal{T})\)</span>。如果我们要求 <span class="math inline">\(K\)</span> shot learning，对于每一个从 <span class="math inline">\(p(\mathcal{T})\)</span> 抽取的具体任务 <span class="math inline">\(\mathcal{T}_i\)</span>，我们需要再通过 <span class="math inline">\(q_i\)</span> 抽取 <span class="math inline">\(K\)</span> 个样本，并且考虑 <span class="math inline">\(\mathcal{L}_{\mathcal{T}_i}\)</span> 的反馈。</p>
<p>之后直接搬一个论文原句，也应该是论文后面算法设计的主要出发点：</p>
<div class="note info no-icon"><p>The model <span class="math inline">\(f\)</span> is then improved by considering how the <em>test</em> error on new data from <span class="math inline">\(q_i\)</span> changes with respect to the parameters.</p>
</div>
<h3 id="a-model-agnostic-meta-learning-algorithm">A Model-Agnostic Meta-Learning Algorithm</h3>
<p>这里提供了算法设计，一个关键词就是之前提到的<strong>敏感度</strong>。</p>
<p>考虑用参数 <span class="math inline">\(\theta\)</span> 表示的模型 <span class="math inline">\(f_\theta\)</span>。在学习具体的任务 <span class="math inline">\(\mathcal{T}_i\)</span> 的时候，参数的更新方式为：</p>
<p><span class="math display">\[
\theta \leftarrow \theta - \alpha\nabla_\theta\mathcal{L}_{\mathcal{T}_i}(f_\theta)
\]</span></p>
<p>为了简化记号，我们假设在元学习之后学习具体任务仅用了一次梯度更新，此时任务 <span class="math inline">\(\mathcal{T}_i\)</span> 的最优参数就是 <span class="math inline">\(\theta_i&#39; := \theta - \alpha\nabla_\theta\mathcal{L}_{\mathcal{T}_i}(f_\theta)\)</span>，那么元学习的目标是：</p>
<p><span class="math display">\[
\min_\theta \sum_{\mathcal{T}_i \in p(\mathcal{T})} \mathcal{L}_{\mathcal{T}_i}(f_{\theta_i&#39;}) = \min_\theta \sum_{\mathcal{T}_i \in p(\mathcal{T})} \mathcal{L}_{\mathcal{T}_i}(f_{\theta - \alpha\nabla_\theta\mathcal{L}_{\mathcal{T}_i}(f_\theta)})
\]</span></p>
<p>那么还是使用简单的梯度下降，元学习中的参数更新就是：</p>
<p><span class="math display">\[
\theta \leftarrow \theta - \beta\nabla_\theta \sum_{\mathcal{T}_i \in p(\mathcal{T})} \mathcal{L}_{\mathcal{T}_i}(f_{\theta_i&#39;})
\]</span></p>
<p>在这里我们注意到出现了二阶梯度，部分深度学习框架能够支持这一操作，但本文也同样指出作一阶近似（即舍去二阶及以上的高阶项）是可行的。</p>
<p>基于此，MAML 的整体框架为：</p>
<p><img src="/uploads/paper-2022-12/4.png" /></p>
<h2 id="species-of-maml">Species of MAML</h2>
<p>本文这里举出了监督学习和强化学习中运用 MAML 的例子，这里我们只关注强化学习。</p>
<p>强化学习特化的 MAML 主要关注的是 loss 的选定，这里选定的就是最经典的负收益。另外，此时 <span class="math inline">\(q_{\mathcal{T}_i}\)</span> 实际上就是环境中的状态转移概率分布：</p>
<p><span class="math display">\[
\mathcal{L}_{\mathcal{T}_i}(f_\phi) = -\mathbb{E}_{(\b x_t, \b a_t) \sim (f_\phi, q_{\mathcal{T}_i})} \left[\sum_{t = 1}^H R(\b x_t, \b a_t)\right]
\]</span></p>
<p>从而，强化学习特化的 MAML 算法为：</p>
<p><img src="/uploads/paper-2022-12/5.png" /></p>
<p>这之后就是一些实验结果了，感觉没啥意思了。</p>
<h1 id="learning-to-adapt-in-dynamic-real-world-environments-through-meta-reinforcement-learning">Learning to Adapt in Dynamic, Real-World Environments through Meta Reinforcement Learning</h1>
<p>这篇文章似乎没有官方的 code base，只有两个 community code base，现在可以在 <a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/learning-to-adapt-in-dynamic-real-world">这个链接</a> 获取，分别是 Tensorflow 和 Pytorch 版本。</p>
<h2 id="abstract-introduction">Abstract &amp; Introduction</h2>
<p>需要解决的问题依然是生成数据很困难以及测试环境中无法预料到的数据可能导致所训练的策略失效，引言之中也基本上叙述了人类可以很快适应自身机体发生的变化和外界环境发生的变化，提出了如果模型经历了足够多的环境微扰，其就可以很快学习适应新环境。</p>
<p>相比较于原先的 meta RL 对任务的定义，这篇文章提出的模型能够处理每个时间步环境或者任务都在变化的情景。也就是说，原先的 meta RL 不过是提前预设好了需要解决什么样的任务，并且每一个 RL trajectory 仅会属于一个任务，而这篇文章的 RL trajectory 中可以涵盖多种任务并且任何时间都可以切换。所以这篇文章的 meta RL 更为 general。</p>
<p>本文也是有 trade off 的，主要表达如下：</p>
<div class="note info no-icon"><p>By specifically training a neural network model to require only a small amount of experience to adapt, we can enable effective online adaptation in complex environments while putting less pressure on needing a perfect global model.</p>
</div>
<p>也就是说，这篇文章为了能够在少数据条件下快速适应新环境，并不会强制要求这个模型在 global 环境下表现到 perfect。</p>
<p>总而言之，这篇文章主要强调学得快，学习速度大约是每个新任务需要在真实世界中经历 1.5 到 3 小时，不到原有 model-free RL 方法学习时间的十分之一。并且，在同等数据量条件下，这篇文章的模型效果也比其他方法优越。此外，为了展示本文模型的学习能力，其将其真实部署在了机器人上，来学习真实世界的情况。</p>
<h2 id="related-works">Related Works</h2>
<p>首先提到最原始的强化学习方法没有泛用性，以及 model-free RL 往往需要大量和真是环境交互从而不实际。发展为 model-based RL 之后，就需要面临对环境建模的 model 需要在不同的任务之间调整的问题，首先否决了单一的全局模型，而先前有人尝试通过 GP 方法合并模型不确定性从而让模型能够表示不同的环境，然而这个方法做出了额外的假设且拓展性差且难以运用到高维空间，而本文提出的结果能够通过观察从而快速切换到新的环境。</p>
<div class="note warning no-icon"><p>这里的问题就是我是真的没有太看懂这个 model 到底是对环境建模的模型还是最后要学习的 policy。</p>
</div>
<p>还有一种处理方式是首先学习一个全局模型，在测试的时候通过梯度下降进行微调。此外，还有工作表明不需要完美的全局模型，可以微调先验知识来处理小变化。然而，这些方法面临着模型的训练目的与测试时的使用方式之间的不匹配的问题。本文通过明确训练一个能快速有效适应的模型来弥合这一差距。</p>
<h2 id="preliminaries">Preliminaries</h2>
<h3 id="model-based-reinforcement-learning">Model-Based Reinforcement Learning</h3>
<p>马尔可夫决策过程在这里建模为 <span class="math inline">\((\mathcal{S}, \mathcal{A}, p, r, \gamma, \rho_0, H)\)</span>。这里 <span class="math inline">\(\mathcal{S}\)</span> 是状态空间，<span class="math inline">\(\mathcal{A}\)</span> 是动作空间，<span class="math inline">\(p(s&#39; \mid s, a)\)</span> 是环境状态转移概率，<span class="math inline">\(r: \mathcal{S} \times \mathcal{A} \to \mathbb{R}\)</span> 是奖励函数，<span class="math inline">\(\rho_0: \mathcal{S} \to \mathbb{R}^+\)</span> 是初始状态概率分布，<span class="math inline">\(\gamma\)</span> 是奖励衰减常数，<span class="math inline">\(H\)</span> 是轨迹长度。</p>
<p>一条轨迹描述为 <span class="math inline">\(\tau(i, j) := (s_i, a_i, \cdots, s_j, a_j, s_{j + 1})\)</span>，我们的目标是求取最优策略 <span class="math inline">\(\pi: \mathcal{S} \to \mathcal{A}\)</span> 使得累计收益期望最大。</p>
<p>model-based RL 目标是近似处理 <span class="math inline">\(p(s&#39; \mid s, a)\)</span>，这个模型也被称为 dynamics model。</p>
<h3 id="meta-learning">Meta Learning</h3>
<p>一般的元学习需要给定假设，即训练用的任务和最终的测试任务都是从同一个任务分布 <span class="math inline">\(\rho(\mathcal{T})\)</span> 中抽取。元学习的目标是获取一个学习流程 <span class="math inline">\(\theta&#39; := u_\psi(\mathcal{D}^{\rm tr}_{\mathcal{T}}, \theta)\)</span> 以通过小数据集 <span class="math inline">\(\mathcal{D}^{\rm tr}_{\mathcal{T}}\)</span> 来学习任务 <span class="math inline">\(\mathcal{T}\)</span>，学习过程可以表示为：</p>
<p><span class="math display">\[
\min_{\theta, \psi} \mathbb{E}_{\mathcal{T} \sim \rho(\mathcal{T})}\left[\mathcal{L}(\mathcal{D}^{\rm test}_{\mathcal{T}}, \theta&#39;)\right]
\]</span></p>
<p>其中 <span class="math inline">\(\theta&#39; := u_\psi(\mathcal{D}^{\rm tr}_{\mathcal{T}}, \theta)\)</span>。</p>
<div class="note info no-icon"><p>这里的记号是真的复杂，事实上训练过程应该是函数 <span class="math inline">\(u_\psi\)</span>，训练过程用参数 <span class="math inline">\(\psi\)</span> 代表，其输出为训练后的最优参数 <span class="math inline">\(\theta&#39;\)</span>，而 <span class="math inline">\(\theta\)</span> 表示初始化参数。</p>
<p>另外注意，<span class="math inline">\(\theta\)</span> 这里是模拟环境的模型的参数，而非 policy 的参数。</p>
</div>
<h4 id="gradient-based-meta-learning">Gradient Based Meta Learning</h4>
<p>MAML 就是一种基于梯度的元学习方法，其最主要的目标是学习一个较好的参数初始化保证在少数调整中即可实现较好的下游任务。</p>
<p>MAML 的 <span class="math inline">\(u_\psi\)</span> 就是一般的梯度下降：</p>
<p><span class="math display">\[
u_\psi(\mathcal{D}^{\rm tr}_{\mathcal{T}}, \theta) = \theta - \alpha\nabla_\theta \mathcal{L}(\mathcal{D}^{\rm tr}_{\mathcal{T}}, \theta)
\]</span></p>
<p>如果学习率 <span class="math inline">\(\alpha\)</span> 是一个固定参数，那么 <span class="math inline">\(\psi = \varnothing\)</span>，如果 <span class="math inline">\(\alpha\)</span> 是一个元学习过程中需要学习的参数，那么 <span class="math inline">\(\psi = \alpha\)</span>。</p>
<p>即使看起来 <span class="math inline">\(u_\psi\)</span> 是平凡的，但是这种方法的表达能力和下面介绍的方法是持平的。</p>
<h4 id="recurrence-based-meta-learning">Recurrence Based Meta Learning</h4>
<p>这里借用了 RNN 的思想，在这个方法中，更新函数 <span class="math inline">\(u_\psi\)</span> 始终需要不断学习，其中 <span class="math inline">\(\psi\)</span> 是用于更新 RNN 隐含层状态的参数，而 <span class="math inline">\(\theta\)</span> 是余下的参数。</p>
<div class="note warning no-icon"><p>这里是真的真的真的没看懂，这在说啥？是我前置知识欠缺太多了？</p>
<p>只能希望下面能有个稍微好一点的说明。</p>
</div>
<h2 id="meta-learning-for-online-model-adaptation">Meta Learning for Online Model Adaptation</h2>
<p>之前提到过本文的 trajectory 中任务可以随着时间步推进而变化，所以本文设定的目标是根据前 <span class="math inline">\(M\)</span> 个时间步的数据预测未来 <span class="math inline">\(K\)</span> 个时间步情况，这里 <span class="math inline">\(M, K\)</span> 都是固定的超参数。</p>
<p>所以在这样的设定下，训练目标就是：</p>
<p><span class="math display">\[
\begin{aligned}
\theta_{\mathcal{E}}&#39; &amp;= u_\psi(\tau_{\mathcal{E}}(t - M, t - 1), \theta) \\
\min_{\theta, \psi}&amp; \mathbb{E}_{\tau_{\mathcal{E}}(t - M, t + K)} [\mathcal{L}(\tau_{\mathcal{E}}(t, t + K), \theta_{\mathcal{E}}&#39;)]
\end{aligned}
\]</span></p>
<p>而这里 loss 定义为：</p>
<p><span class="math display">\[
\mathcal{L}(\tau_{\mathcal{E}}(t, t + K), \theta_{\mathcal{E}}&#39;) := -\frac{1}{K} \sum_{k = t}^{t + K} \log\hat{p}_{\theta_{\mathcal{E}&#39;}}(s_{k + 1} \mid s_k, a_k)
\]</span></p>
<p>GrBAL 采取的更新策略为：</p>
<p><span class="math display">\[
\theta_{\mathcal{E}}&#39; = u_\psi(\tau_{\mathcal{E}}(t - M, t - 1), \theta) = \theta_{\mathcal{E}} + \psi\nabla_\theta\frac{1}{M} \sum_{m = t - M}^{t - 1} \log\hat{p}_{\theta_{\mathcal{E}}}(s_{m + 1} \mid s_m, a_m)
\]</span></p>
<p>ReBAL 这里又是没看懂，所以这个基于循环模型的到底是个什么东西。</p>
<h2 id="model-based-meta-reinforcement-learning">Model Based Meta Reinforcement Learning</h2>
<p>这一部分就是在讲解如何在线上做 adaptation，我觉得整体不如直接看算法直观：</p>
<p><img src="/uploads/paper-2022-12/6.png" /></p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/12/05/ku-grammar/" rel="prev" title="一些散落的古日语语法——以ク语法为例">
                  <i class="fa fa-chevron-left"></i> 一些散落的古日语语法——以ク语法为例
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/12/21/graduation-code-base/" rel="next" title="毕业设计代码框架文档">
                  毕业设计代码框架文档 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments"><div id="twikoo-comments"></div></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ashitemaru</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">400k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">6:03</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://unpkg.com/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{&quot;enable&quot;:true,&quot;theme&quot;:&quot;default&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;unpkg.com&#x2F;mermaid@8.9.3&#x2F;dist&#x2F;mermaid.min.js&quot;,&quot;integrity&quot;:&quot;sha256-OyJHvRcZHaRR6Ig73ppxF4QXk8HzvfgTprRWkulCkfY&#x3D;&quot;}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;unpkg.com&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;,&quot;integrity&quot;:&quot;sha256-ncNI9OXOS5Ek4tzVYiOMmN&#x2F;KKCPZ6V0Cpv2P&#x2F;zHntiA&#x3D;&quot;}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="twikoo" type="application/json">{&quot;enable&quot;:true,&quot;visitor&quot;:true,&quot;envId&quot;:&quot;https:&#x2F;&#x2F;vercel-deploy-two.vercel.app&quot;,&quot;el&quot;:&quot;#twikoo-comments&quot;}</script>
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.twikoo.el)
    .then(() => NexT.utils.getScript(
      'https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js',
      { condition: window.twikoo }
    ))
    .then(() => {
      twikoo.init(CONFIG.twikoo);
    });
});
</script>

</body>
</html>
