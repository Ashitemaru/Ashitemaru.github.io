<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic%7CNoto+Serif+KR:300,300italic,400,400italic,700,700italic%7CMS+PMincho:300,300italic,400,400italic,700,700italic%7CFira+Code:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext" referrerpolicy="no-referrer">

<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://unpkg.com/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;ashitemaru.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Muse&quot;,&quot;version&quot;:&quot;8.5.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;width&quot;:320,&quot;display&quot;:&quot;always&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:true,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:&quot;disqusjs&quot;,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;manual&quot;,&quot;top_n_per_article&quot;:1,&quot;unescape&quot;:false,&quot;preload&quot;:false}}</script><script src="/js/config.js"></script>
<meta name="description" content="这个月正式参与进入了实验室和快手的一个合作项目，依然还是需要从论文先开始。">
<meta property="og:type" content="article">
<meta property="og:title" content="2022 年 9 月论文笔记">
<meta property="og:url" content="https://ashitemaru.github.io/2022/09/20/paper-2022-09/index.html">
<meta property="og:site_name" content="Ashitemaru">
<meta property="og:description" content="这个月正式参与进入了实验室和快手的一个合作项目，依然还是需要从论文先开始。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/paper-2022-09/1.png">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/paper-2022-09/2.png">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/paper-2022-09/3.png">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/paper-2022-09/4.png">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/paper-2022-09/5.png">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/paper-2022-09/6.png">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/paper-2022-09/7.png">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/paper-2022-09/8.png">
<meta property="article:published_time" content="2022-09-20T20:48:35.000Z">
<meta property="article:modified_time" content="2022-09-20T20:48:35.000Z">
<meta property="article:author" content="Ashitemaru">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ashitemaru.github.io/uploads/paper-2022-09/1.png">


<link rel="canonical" href="https://ashitemaru.github.io/2022/09/20/paper-2022-09/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;ashitemaru.github.io&#x2F;2022&#x2F;09&#x2F;20&#x2F;paper-2022-09&#x2F;&quot;,&quot;path&quot;:&quot;2022&#x2F;09&#x2F;20&#x2F;paper-2022-09&#x2F;&quot;,&quot;title&quot;:&quot;2022 年 9 月论文笔记&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>2022 年 9 月论文笔记 | Ashitemaru</title>
  



<link rel="stylesheet" href="https://www.unpkg.com/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://www.unpkg.com/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://www.unpkg.com/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
<script src="https://www.unpkg.com/twikoo@1.4.1/dist/twikoo.all.min.js"></script>
<script src="/js/jquery.min.js"></script>
<script>
function getURL(e) {
    var http = e.slice(0, 4)
    var https = e.slice(0, 5)
    if (http == "http" || https == "https") {
        return e
    } else if (e == "" || e == null || e == undefined) {
        return e
    } else {
        e = 'http://' + e
        return e
    }
}

function newComment() {
    twikoo.getRecentComments({
        envId: 'https://vercel-deploy-two.vercel.app',
        pageSize: 10,
        includeReply: false
    }).then(function (res) {
        var hotComments = $("#hot-comments");
        for (var i = 0; i < res.length; i++) {
            var nick = res[i].nick;
            var content = res[i].commentText;
            var newcontent = content.substring(0, 50);
            var url = res[i].url;
            var avatar = res[i].avatar;
            var link = getURL(res[i].link);
            var updatedAt = res[i].relativeTime;
            var commentId = '#' + res[i].id;
            hotComments.append(
                `<li class="px1 pb2 flex items-center">
                    <img style="width:40px;height:40px" class="circle mx1 listavatar" src="${avatar}">
                    <div style="display:flex;flex-direction:column;width:100%;">
                        <div style="display:flex;justify-content:space-between;flex-direction:row;align-items:center;">
                            <div class="h5 listauthor overflow-hidden" title="${nick}">
                                <a target="_blank" rel="noopener external nofollow noreferrer" href="${link}">${nick}</a>
                            </div>
                            <div class="h6 mr1 listdate wenzi hang1" style="color:#777777;">${updatedAt}</div>
                        </div>
                        <div style="display:flex;flex-direction:row;width:100%;">
                            <a class="h5 list-comcontent" style="overflow:hidden;display:flex;border-bottom:0px;text-overflow:ellipsis;line-height:1.5;text-align:left" href="${url}${commentId}">${newcontent}</a>
                        </div>
                    </div>
                </li>`
            );
        }
    }).catch(function (err) {
        console.error(err);
    });
}

function replaceRuby() {
    $('code')
        .filter((_, node) => {
            var list = $(node).text().split(' ');
            return list.length === 3 && list[0] === "@";
        })
        .replaceWith((_, text) => {
            var list = text.split(' ');
            var written = list[1];
            var read = list[2];
            return $(`<ruby>${written}<rp>(</rp><rt>${read}</rt><rp>)</rp></ruby>`);
        });
}

$(function () {
    newComment();
    replaceRuby();
});
</script>

<!-- CSS -->
<link href="/css/app.min.css" rel="stylesheet">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Ashitemaru</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">11</span></a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">52</span></a></li>
        <li class="menu-item menu-item-skill-docs"><a href="https://sast-skill-docers.github.io/sast-skill-docs/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i>技能引导文档</a></li>
        <li class="menu-item menu-item-thuinfo"><a href="https://www.thuinfo.net/" rel="noopener" target="_blank"><i class="fa fa-sitemap fa-fw"></i>THUInfo</a></li>
        <li class="menu-item menu-item-se-index"><a href="https://thuse-course.github.io/course-index/" rel="noopener" target="_blank"><i class="fa fa-calendar fa-fw"></i>软工课程主页</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#causalsim---unbiased-trace-driven-network-simulation"><span class="nav-number">1.</span> <span class="nav-text">CausalSim - Unbiased Trace-Driven Network Simulation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#abstract-introduction"><span class="nav-number">1.1.</span> <span class="nav-text">Abstract &amp; Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#motivation"><span class="nav-number">1.2.</span> <span class="nav-text">Motivation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#the-problem-of-trace-driven-network-simulation"><span class="nav-number">1.2.1.</span> <span class="nav-text">The problem of trace-driven network simulation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#causal-inference-to-the-rescue"><span class="nav-number">1.2.2.</span> <span class="nav-text">Causal inference to the rescue</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#model"><span class="nav-number">1.3.</span> <span class="nav-text">Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#causal-dynamics"><span class="nav-number">1.3.1.</span> <span class="nav-text">Causal dynamics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#trace-driven-simulation-is-counterfactual-estimation"><span class="nav-number">1.3.2.</span> <span class="nav-text">Trace-driven simulation is counterfactual estimation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#causalsim---key-insights"><span class="nav-number">1.4.</span> <span class="nav-text">CausalSim - Key insights</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#counterfactual-estimation-as-matrix-completion"><span class="nav-number">1.4.1.</span> <span class="nav-text">Counterfactual estimation as matrix completion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#exploiting-the-policy-invarience-of-latent-factors"><span class="nav-number">1.4.2.</span> <span class="nav-text">Exploiting the policy invarience of latent factors</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#causalsim---details"><span class="nav-number">1.5.</span> <span class="nav-text">CausalSim - Details</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#evaluation"><span class="nav-number">1.6.</span> <span class="nav-text">Evaluation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#a-real-world-abr-experiment"><span class="nav-number">1.6.1.</span> <span class="nav-text">A real-world ABR experiment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#learning-abr-policies-with-causalsim"><span class="nav-number">1.6.2.</span> <span class="nav-text">Learning ABR policies with CausalSim</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#server-load-balancing"><span class="nav-number">1.6.3.</span> <span class="nav-text">Server load balancing</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#final-analysis"><span class="nav-number">1.7.</span> <span class="nav-text">Final analysis</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#deployment-efficient-reinforcement-learning-via-model-based-offline-optimization"><span class="nav-number">2.</span> <span class="nav-text">Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#abstract-introduction-1"><span class="nav-number">2.1.</span> <span class="nav-text">Abstract &amp; Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#preliminaries"><span class="nav-number">2.2.</span> <span class="nav-text">Preliminaries</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#deployment-efficiency"><span class="nav-number">2.3.</span> <span class="nav-text">Deployment efficiency</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#behavior-regularized-model-ensemble"><span class="nav-number">2.4.</span> <span class="nav-text">Behavior regularized model ensemble</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#imaginary-rollout-from-model-ensemble"><span class="nav-number">2.4.1.</span> <span class="nav-text">Imaginary rollout from model ensemble</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#policy-update-with-behavior-regularization"><span class="nav-number">2.4.2.</span> <span class="nav-text">Policy update with behavior regularization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#experiments"><span class="nav-number">2.5.</span> <span class="nav-text">Experiments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#final-analysis-1"><span class="nav-number">2.6.</span> <span class="nav-text">Final analysis</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ashitemaru"
      src="/uploads/avatar.png">
  <p class="site-author-name" itemprop="name">Ashitemaru</p>
  <div class="site-description" itemprop="description">A cat that likes Sakana.</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">52</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Ashitemaru" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Ashitemaru" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:qhd19@mails.tsinghua.edu.cn" title="E-Mail → mailto:qhd19@mails.tsinghua.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/Ashitemaru" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;Ashitemaru" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Friends
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.unidy.cn/" title="https:&#x2F;&#x2F;www.unidy.cn" rel="noopener" target="_blank">UNIDY</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.dilant.cn/" title="https:&#x2F;&#x2F;www.dilant.cn" rel="noopener" target="_blank">Dilant</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://zcy.moe/" title="https:&#x2F;&#x2F;zcy.moe" rel="noopener" target="_blank">猫猫</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://c7w.tech/" title="https:&#x2F;&#x2F;c7w.tech" rel="noopener" target="_blank">c7w</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.leenldk.top/" title="http:&#x2F;&#x2F;www.leenldk.top&#x2F;" rel="noopener" target="_blank">leenldk (20)</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://sun80449.github.io/" title="https:&#x2F;&#x2F;sun80449.github.io" rel="noopener" target="_blank">lcr</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://pwe.cat/" title="https:&#x2F;&#x2F;pwe.cat" rel="noopener" target="_blank">索尔</a>
        </li>
    </ul>
  </div>
<div class="sidebar-1 mybox relative">
    <div class="p2">
        <i class="fab fa-facebook-messenger mr1"></i>
        Latest Comments
    </div>
    <div id="hot-comments"></div>
</div>
          </div>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/Ashitemaru" class="github-corner" title="Come here for fun." aria-label="Come here for fun." rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ashitemaru.github.io/2022/09/20/paper-2022-09/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.png">
      <meta itemprop="name" content="Ashitemaru">
      <meta itemprop="description" content="A cat that likes Sakana.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ashitemaru">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          2022 年 9 月论文笔记
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-09-20 20:48:35" itemprop="dateCreated datePublished" datetime="2022-09-20T20:48:35+00:00">2022-09-20</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2022-09-20 20:48:35" itemprop="dateModified" datetime="2022-09-20T20:48:35+00:00">2022-09-20</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91%E8%AE%A1%E7%AE%97%E6%9C%BA/" itemprop="url" rel="index"><span itemprop="name">【论文笔记】计算机</span></a>
        </span>
    </span>

  
    <span id="/2022/09/20/paper-2022-09/" class="post-meta-item twikoo_visitors" data-flag-title="2022 年 9 月论文笔记" title="阅读次数">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="twikoo_visitors"></span>
    </span>
  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>这个月正式参与进入了实验室和快手的一个合作项目，依然还是需要从论文先开始。</p>
<span id="more"></span>
<h1 id="causalsim---unbiased-trace-driven-network-simulation">CausalSim - Unbiased Trace-Driven Network Simulation</h1>
<h2 id="abstract-introduction">Abstract &amp; Introduction</h2>
<p>在测评网络协议在线上的实际性能的时候，虽然 RCT 方法是最为标准的，但是由于其极其昂贵，对周边基础设施的要求也很高，所以只有若干大型网络运营方才能支持这种测评方法。基于此，一般的研究所会选择网络模拟器或者 trace-based simulation。但是网络模拟器往往难以模拟出真实网络环境中的复杂情况，并且这类模拟器需要对网络环境的极其详细的掌握。而 trace-based simulation 又常常因为数据质量不高或者有偏导致测评不精确。</p>
<p>【RCT 方法之后看论文】</p>
<p>应用反事实模拟以利用离线历史数据的主要思路为，首先获取在某一个给定网络条件下使用某一个网络协议时的 trajectory，而我们需要训练一个模型来预测在这个网络条件下使用其他网络协议的时候的 trajectory。基于这个模型，我们只需要已知一个协议的性能，就能够得知新协议的性能表现而不需要实际部署该协议。</p>
<p>使用反事实模拟的好处包括：</p>
<ul>
<li>节省部署协议和使用 RCT 方法评测的成本</li>
<li>能够在完全一致的网络条件下比较不同协议的差异</li>
<li>使用模型进行反事实模拟得到的数据可在社区内交流，帮助无法访问大型网络以测试其协议设计的开发者</li>
</ul>
<p>当然，反事实模拟的方法是困难的，原因之一是用于采集观测数据的策略可能破坏无偏性，这一点我们后续阐述。</p>
<p>那么本文的核心成果就是：</p>
<div class="note info no-icon"><p>We present CausalSim, a framework for learning <strong>unbiased trace-driven counterfactual simulation models</strong> for network protocols.</p>
</div>
<h2 id="motivation">Motivation</h2>
<p>总体而言，该文章尝试解决的问题依然是，考虑到直接线上测试算法性能较为耗时且代价较大，能否：</p>
<ul>
<li>使用历史数据预测算法的性能</li>
<li>服务运营商能否使用历史数据构建一个模拟器使得开发者可以在不接触实际网络环境的条件下评测算法</li>
</ul>
<h3 id="the-problem-of-trace-driven-network-simulation">The problem of trace-driven network simulation</h3>
<p>这一部分假设了一个新算法 FabABR，然后在 Puffer 数据集上收集了类似 BOLA-BASIC、BBA、Fugu 等已知的算法在给定的网络条件下的 buffer occupancy，令 BOLA-BASIC v1 充当新算法 FabABR，之后构建了两个模型来通过其他已知算法的 buffer occupancy 预测 FabABR 的 buffer occupancy，并和 ground truth 比较。</p>
<p>当然，在实际环境中，我们是没有 ground truth 的，因为我们没有新算法通过 RCT 方法评测得到的 buffer occupancy 作为 ground truth。</p>
<p>第一个模型是 ExpertSim。其原理为，在某个网络条件下，如果某个算法在时刻 <span class="math inline">\(t\)</span> 达成的吞吐率为 <span class="math inline">\(\hat c_t\)</span>，那么我们假设在该网络条件下，新算法达成的吞吐率也是 <span class="math inline">\(\hat c_t\)</span>。这也是该模型的首要假设，即不同算法对码率的决策不影响观测到的网络吞吐率，而这个假设广泛应用于各种模型。</p>
<p>在这个假设下，记时刻 <span class="math inline">\(t\)</span> 时的 buffer size 为 <span class="math inline">\(b_t\)</span>，而时刻 <span class="math inline">\(t + 1\)</span> 时的 buffer size 为 <span class="math inline">\(b_{t + 1}\)</span>，时刻 <span class="math inline">\(t\)</span> 的视频块时间为 <span class="math inline">\(T\)</span>，根据 FabABR 选择的码率该视频块的大小为 <span class="math inline">\(s_t\)</span>，那么：</p>
<p><span class="math display">\[
b_{t + 1} = \max\left(0, b_t - \frac{s_t}{\hat c_t}\right) + T
\]</span></p>
<p>第二个模型为 SLSim。其模型为两层全链接层，每层 128 个 ReLU 激活的神经元。该网络的输入为 <span class="math inline">\(b_t, \hat c_t, s_t\)</span>，输出为 <span class="math inline">\(b_{t + 1}\)</span>，符号的意义同上。由于我们具有 ground truth，我们就将其作为 supervisor 进行训练，loss 设定为 L2 loss。最后的实验结果为：</p>
<p><img src="/uploads/paper-2022-09/1.png" height="50%" width="50%" /></p>
<p>这里可以看见两种模型都不及 CausalSim 做出的预测。而这一差距的本质就是这两个模型都默认了不同算法对码率的决策不影响观测到的网络吞吐率，但实际上由于类似于 TCP 慢启动、和其他流量竞争等因素，不同的码率选择实际上会影响实际的吞吐率。而 Puffer 数据集实际上就能证明该假设错误：</p>
<p><img src="/uploads/paper-2022-09/2.png" height="50%" width="50%" /></p>
<p>可见不同算法影响了吞吐率的分布。这种错误的假设导致了数据的有偏。</p>
<h3 id="causal-inference-to-the-rescue">Causal inference to the rescue</h3>
<p>如果我们能够获取更深层次的网络情况而非仅仅是表层的吞吐率，我们完全就能修正原先的错误假设，这是因为我们可以将这些底层的网络情况视作独立于 ABR 算法的因素。</p>
<p>但是这些底层网络情况并不出现在数据集中，我们需要通过表层数据推测这些底层情况，而这也就是反事实推理介入的地方。</p>
<p>具体的设计见下述部分。</p>
<h2 id="model">Model</h2>
<h3 id="causal-dynamics">Causal dynamics</h3>
<p>我们给出这样的建模，令 <span class="math inline">\(o_t^\star\)</span> 表示时刻 <span class="math inline">\(t\)</span> 的时候系统观测到的表层数据，<span class="math inline">\(u_t\)</span> 表示时刻 <span class="math inline">\(t\)</span> 的时候系统的底层数据，<span class="math inline">\(a_t\)</span> 表示时刻 <span class="math inline">\(t\)</span> 时我们对系统做出的决策，那么系统行为可以建模为：</p>
<p><span class="math display">\[
o_{t + 1}^\star = \mathcal{F}_{\rm evolution}(o_t^\star, u_t, a_t)
\]</span></p>
<p>在 ABR 问题中，<span class="math inline">\(o_t^\star\)</span> 包括的因素有 buffer size、实际吞吐率、Min RTT、可选视频块大小，而 <span class="math inline">\(u_t\)</span> 包括的因素有瓶颈连接时间、竞争流量的数量、竞争流量的 congestion control。</p>
<p>此外，我们可以将 <span class="math inline">\(o_t^\star\)</span> 拆分为 <span class="math inline">\(o_t\)</span> 和 <span class="math inline">\(m_t\)</span>，这里 <span class="math inline">\(m_t\)</span> 是观测数据中受到底层数据所影响的部分，从而我们可以拆分出纯外部数据 <span class="math inline">\(o_t\)</span>，而后续我们所称呼的“观测数据”即指代 <span class="math inline">\(o_t\)</span>。这一步拆分后，建模可以变为：</p>
<p><span class="math display">\[
\begin{aligned}
m_t &amp;= \mathcal{F}_{\rm mediation}(a_t, u_t) \\
o_{t + 1} &amp;= \mathcal{F}_{\rm system}(o_t, m_t, a_t) \\
o_t^\star &amp;= [o_t, m_t]
\end{aligned}
\]</span></p>
<p>这里，我们需要说明的是 <span class="math inline">\(m_t\)</span> 是可观测的，而 <span class="math inline">\(u_t\)</span> 是潜在而不可观测的。另外，<span class="math inline">\(m_t\)</span> 受到决策的影响，而 <span class="math inline">\(u_t\)</span> 不受到决策的影响。</p>
<p>在 ABR 问题中，<span class="math inline">\(m_t\)</span> 就是我们实现的吞吐率，其不仅受到潜在网络环境影响，还受到 ABR 码率决策的影响，而这也就是先前两个策略失败的核心原因。此外，根据该建模，在得知吞吐率 <span class="math inline">\(m_t\)</span> 的信息之后，我们完全不需要了解其他信息即可推断出其他可观测的信息 <span class="math inline">\(o_t\)</span>。</p>
<h3 id="trace-driven-simulation-is-counterfactual-estimation">Trace-driven simulation is counterfactual estimation</h3>
<p>首先统一符号，我们假设数据集一共采用了 <span class="math inline">\(K\)</span> 种不同算法，一共生成了 <span class="math inline">\(N\)</span> 条 trajectory，我们记第 <span class="math inline">\(i\)</span> 个 trajectory 的长度为 <span class="math inline">\(H_i\)</span>。</p>
<p>那么我们的训练策略可以描述为，首先对于任何 <span class="math inline">\(i\)</span>，我们给出决策序列 <span class="math inline">\(\{\tilde a_t^i\}_{t = 1}^{H_i}\)</span>，给定初始观测 <span class="math inline">\(o_1^i\)</span>，并且我们确定所有 trajectory 均基于同一个潜在状态序列 <span class="math inline">\(\{u_t^i\}_{t = 1}^{H_i}\)</span>，我们的目的是预测出观测序列 <span class="math inline">\(\{\tilde o_t^i\}_{t = 1}^{H_i}\)</span>。</p>
<p>此外，我们注意到学习 <span class="math inline">\(\mathcal{F}_{\rm system}\)</span> 是完全监督的，因为所有需要的数据均可以观测。所以最终的困难点在于估计 <span class="math inline">\(\{u_t^i\}_{t = 1}^{H_i}, \{m_t^i\}_{t = 1}^{H_i}\)</span> 以及学习 <span class="math inline">\(\mathcal{F}_{\rm mediation}\)</span>。</p>
<h2 id="causalsim---key-insights">CausalSim - Key insights</h2>
<h3 id="counterfactual-estimation-as-matrix-completion">Counterfactual estimation as matrix completion</h3>
<p>这一步的核心在于将反事实推理等效为矩阵填充问题。我们假设动作空间大小为 <span class="math inline">\(A\)</span>，即 <span class="math inline">\(a_t^i \in \{1, 2, \cdots, A\}\)</span>，另外记：</p>
<p><span class="math display">\[
U := \sum_{i = 1}^NH_i
\]</span></p>
<p>那么我们考虑一个 <span class="math inline">\(A \times U\)</span> 的矩阵，其中每一行代表一种决策，每一列代表潜在状态。这里我们将第 <span class="math inline">\(i\)</span> 个 trajectory 的第 <span class="math inline">\(t\)</span> 时刻中所有的 <span class="math inline">\((i, t)\)</span> 按 <span class="math inline">\(i\)</span> 为主序数排列为长度为 <span class="math inline">\(U\)</span> 的序列，从而每一个 <span class="math inline">\((i, t)\)</span> 都能对应到 <span class="math inline">\(M\)</span> 中的某一列。</p>
<p>在第 <span class="math inline">\(i\)</span> 个 trajectory 的第 <span class="math inline">\(t\)</span> 时刻，我们观测到 <span class="math inline">\(m_t^i = \mathcal{F}_{\rm mediation}(a_t^i, u_t^i)\)</span>，而 <span class="math inline">\(m_t^i\)</span> 就是矩阵 <span class="math inline">\(M\)</span> 中 <span class="math inline">\(a_t^i\)</span> 所对应的行和 <span class="math inline">\((i, t)\)</span> 所对应的列处的已知元素，该列其余元素均为未知元素。也就是说我们在初始条件下，每一列都会有一个已知元素。</p>
<p>而现有的矩阵填充算法并不能直接应用到本问题上，因为本问题中的 <span class="math inline">\(M\)</span> 元素缺失的 pattern 并不随机并且缺失元素数量过多。但是另外一方面，矩阵 <span class="math inline">\(M\)</span> 显然具有更为优越的结构，因为元素缺失的 pattern 和已知的 ABR 算法决策流程相关，并且 <span class="math inline">\(N\)</span> 个 trajectory 的网络潜在条件是一致的。</p>
<h3 id="exploiting-the-policy-invarience-of-latent-factors">Exploiting the policy invarience of latent factors</h3>
<p>我们这里考虑一个简单情况，即 <span class="math inline">\(A = 2, U = 2n\)</span>，并且矩阵 <span class="math inline">\(M\)</span> 秩为 <span class="math inline">\(1\)</span>。这说明存在 <span class="math inline">\(a \in \mathbb{R}^2, u \in \mathbb{R}^{2n}\)</span> 满足 <span class="math inline">\(M = au^T\)</span>。之后我们令 <span class="math inline">\(K = 2\)</span>。</p>
<p>考虑到：</p>
<p><span class="math display">\[
\frac{M_{1, j}}{M_{2, j}} = \frac{a_1u_j}{a_2u_j} = \frac{a_1}{a_2}
\]</span></p>
<p>并且每一列必然有一个已知元素，所以我们只需要估计 <span class="math inline">\(a_1 / a_2\)</span>。</p>
<p>另外，基于两个 trajectory 均基于一致的网络潜在条件，所以对于较大的 <span class="math inline">\(n\)</span>，每一列的期望应当类似，所以有以下估计（这个估计我感觉比较奇怪，说理也比较不充分）：</p>
<p><span class="math display">\[
\frac{1}{n}\sum_{j = 1}^n u_j \approx \frac{1}{n}\sum_{j = n + 1}^{2n} u_j
\]</span></p>
<p>之后我们就可以得到下述估计：</p>
<p><span class="math display">\[
\frac{\sum_{j = 1}^n M_{1, j}}{\sum_{j = n + 1}^{2n} M_{2, j}} = \frac{\sum_{j = 1}^n a_1u_j}{\sum_{j = n + 1}^{2n} a_2u_j} \approx \frac{a_1}{a_2}
\]</span></p>
<p>从而我们得到了我们需要的估计，从而我们就能补充完整整个矩阵。</p>
<h2 id="causalsim---details">CausalSim - Details</h2>
<p>最后采用了这样的一个网络结构：</p>
<p><img src="/uploads/paper-2022-09/3.png" height="50%" width="50%" /></p>
<p>该网络的目标是为了获取不变的潜在条件 <span class="math inline">\(\{u_t^i\}_{t = 1}^{H_i}\)</span>，而完成这项任务的网络是 Policy Discriminator，其接受 <span class="math inline">\(\{u_t^i\}_{t = 1}^{H_i}\)</span> 作为输入，输出为其认为该行为决策是由哪一个算法做出的。由于其为一个简单的多分类任务，所以其 loss 就采用简单的交叉熵表示为：</p>
<p><span class="math display">\[
\mathcal{L}_{\rm disc} := \mathbb{E}_D[-\ln \mathbb{P}(\pi \mid \hat u)]
\]</span></p>
<p>而我们的目标是让 Policy Discriminator 的 loss 尽可能大，从而通过分类器无法区分各类算法保证抽取的潜在不变条件确实和具体的算法无关。</p>
<p>整个网络采用的 loss 为：</p>
<p><span class="math display">\[
\mathcal{L}_{\rm total} := \mathbb{E}_D[\delta(m_t^i - \hat m_t^i)^2 + (o_t^i - \hat o_t^i)^2] - \kappa\mathcal{L}_{\rm disc}
\]</span></p>
<h2 id="evaluation">Evaluation</h2>
<h3 id="a-real-world-abr-experiment">A real-world ABR experiment</h3>
<p>实验过程比较简单，问题出现在如何评测。因为 Puffer 数据集里每一个 trajectory 仅使用了一种算法跑过，所以实际上是无法对多个新算法进行评测的。所以其采用了评价变量分布的方式进行评测。即，不同的算法在不同的 trajectory 上运行，只要潜在网络因素不变，各类变量的分布应该是类似的。那么基于此，我们只要再做一次模拟，之后指定一个变量，评价两次模拟中该变量分布之间的差距。这里评价分布的差距使用 EMD：</p>
<p><span class="math display">\[
{\rm EMD}(\mathcal{P}, \mathcal{Q}) := \int_{-\infty}^{+\infty} |\mathcal{P}(x) - \mathcal{Q}(x)| {\rm d}x
\]</span></p>
<p>这里 <span class="math inline">\(\mathcal{P}, \mathcal{Q}\)</span> 都是累积分布函数。</p>
<p>评测结果自然是 CausalSim 很厉害：</p>
<p><img src="/uploads/paper-2022-09/4.png" height="50%" width="50%" /></p>
<h3 id="learning-abr-policies-with-causalsim">Learning ABR policies with CausalSim</h3>
<p>既然现在已经具有了工作良好的模拟器，我们就可以利用该模拟器训练 ABR，具体的过程不阐述，结果如下：</p>
<p><img src="/uploads/paper-2022-09/5.png" height="70%" width="70%" /></p>
<p>总体而言，CausalSim 显然和真实环境下训练的结果贴近，而其他的模拟器均有很大偏差。并且对于高 RTT 的 Agent，这个差距更大。这个差距的根本原因依然是其他模拟器都是有偏的，在例如慢启动等条件下，容易选择保守的码率。</p>
<h3 id="server-load-balancing">Server load balancing</h3>
<p>【PASS】</p>
<h2 id="final-analysis">Final analysis</h2>
<p>读这一篇论文目的是为了研究怎么离线评测算法，也就是训练一个合理的环境模拟器出来。但感觉本文的一个核心思想为将网络吞吐率等因素视作和 ABR 决策有关的变量，将其纳入考虑。严格而言就是更新了对环境内部逻辑的建模，使之更为贴近真实环境。而如果不把网络吞吐率纳入考虑，则前一时刻无论做出什么选择，下一时刻环境都会要求算法实现同样的吞吐率，从而导致算法趋近于保守，在高 RTT Agent 上表现出仅选择较低码率。</p>
<p>方法上的话，感觉 Policy Discriminator 没有太见过，是一个训练无关性的好方法。</p>
<h1 id="deployment-efficient-reinforcement-learning-via-model-based-offline-optimization">Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization</h1>
<h2 id="abstract-introduction-1">Abstract &amp; Introduction</h2>
<p>离线 RL 的问题在于在线和环境交互可能导致较大开销，为了解决这一问题，主要的策略分为：</p>
<ul>
<li>限制 agent 的行为，防止其做出错误举动。这里 BCQ 就采用了这样的做法（但是 BCQ 论文实在是需要慢慢看）</li>
<li>强化 agent 的 exploitation 能力，令其最大化利用有限的数据</li>
</ul>
<p>本文的成果是提出了衡量 RL 算法的一个指标，即 deployment efficiency，其基于了 RL 学习过程中数据采集策略发生变化的次数（因为策略发生变化意味着需要一次新的部署）。完全离线的 RL 算法如 BCQ、BRAC 具有很高的 deployment efficiency，而完全在线的 RL 算法如 DDPG、SAC 则较低。</p>
<p>论文特别指出，deployment efficiency 和已有的 data efficiency 并不完全类似，这里后者衡量了 RL 学习过程中采集数据的次数。这意味着，即使 data efficiency 很低（数据采样次数极大），只要收集数据的策略没变，那么 deployment efficiency 依然较高。</p>
<p>目前大部分的离线 RL 算法都在大数据集上进行训练和评测，以保证 deployment efficiency 和 data efficiency 都很高。但实际上我们注意到由于 extrapolation error，这种简单直接的算法其实并不优越。而对于在线算法，虽然可以克服数据集数据分布不符合真实数据分布的问题，但实际上会限制 policy 的表达能力【Question #1：这里没有特别明白】。</p>
<p>此外，本文还提出了 BREMEN，即 Behavior Regularized Model ENsemble。</p>
<div class="note info no-icon"><p>We propose Behavior-Regularized Model-ENsemble (BREMEN), which learns an ensemble of dynamics models in conjunction with a policy using imaginary rollouts while implicitly regularizing the learned policy via appropriate parameter initialization and conservative trust-region learning updates.</p>
</div>
<p>自然，BREMEN 在高维连续决策空间上也表现很好。</p>
<h2 id="preliminaries">Preliminaries</h2>
<p>首先定义 MDP <span class="math inline">\((\mathcal{S}, \mathcal{A}, p, r, \gamma)\)</span>，而 policy <span class="math inline">\(\pi\)</span> 定义了 agent 的行为，<span class="math inline">\(\pi\)</span> 接受一个状态，输出一个状态空间 <span class="math inline">\(\mathcal{A}\)</span> 上的概率分布，表示 agent 做出给定行为的概率。我们的目标是获取下述最优 policy：</p>
<p><span class="math display">\[
\pi^* = \mathop{\rm argmax}_\pi \eta[\pi] = \mathop{\rm argmax}_\pi \mathbb{E}_\pi\left[\sum_{t = 0}^{+\infty} \gamma^tr(s_t)\right]
\]</span></p>
<p>而所谓 model-based 的 RL 方法，就需要对 MDP 中的 <span class="math inline">\(p(s&#39; \mid s, a)\)</span> 建模估计。</p>
<p>RL 的学习过程中包含两种行为，分别为获取 MDP 转移对（即 deployment）与更新 policy 参数（即 learning）。如果学习过程中每次更新参数后均将收集到的数据舍弃，那么该学习就是 on-policy 的。反之，如果不断积累数据形成 replay dataset <span class="math inline">\(\mathcal{D}\)</span>，那么该学习就是 off-policy 的，因为用于更新 policy 的数据并不一定通过当前 policy 收集。</p>
<p>但是上述的学习过程都是 online 的，因为其 deployment 和 learning 是混合进行的，而纯 offline 算法中 agent 不会直接与环境进行任何的交互，agent 只能从固定的数据集之中学习。当然，介于 online 和 offline 之间有 semi-batch RL，其实现了较高的 deployment efficiency，当然这种方法并没有完全研究过。</p>
<h2 id="deployment-efficiency">Deployment efficiency</h2>
<p>这里主要阐述这样的观点，即对于 online RL，无论其是 on-policy 还是 off-policy，在一次迭代之后，必然会部署新的 policy 并获取数据，这显然是 deployment unefficient 的。相反，纯 offline RL 只会学习一次部署得到的数据。而一个理想的平衡算法应当位于这两个极端之间。</p>
<p>论文认为现在的 RL 算法忽视了 deployment efficiency，并且现行的部分 SOTA 算法甚至需要百万量级的部署次数（如 SAC）。本文提出了只需要五次到十次部署即可有效学习的算法。</p>
<h2 id="behavior-regularized-model-ensemble">Behavior regularized model ensemble</h2>
<h3 id="imaginary-rollout-from-model-ensemble">Imaginary rollout from model ensemble</h3>
<p>为了解决 model bias 的问题，BREMEN 使用了包含 <span class="math inline">\(K\)</span> 个 deterministic dynamic model 的 <span class="math inline">\(\hat f_\phi := \left\{\hat f_{\phi_1}, \cdots, \hat f_{\phi_K}\right\}\)</span>，其中 <span class="math inline">\(\phi_i\)</span> 表示模型参数。这些模型的训练方式都是最小化下述均方误差，即 model 预测的后续状态和数据集 <span class="math inline">\(\mathcal{D}\)</span> 指示的后续状态：</p>
<p><span class="math display">\[
\min_{\phi_i} \frac{1}{|\mathcal{D}|} \sum_{(s_t, a_t; s_{t + 1}) \in \mathcal{D}} \frac{1}{2} \left\|s_{t + 1} - \hat f_{\phi_i}(s_t, a_t)\right\|_2^2
\]</span></p>
<p>在实际训练 policy <span class="math inline">\(\pi_\theta\)</span> 的时候，会从这 <span class="math inline">\(K\)</span> 个 model 中随机选择一个来提供下一状态：</p>
<p><span class="math display">\[
a_t \sim \pi_\theta(\cdot \mid \hat s_t), \hat s_{t + 1} = \hat f_{\phi_t}(\hat s_t, a_t), i \sim \{1, 2, \cdots, K\}
\]</span></p>
<h3 id="policy-update-with-behavior-regularization">Policy update with behavior regularization</h3>
<p>这里训练具体的 policy 的时候，需要使用置信区间约束来解决 distribution shift 的问题，即在每一次部署之后都使用 behavior clone 得到的 policy <span class="math inline">\(\hat\pi_\beta\)</span> 对 <span class="math inline">\(\pi_\theta\)</span> 重新初始化。具体而言，在每一次部署之后我们都能得到一个更新过的数据集 <span class="math inline">\(\mathcal{D}\)</span>，从而我们使用 BC 对真实的 policy <span class="math inline">\(\pi_b\)</span> 进行估计。具体的训练方式为下述，这里论文作了 fixed variance 假设：</p>
<p><span class="math display">\[
\min_\beta \frac{1}{|\mathcal{D}|} \sum_{(s_t, a_t) \in \mathcal{D}} \frac{1}{2} \left\|a_t - \hat\pi_\beta(s_t)\right\|_2^2
\]</span></p>
<div class="note info no-icon"><p>好像 behavior clone 和 distribution shift 都是模仿学习的东西，这里还真的不太会，后面补。</p>
<p>一个可能有用的 <a target="_blank" rel="noopener" href="https://wensun.github.io/CS4789_data/Imitation_Learning_April_8_annotated.pdf">slide</a>。</p>
<hr />
<p>似乎学明白了一点东西，模仿学习是一种改进的 RL，其面对的是无法合理得到 reward signal 的 RL 环境，但是其可以获得部分专家数据，那么我们就可以模仿专家数据进行学习，这就是 behavior clone。</p>
<p>behavior clone 的基本思想借鉴了监督学习。例如我们获得了专家数据集 <span class="math inline">\(\mathcal{D}\)</span>，那么我们的训练方式就是：</p>
<p><span class="math display">\[
\theta^* := \mathop{\rm argmin}_\theta \mathbb{E}_{(s, a) \sim \mathcal{D}} \mathcal{L}(\pi_\theta(s), a)
\]</span></p>
<p>这里 <span class="math inline">\(\mathcal{L}\)</span> 是 loss，可以设定为负 reward 等。</p>
<p>但这个方法有很多问题，包括在数据集较小的时候学习效率极低、无法分辨真正导致 reward 上升的行为等，但是更为致命的问题是 distribution shift。</p>
<p>这个问题指的是专家数据和实际数据分布不一致，尤其指训练集和测试集分布之间有偏差。由于我们参考了监督学习的思想，behavior clone 实际上也继承了监督学习的一个假设，即数据集分布和真实数据分布一致。但是，即使这个假设并不严格成立，监督学习也是可以忽略的，因为监督学习的各个数据之间独立，可以忽略。然而 RL 是逐步决策的，某一步的偏差可能导致后续误差的累积导致后续决策的极大偏移。</p>
<p>有一种解决方式是数据聚合，这里就和论文无关了，故不做讨论。</p>
</div>
<p>而通过 <span class="math inline">\(\hat\pi_\beta\)</span> 重新初始化 <span class="math inline">\(\pi_\theta\)</span> 的方式为正态分布初始化，均值设定为 <span class="math inline">\(\hat\pi_\beta\)</span> 而标准差置 <span class="math inline">\(1\)</span>。论文认为在这里将 BC 和普通的梯度下降相结合可以使得 <span class="math inline">\(\pi_\theta\)</span> 趋向于新数据集 <span class="math inline">\(\mathcal{D}\)</span> 代表的 policy，可以认为是一种 distribution shift 问题的补救措施。</p>
<p>为了进一步补救，其进一步使用 KL-based trust region optimization。总体上 BREMEN 的 policy 更新策略就是：</p>
<p><span class="math display">\[
\theta_{k + 1} = \mathop{\rm argmax}_\theta \mathbb{E}_{(s, a) \sim (\pi_{\theta_k}, \hat f_{\phi_i})} \left[\frac{\pi_\theta(a \mid s)}{\pi_{\theta_k}(a \mid s)} A^{\pi_{\theta_k}}(s, a)\right]
\]</span></p>
<p>这里要求：</p>
<p><span class="math display">\[
\mathbb{E}_{s \sim (\pi_{\theta_k}, \hat f_{\phi_i})} D_{\rm KL}(\pi_\theta(\cdot \mid s) \parallel \pi_{\theta_k}(\cdot \mid s)) \leq \delta
\]</span></p>
<p>此外定义初始值：</p>
<p><span class="math display">\[
\pi_{\theta_0} := \mathcal{N}(\hat\pi_\beta, 1)
\]</span></p>
<p>得到的 BREMEN 算法流程为：</p>
<p><img src="/uploads/paper-2022-09/6.png" /></p>
<p>总体来看，简化了部署的过程，然后使用类似 TRPO 中信任域的方式约束了 agent 的行为。</p>
<h2 id="experiments">Experiments</h2>
<p>既然 BREMEN 的优势在于少量部署条件下能高效学习，其第一个实验就是和其余 RL 在约束部署次数的条件下比较 reward，效果是明显的：</p>
<p><img src="/uploads/paper-2022-09/7.png" /></p>
<p>这里每一条竖直虚线都代表一次部署，部署次数约束在五次到十次之间，每次 batch size 约束在 200k 到 100k。可见 BREMEN 明显优于大部分算法，而几乎仅劣于完全在线的 SAC，但是 SAC 可能需要近百万量级的部署次数。</p>
<p>此外，为了测试 data efficiency，论文采用的方法是训练一个 SAC 后用该 policy 采集一个 dataset 并用该 dataset 训练各种离线算法，根据下述结果可以看出 BREMEN 能在很小的数据集下就能超越 BC 的 baseline，而其余类似 BCQ 等的方法甚至无法超越 baseline，这说明了 BREMEN 不仅 deployment efficient 而且是 data efficient 的：</p>
<p><img src="/uploads/paper-2022-09/8.png" /></p>
<h2 id="final-analysis-1">Final analysis</h2>
<p>说实话由于 RL 基础还是有点欠缺，自己不是很了解信任域相关方法的数学基础，所以对其数学论证也没看很细致，尤其是涉及到 implicit KL regularization 的部分。</p>
<p>但总体而言这篇文章提出的核心思想是需要考虑部署带来的开销，并且提出了利用 BC 等方法来强化模型的 exploitation 能力并且尝试使用信任域的想法约束 agent 的行为，总体上依然符合离线 RL 的两大思路。</p>
<p>数学证明之类的可能真的要等到补补 TRPO 之类 RL 方法之后写，目前打算基于 Sutton 把博客里面这些有关 RL 的一些东西串起来。</p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/09/14/phd-interview/" rel="prev" title="直博招生专业面试的一些">
                  <i class="fa fa-chevron-left"></i> 直博招生专业面试的一些
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/09/25/note-of-ctrl/" rel="next" title="《现代控制技术》学习笔记">
                  《现代控制技术》学习笔记 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments"><div id="twikoo-comments"></div></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ashitemaru</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">424k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">6:26</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://unpkg.com/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{&quot;enable&quot;:true,&quot;theme&quot;:&quot;default&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;unpkg.com&#x2F;mermaid@8.9.3&#x2F;dist&#x2F;mermaid.min.js&quot;,&quot;integrity&quot;:&quot;sha256-OyJHvRcZHaRR6Ig73ppxF4QXk8HzvfgTprRWkulCkfY&#x3D;&quot;}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;unpkg.com&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;,&quot;integrity&quot;:&quot;sha256-ncNI9OXOS5Ek4tzVYiOMmN&#x2F;KKCPZ6V0Cpv2P&#x2F;zHntiA&#x3D;&quot;}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="twikoo" type="application/json">{&quot;enable&quot;:true,&quot;visitor&quot;:true,&quot;envId&quot;:&quot;https:&#x2F;&#x2F;vercel-deploy-two.vercel.app&quot;,&quot;el&quot;:&quot;#twikoo-comments&quot;}</script>
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.twikoo.el)
    .then(() => NexT.utils.getScript(
      'https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js',
      { condition: window.twikoo }
    ))
    .then(() => {
      twikoo.init(CONFIG.twikoo);
    });
});
</script>

</body>
</html>
