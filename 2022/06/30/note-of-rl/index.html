<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic%7CNoto+Serif+KR:300,300italic,400,400italic,700,700italic%7CMS+PMincho:300,300italic,400,400italic,700,700italic%7CFira+Code:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext" referrerpolicy="no-referrer">

<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://unpkg.com/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;ashitemaru.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Muse&quot;,&quot;version&quot;:&quot;8.5.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;width&quot;:320,&quot;display&quot;:&quot;always&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:true,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:&quot;disqusjs&quot;,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;manual&quot;,&quot;top_n_per_article&quot;:1,&quot;unescape&quot;:false,&quot;preload&quot;:false}}</script><script src="/js/config.js"></script>
<meta name="description" content="目前实验室的项目和强化学习关系比较大，然而自己确实没有什么强化学习的基础。 实验室的学长似乎有一个不错的强化学习入门学习仓库，感觉可以照着这个慢慢学来。 \[ \newcommand{\b}{\boldsymbol} \newcommand{\argmax}{\mathop{\rm argmax}} \newcommand{\argmin}{\mathop{\rm argmin}} \newcom">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习入门学习笔记">
<meta property="og:url" content="https://ashitemaru.github.io/2022/06/30/note-of-rl/index.html">
<meta property="og:site_name" content="Ashitemaru">
<meta property="og:description" content="目前实验室的项目和强化学习关系比较大，然而自己确实没有什么强化学习的基础。 实验室的学长似乎有一个不错的强化学习入门学习仓库，感觉可以照着这个慢慢学来。 \[ \newcommand{\b}{\boldsymbol} \newcommand{\argmax}{\mathop{\rm argmax}} \newcommand{\argmin}{\mathop{\rm argmin}} \newcom">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/note-of-rl/1.png">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/note-of-rl/2.png">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/note-of-rl/3.png">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/note-of-rl/4.png">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/note-of-rl/5.png">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/note-of-rl/6.png">
<meta property="article:published_time" content="2022-06-30T20:44:56.000Z">
<meta property="article:modified_time" content="2022-06-30T20:44:56.000Z">
<meta property="article:author" content="Ashitemaru">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ashitemaru.github.io/uploads/note-of-rl/1.png">


<link rel="canonical" href="https://ashitemaru.github.io/2022/06/30/note-of-rl/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;ashitemaru.github.io&#x2F;2022&#x2F;06&#x2F;30&#x2F;note-of-rl&#x2F;&quot;,&quot;path&quot;:&quot;2022&#x2F;06&#x2F;30&#x2F;note-of-rl&#x2F;&quot;,&quot;title&quot;:&quot;强化学习入门学习笔记&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>强化学习入门学习笔记 | Ashitemaru</title>
  



<link rel="stylesheet" href="https://www.unpkg.com/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://www.unpkg.com/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://www.unpkg.com/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
<script src="https://www.unpkg.com/twikoo@1.4.1/dist/twikoo.all.min.js"></script>
<script src="/js/jquery.min.js"></script>
<script>
function getURL(e) {
    var http = e.slice(0, 4)
    var https = e.slice(0, 5)
    if (http == "http" || https == "https") {
        return e
    } else if (e == "" || e == null || e == undefined) {
        return e
    } else {
        e = 'http://' + e
        return e
    }
}

function newComment() {
    twikoo.getRecentComments({
        envId: 'https://vercel-deploy-two.vercel.app',
        pageSize: 10,
        includeReply: false
    }).then(function (res) {
        var hotComments = $("#hot-comments");
        for (var i = 0; i < res.length; i++) {
            var nick = res[i].nick;
            var content = res[i].commentText;
            var newcontent = content.substring(0, 50);
            var url = res[i].url;
            var avatar = res[i].avatar;
            var link = getURL(res[i].link);
            var updatedAt = res[i].relativeTime;
            var commentId = '#' + res[i].id;
            hotComments.append(
                `<li class="px1 pb2 flex items-center">
                    <img style="width:40px;height:40px" class="circle mx1 listavatar" src="${avatar}">
                    <div style="display:flex;flex-direction:column;width:100%;">
                        <div style="display:flex;justify-content:space-between;flex-direction:row;align-items:center;">
                            <div class="h5 listauthor overflow-hidden" title="${nick}">
                                <a target="_blank" rel="noopener external nofollow noreferrer" href="${link}">${nick}</a>
                            </div>
                            <div class="h6 mr1 listdate wenzi hang1" style="color:#777777;">${updatedAt}</div>
                        </div>
                        <div style="display:flex;flex-direction:row;width:100%;">
                            <a class="h5 list-comcontent" style="overflow:hidden;display:flex;border-bottom:0px;text-overflow:ellipsis;line-height:1.5;text-align:left" href="${url}${commentId}">${newcontent}</a>
                        </div>
                    </div>
                </li>`
            );
        }
    }).catch(function (err) {
        console.error(err);
    });
}

function replaceRuby() {
    $('code')
        .filter((_, node) => {
            var list = $(node).text().split(' ');
            return list.length === 3 && list[0] === "@";
        })
        .replaceWith((_, text) => {
            var list = text.split(' ');
            var written = list[1];
            var read = list[2];
            return $(`<ruby>${written}<rp>(</rp><rt>${read}</rt><rp>)</rp></ruby>`);
        });
}

$(function () {
    newComment();
    replaceRuby();
});
</script>

<!-- CSS -->
<link href="/css/app.min.css" rel="stylesheet">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Ashitemaru</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">11</span></a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">51</span></a></li>
        <li class="menu-item menu-item-skill-docs"><a href="https://sast-skill-docers.github.io/sast-skill-docs/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i>技能引导文档</a></li>
        <li class="menu-item menu-item-thuinfo"><a href="https://www.thuinfo.net/" rel="noopener" target="_blank"><i class="fa fa-sitemap fa-fw"></i>THUInfo</a></li>
        <li class="menu-item menu-item-se-index"><a href="https://thuse-course.github.io/course-index/" rel="noopener" target="_blank"><i class="fa fa-calendar fa-fw"></i>软工课程主页</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">1.</span> <span class="nav-text">强化学习的基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%9A%84%E5%88%86%E7%B1%BB"><span class="nav-number">1.1.</span> <span class="nav-text">强化学习方法的分类</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#q-learning"><span class="nav-number">2.</span> <span class="nav-text">Q Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="nav-number">2.1.</span> <span class="nav-text">代码示例</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#sarsa"><span class="nav-number">3.</span> <span class="nav-text">Sarsa</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B-1"><span class="nav-number">3.1.</span> <span class="nav-text">代码示例</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#sarsa-lambda"><span class="nav-number">4.</span> <span class="nav-text">Sarsa lambda</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B-2"><span class="nav-number">4.1.</span> <span class="nav-text">代码示例</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#deep-q-network-dqn"><span class="nav-number">5.</span> <span class="nav-text">Deep Q Network (DQN)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B-3"><span class="nav-number">5.1.</span> <span class="nav-text">代码示例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dqn-enhancement"><span class="nav-number">5.2.</span> <span class="nav-text">DQN Enhancement</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#double-dqn"><span class="nav-number">5.2.1.</span> <span class="nav-text">Double DQN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#prioritized-experience-replay"><span class="nav-number">5.2.2.</span> <span class="nav-text">Prioritized Experience Replay</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#policy-gradients"><span class="nav-number">6.</span> <span class="nav-text">Policy Gradients</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B-4"><span class="nav-number">6.1.</span> <span class="nav-text">代码示例</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#actor-critic"><span class="nav-number">7.</span> <span class="nav-text">Actor Critic</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E6%8E%A8%E7%90%86"><span class="nav-number">7.1.</span> <span class="nav-text">数学推理</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#advantage-actor-critic-a2c"><span class="nav-number">8.</span> <span class="nav-text">Advantage Actor Critic (A2C)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#bellman-equation"><span class="nav-number">8.1.</span> <span class="nav-text">Bellman equation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B-5"><span class="nav-number">8.2.</span> <span class="nav-text">代码示例</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#deep-deterministic-policy-gradients-ddpg"><span class="nav-number">9.</span> <span class="nav-text">Deep Deterministic Policy Gradients (DDPG)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B-6"><span class="nav-number">9.1.</span> <span class="nav-text">代码示例</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#asynchronous-advantage-actor-critic-a3c"><span class="nav-number">10.</span> <span class="nav-text">Asynchronous Advantage Actor Critic (A3C)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#trust-region-policy-optimization-trpo"><span class="nav-number">11.</span> <span class="nav-text">Trust Region Policy Optimization (TRPO)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#kl-%E6%95%A3%E5%BA%A6"><span class="nav-number">11.1.</span> <span class="nav-text">KL 散度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#off-policy-policy-gradients"><span class="nav-number">11.2.</span> <span class="nav-text">Off-policy Policy gradients</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ashitemaru"
      src="/uploads/avatar.png">
  <p class="site-author-name" itemprop="name">Ashitemaru</p>
  <div class="site-description" itemprop="description">A cat that likes Sakana.</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">51</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Ashitemaru" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Ashitemaru" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:qhd19@mails.tsinghua.edu.cn" title="E-Mail → mailto:qhd19@mails.tsinghua.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/Ashitemaru" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;Ashitemaru" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Friends
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.unidy.cn/" title="https:&#x2F;&#x2F;www.unidy.cn" rel="noopener" target="_blank">UNIDY</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.dilant.cn/" title="https:&#x2F;&#x2F;www.dilant.cn" rel="noopener" target="_blank">Dilant</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://zcy.moe/" title="https:&#x2F;&#x2F;zcy.moe" rel="noopener" target="_blank">猫猫</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://c7w.tech/" title="https:&#x2F;&#x2F;c7w.tech" rel="noopener" target="_blank">c7w</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.leenldk.top/" title="http:&#x2F;&#x2F;www.leenldk.top&#x2F;" rel="noopener" target="_blank">leenldk (20)</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://sun80449.github.io/" title="https:&#x2F;&#x2F;sun80449.github.io" rel="noopener" target="_blank">lcr</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://pwe.cat/" title="https:&#x2F;&#x2F;pwe.cat" rel="noopener" target="_blank">索尔</a>
        </li>
    </ul>
  </div>
<div class="sidebar-1 mybox relative">
    <div class="p2">
        <i class="fab fa-facebook-messenger mr1"></i>
        Latest Comments
    </div>
    <div id="hot-comments"></div>
</div>
          </div>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/Ashitemaru" class="github-corner" title="Come here for fun." aria-label="Come here for fun." rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ashitemaru.github.io/2022/06/30/note-of-rl/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.png">
      <meta itemprop="name" content="Ashitemaru">
      <meta itemprop="description" content="A cat that likes Sakana.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ashitemaru">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          强化学习入门学习笔记
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-30 20:44:56" itemprop="dateCreated datePublished" datetime="2022-06-30T20:44:56+00:00">2022-06-30</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2022-06-30 20:44:56" itemprop="dateModified" datetime="2022-06-30T20:44:56+00:00">2022-06-30</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%AE%A1%E7%AE%97%E6%9C%BA/" itemprop="url" rel="index"><span itemprop="name">【学习笔记】计算机</span></a>
        </span>
    </span>

  
    <span id="/2022/06/30/note-of-rl/" class="post-meta-item twikoo_visitors" data-flag-title="强化学习入门学习笔记" title="阅读次数">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="twikoo_visitors"></span>
    </span>
  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>35k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>32 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>目前实验室的项目和强化学习关系比较大，然而自己确实没有什么强化学习的基础。</p>
<p>实验室的学长似乎有一个不错的强化学习入门学习仓库，感觉可以照着这个慢慢学来。</p>
<p><span class="math display">\[
\newcommand{\b}{\boldsymbol}
\newcommand{\argmax}{\mathop{\rm argmax}}
\newcommand{\argmin}{\mathop{\rm argmin}}
\newcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\opE}{\mathop{\mathbb{E}}}
\newcommand{\relmiddle}[1]{\mathrel{}\middle#1\mathrel{}}
\]</span></p>
<span id="more"></span>
<p>本笔记也有参考代码 Repo，虽然大部分是从别人那边搬过来的，但是我自己还是稍微做了一些调整，增加了一些可读性然后修复了一些 deprecated 的问题。</p>
<p>代码 Repo 的地址：<a target="_blank" rel="noopener" href="https://github.com/Ashitemaru/RL-tutorial">Reinforce learning code repository</a>。</p>
<h1 id="强化学习的基本概念">强化学习的基本概念</h1>
<p>强化学习和常见的监督学习有一定的类似，但是也有区别。监督学习我们给出的监督往往是数据集上的标签，但是强化学习我们能给出的提示是给机器的行为作出评分。即，在强化学习中，我们会有一个不断给机器的行为评分的监督者，我们希望机器能够通过这个评分监督者学习到如何作出高分行为。</p>
<p>监督学习的标签往往需要事先由人类标注出来，也就是说监督学习学习的是人类标签的模式，但是强化学习不一定需要事先标注，他需要学习我们给出的评分策略的模式。</p>
<h2 id="强化学习方法的分类">强化学习方法的分类</h2>
<p>根据强化学习方法是否需要理解环境，可以将强化学习算法分为 model-based 和 model-free 两类。</p>
<p>model-free 方法不需要机器去理解环境，他每一步决策都需要等待真实环境的反馈才能进行下一步。而 model-based 方法可以通过 model 来模拟现实环境，这样机器可以通过这个模拟现实环境的 model 来预测自己的行为对环境的影响，甚至分析行为不同分支的影响，从而做到对现实环境的理解。</p>
<p>model-free 方法包括 Q Learning、Sarsa、Policy gradients。</p>
<hr />
<p>根据强化学习方法最后选择行为的依据，可以将强化学习算法分为基于概率（policy-based）和基于价值（value-based）两类。</p>
<p>基于概率的强化学习算法是最自然的，其会根据其感官接收到的信息，给出其下一步各个行为的概率，然后根据这个概率随机选择下一步的行为。这些算法包括 Policy gradients。</p>
<p>基于价值的强化学习算法则会测算各个行动的价值，并且选择价值最高的作为自己下一步的行为。这类算法包括 Sarsa、Q Learning。</p>
<p>基于概率的方法的一个优势在于能够根据概率分布作出连续动作的选择，而基于价值的方法不得不在选择一步动作后才能作出下一步动作的价值评估。</p>
<p>当然现在也有将两者结合起来的算法，即 Actor-Critic 方法，这个方法内 Actor 会基于概率给出动作，而 Critic 会给出价值评估，这样的话能够在 Policy gradients 的基础上加快训练速度。</p>
<hr />
<p>根据其更新参数的策略，可以将强化学习算法分为回合制更新（Monte-Carlo update）和单步更新（temporal difference update）。</p>
<p>Monte-Carlo learning 和基础版本的 Policy gradients 方法都是回合制更新的，然而 Q Learning、Sarsa 和升级版本的 Policy gradients 都会基于更为现实和有效的单步更新。</p>
<hr />
<p>另外还有在线算法（on policy）和离线算法（off policy）的区别。离线算法的优势就是我可以在脱离交互环境的条件下，学习先前的交互记录。而在线算法必须要在交互环境内更新参数。</p>
<p>最典型的在线算法是 Sarsa 和强化过后的 Sarsa lambda。而典型的离线算法为 Q Learning 和强化过的 Deep Q network。</p>
<h1 id="q-learning">Q Learning</h1>
<p>首先引入 Q 表这个概念，Q 表可以看作一个函数 <span class="math inline">\(Q(s, a)\)</span>，这个函数值的含义为在状态 <span class="math inline">\(s\)</span> 的条件下，作出行为 <span class="math inline">\(a\)</span> 的潜在收益。而 Q Learning 的决策过程就是选择当前状态的所有可选行为中潜在收益最高的行为。</p>
<p>Q 表的更新是在进行决策之后开始的，如果我们原先在状态 <span class="math inline">\(s\)</span>，并且我们通过行为 <span class="math inline">\(a\)</span> 到达新状态 <span class="math inline">\(s&#39;\)</span>，这个时候我们需要重新估算 <span class="math inline">\(Q(s, a)\)</span> 的值。这个估计值新增的部分应当包含两个部分，即我们采取这个行为立刻能得到的奖励和我们在 <span class="math inline">\(s&#39;\)</span> 状态处能够达到的最大可能收益。</p>
<p>我们的更新策略如下：</p>
<p><span class="math display">\[
Q&#39;(s, a) := Q(s, a) + \alpha\left[r + \gamma\max_{a&#39; \in \Gamma(s&#39;)} Q(s&#39;, a&#39;) - Q(s, a)\right]
\]</span></p>
<p>这里 <span class="math inline">\(\Gamma(s&#39;)\)</span> 表示在状态 <span class="math inline">\(s&#39;\)</span> 处能够选择的所有行为构成的集合，<span class="math inline">\(\alpha\)</span> 表示<strong>学习率</strong>。</p>
<p>这样我们就能看到调整估计值的逻辑。我们首先获取在新状态 <span class="math inline">\(s&#39;\)</span> 处所有行为可能得到的最大收益 <span class="math inline">\(\max_{a&#39; \in \Gamma(s&#39;)} Q(s&#39;, a&#39;)\)</span>，乘以一个衰减 <span class="math inline">\(\gamma\)</span>，之后加上采取行为 <span class="math inline">\(a\)</span> 后立刻能得到的收益 <span class="math inline">\(r\)</span> 计算出行为 <span class="math inline">\(a\)</span> 的潜在收益。之后我们将这个估计值和原先的值作差得到差距，乘以学习率 <span class="math inline">\(\alpha\)</span> 之后就得到了我们需要更新到 <span class="math inline">\(Q(s, a)\)</span> 上的增量。</p>
<p>实际使用的 Q Learning 为了防止完全贪心算法导致算法无法充分探索环境，故在作出决策的时候其实也不一定完全按照最高价值选择，而是可能通过 epsilon greedy 的策略。即提前设定一个 <span class="math inline">\(\varepsilon \in (0, 1)\)</span>，以 <span class="math inline">\(\varepsilon\)</span> 概率按照 Q 表最优价值选择行为，以 <span class="math inline">\(1 - \varepsilon\)</span> 概率随机选择行为。这里 <span class="math inline">\(\varepsilon\)</span> 也被称为<strong>贪婪度</strong>。</p>
<p>在学习过程的初期，我们希望机器能够随机探索环境，所以此时 <span class="math inline">\(\varepsilon\)</span> 参数会设定较小。而后期我们在已经具有较为可靠的 Q 表并且希望得到最优解的时候，就可以适当调整到较高的 <span class="math inline">\(\varepsilon\)</span>。</p>
<hr />
<p>如果我们分析参数 <span class="math inline">\(\gamma\)</span> 在这个学习过程中的作用，我们注意到第一步的奖励会以 <span class="math inline">\(\gamma\)</span> 衰减，而第二步的奖励会以 <span class="math inline">\(\gamma^2\)</span> 衰减，以此类推，越远的奖励衰减的次数越高。</p>
<p>这就说明，如果令 <span class="math inline">\(\gamma = 0\)</span>，就意味着除了最近的直接奖励，其余奖励均会清空，此时机器只会关注最近的直接奖励。而如果令 <span class="math inline">\(\gamma = 1\)</span>，则意味着机器会平等地对待所有可能的奖励。而一般条件下 <span class="math inline">\(0 &lt; \gamma &lt; 1\)</span>，此时机器会按照指数衰减的方式对待从近到远的奖励。所以 <span class="math inline">\(\gamma\)</span> 也被称为<strong>奖励衰减指数</strong>。</p>
<h2 id="代码示例">代码示例</h2>
<p>这里在 <code>$&#123;repo&#125;/Q-learning</code> 下写了一个简单的通过 Q Learning 学习走迷宫的小程序，这里稍微介绍一下。</p>
<p>其实主要关注主函数之中的主循环就可以知道 Q Learning 是如何进行的了：</p>
<figure class="highlight python"><figcaption><span>Python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># $&#123;repo&#125;/Q-learning/main.py</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>():</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="comment"># Initial state</span></span><br><span class="line">        state = env.reset()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># Refresh the canvas</span></span><br><span class="line">            env.render()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL choose action based on state</span></span><br><span class="line">            action = RL.choose_action(<span class="built_in">str</span>(state))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL take action and get next state and reward</span></span><br><span class="line">            next_state, reward, done = env.step(action)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL learn from this transition</span></span><br><span class="line">            RL.learn(<span class="built_in">str</span>(state), action, reward, <span class="built_in">str</span>(next_state))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Swap state, move ahead</span></span><br><span class="line">            state = next_state</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Break while loop when end of this episode</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># End of game</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Game over&quot;</span>)</span><br><span class="line">    env.destroy()</span><br></pre></td></tr></table></figure>
<p>这里的 <code>RL</code> 相当于我们学习认识环境的机器，其具备的接口包括选择下一步行动（即 <code>RL.choose_action</code>）和根据环境的反馈做出学习（即 <code>RL.learn</code>）。而 <code>env</code> 则模拟了机器需要去认识的环境，其实质上是一个方形地图，地图上有两个地狱和一个代表最终结果的天堂，机器应该尝试着去学习一个绕过地狱到达天堂的路径。</p>
<p>其实主逻辑十分简单。我们需要训练 100 个 epoch，对于每个 epoch 我们首先初始化环境和机器所在的位置，然后在主循环中重复进行下述动作：</p>
<ul>
<li>令机器选择一个行为</li>
<li>将这个行为传递给环境，令环境给出反馈（包括奖励函数、是否终止、机器即将转到的状态）</li>
<li>机器通过环境的反馈进行学习</li>
<li>令机器采取该行为</li>
</ul>
<p>这里机器选择一个行为即我们提到的 epsilon greedy 策略，而其学习方式就是 Q 表的更新策略也已经叙述过。环境给出反馈则是简单判定机器是否走到了地狱或者天堂，并给出相应的结果。</p>
<p>只需要这样简单的代码，我们就可以看见一个红色方块仅依靠着环境的反馈学习到了如何登上天堂。</p>
<h1 id="sarsa">Sarsa</h1>
<p>Sarsa 和 Q Learning 的策略选择方面是一致的，也就是使用 epsilon greedy 策略选择自己下一步实际的路径，但是 Sarsa 在更新 Q 表的策略上和 Q Learning 是不同的。</p>
<p>我们观察 Q Learning 的更新策略：</p>
<p><span class="math display">\[
Q&#39;_{\rm QL}(s, a) := Q(s, a) + \alpha\left[r + \gamma{\color{red} \max_{a&#39; \in \Gamma(s&#39;)} Q(s&#39;, a&#39;)} - Q(s, a)\right]
\]</span></p>
<p>而 Sarsa 的更新策略为：</p>
<p><span class="math display">\[
Q&#39;_{\rm Sarsa}(s, a) := Q(s, a) + \alpha\left[r + \gamma{\color{red} Q(s&#39;, {\rm EG}(s&#39;))} - Q(s, a)\right]
\]</span></p>
<p>这里 <span class="math inline">\(a = {\rm EG}(s)\)</span> 表示的是在状态 <span class="math inline">\(s\)</span> 处使用 epsilon greedy 策略选择行为 <span class="math inline">\(a\)</span>。</p>
<p>除去这里更新策略的不同之外，Sarsa 在到达新状态 <span class="math inline">\(s&#39;\)</span> 后决定此时需要采取的行动的时候，就会采取之前更新策略中的 <span class="math inline">\({\rm EG}(s&#39;)\)</span> 直接作为下一步。</p>
<hr />
<p>Sarsa 的策略意味着，其在评估新状态 <span class="math inline">\(s&#39;\)</span> 的潜在收益的时候，并不会像 Q Learning 一样贪心选择最大的可能收益，而是更为现实地选择下一步行为，并且一定会采取这一步行为。</p>
<p>而 Q Learning 这里使用最大值贪心的更新策略也意味着其会完全沿着局部最优的路径前行，而无视可能造成的负面效果。Sarsa 则相对而言较为实际且保守。</p>
<p>另外，Q Learning 由于可以脱离环境，所以是 off policy 的。而 Sarsa 需要边转移状态边更新 Q 表，所以是 on policy 的。</p>
<h2 id="代码示例-1">代码示例</h2>
<p>这里不具体展示代码了，代码位于 <code>$&#123;repo&#125;/Sarsa</code> 目录之下。</p>
<p>需要注意和 Q Learning 算法不同的地方在于，在主循环里面我们除了需要保存一个 <code>state</code> 变量，我们还需要一个 <code>action</code> 变量保存即将需要采取的动作。</p>
<p>如果比对两种算法的效率，其实可以发现 Sarsa 算法的收敛速度明显慢于 Q Learning 算法。</p>
<h1 id="sarsa-lambda">Sarsa lambda</h1>
<p>Sarsa lambda 相对于 Sarsa 的优化点在于，机器最初探索环境的时候所采取的那些行为很有可能和最终结果无关，但是在经典的 Sarsa 算法中，每一步都会以同等的权重更新 Q 表，这很有可能导致后续真正导致较高奖励函数的行为没有匹配到较高权重。</p>
<p>所以我们可以采取近似于回合制更新的方式取代经典的 Sarsa 单步更新策略。我们令到达最终结果的最后一步以原始权重更新 Q 表，倒数第二步需要乘以常数 <span class="math inline">\(\lambda\)</span> 再更新到 Q 表，以此类推，倒数第 <span class="math inline">\(n\)</span> 步需要以权重 <span class="math inline">\(\lambda^{n - 1}\)</span> 更新到 Q 表。</p>
<p>我们可以注意到 <span class="math inline">\(\lambda = 0\)</span> 的时候，只有最后一步被更新到 Q 表，这就是最经典的回合更新。而 <span class="math inline">\(\lambda = 1\)</span> 的时候就是经典的 Sarsa 单步更新。</p>
<p>这里的 <span class="math inline">\(\lambda\)</span> 就被称为<strong>路径衰减指数</strong>。</p>
<h2 id="代码示例-2">代码示例</h2>
<p>这里只需要在 Sarsa 的基础上加上一个 E 表来累计每一步的 Q 表更新值即可，每走一步就需要将 E 表以 <span class="math inline">\(\lambda\)</span> 权重衰减一次。</p>
<p>主要观察 E 表在模型学习过程中的作用：</p>
<figure class="highlight python"><figcaption><span>Python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># $&#123;repo&#125;/Sarsa/model.py - class SarsaLambdaTable</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, state, action, reward, next_state, next_action</span>):</span></span><br><span class="line">    self.check_state_exist(next_state)</span><br><span class="line"></span><br><span class="line">    q_predict = self.q_table.loc[state, action]</span><br><span class="line">    <span class="keyword">if</span> next_state != <span class="string">&quot;terminal&quot;</span>:</span><br><span class="line">        q_target = reward + self.gamma * self.q_table.loc[next_state, next_action]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        q_target = reward</span><br><span class="line">        </span><br><span class="line">    error = q_target - q_predict</span><br><span class="line"></span><br><span class="line">    self.e_table.loc[state, :] *= <span class="number">0</span></span><br><span class="line">    self.e_table.loc[state, action] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    self.q_table += self.lr * error * self.e_table</span><br><span class="line">    self.e_table *= self.gamma * self.lambda_ <span class="comment"># Decay</span></span><br></pre></td></tr></table></figure>
<p>前半部分和经典 Sarsa 是完全一致的，但是注意后续的两个更新。在说明具体更新之前，我们可以说明 E 表的含义，E 表实际上就是将具体值更新到 Q 表时的权重表。</p>
<p>首先是将当前状态对应的 E 表行清零并将当前动作对应的列设为 <code>1</code>，即更新 E 表。之后以当前权重将误差更新到 Q 表，最后对权重作出衰减。</p>
<div class="note info"><p>我觉得他这个代码框架存在一个问题，就是当机器走到角落的时候，实际上它的决策空间会变小，但是代码里面没有体现这一点，只是简单的让机器不动。比如机器在左上角选择向左走，那么只是简单地让机器停在左上角。</p>
<p>这是一个并不好的处理，因为如果此时机器还没有碰巧到达天堂产生正向激励的话，机器完全会缩在角落里来避免地狱的反向激励。</p>
<p>有时间给他代码改改吧。</p>
</div>
<h1 id="deep-q-network-dqn">Deep Q Network (DQN)</h1>
<p>在我们面对更大的解空间的时候，存储过大的 Q 表会导致内存不够使用，所以此时我们需要令 <span class="math inline">\(Q(s, a)\)</span> 使用其他的方法计算，比如说我们可以引入深度神经网络来计算 <span class="math inline">\(Q(s, a)\)</span>。</p>
<p>这里我们可以引入这样的一个深度神经网络，该网络接受的输入是当前机器的状态 <span class="math inline">\(s\)</span>，该网络给出的输出则是各种可行的行为以及其评估值，我们只需要按照最基本的 Q Learning 原则根据神经网络的输出给出。</p>
<p>在这个给定的决策过程下，我们需要给出神经网络的训练策略。实际上我们只需要给出网络 loss 的计算方式就可以。我们思考 Q Learning 之中的训练策略，我们可以得知，在经典的 Q Learning 算法中我们会求出估计的 Q 值，将其与现实的 Q 值做差之后乘以学习率叠加到原先的 Q 表上。</p>
<p>而如果将这样的策略和神经网络联系起来，我们会发现估计的 Q 值和现实的 Q 值之间的差距实际上就表现出了 loss，我们只需要将这个 loss 对网络各个参数的梯度乘以学习率叠加到网络上就可以。</p>
<p>基于这样的认识，我们给出下述 loss 计算，使用平方误差的基本思想：</p>
<p><span class="math display">\[
\mathcal{L} := \left[r + \gamma\max_{a&#39; \in \Gamma(s&#39;)} Q(s&#39;, a&#39;) - Q(s, a)\right]^2
\]</span></p>
<p>这里在状态 <span class="math inline">\(s\)</span> 下选择动作 <span class="math inline">\(a\)</span> 会转移到状态 <span class="math inline">\(s&#39;\)</span>。</p>
<hr />
<p>另外，我们可以使用两种方法强化 DQN，这两种方法包括 Experience replay 和 Fixed Q targets。</p>
<p>前者实际上基于 Q Learning 的 off policy 特性，我们可以使用以往的训练数据乃至其他机器的经验数据来训练网络。</p>
<p>后者的含义是使用两个同样结构但是参数不同的网络，一个使用较早的参数来给出估计的 Q 值，而且这个网络的参数几乎不会在训练过程中更新，这个网络代表了过去的经验。一个使用较新的参数来给出现实的 Q 值，这个网络会在训练过程中更新参数并且会被应用到实际场景之中。这样做的原因是取消数据的相关性和提高训练的稳定性，具体的原因后续研讨。</p>
<p>在引入了这两个强化方法之后，loss 的具体计算方式会有所不同，具体的讨论见下方。</p>
<h2 id="代码示例-3">代码示例</h2>
<p>这里我们使用 Tensorflow 实现 DQN，这里我们使用的 DQN 具体算法描述为：</p>
<ul>
<li>首先初始化一个容量为 <span class="math inline">\(N\)</span> 的记忆库 <span class="math inline">\(D\)</span> 用于存放先前的经验，这里 <span class="math inline">\(N\)</span> 是先前设定的超参</li>
<li>随机初始化两个结构一致的神经网络 <span class="math inline">\(Q, \hat Q\)</span>，其初始化参数分别为 <span class="math inline">\(\b\theta, \b\theta^-\)</span>，初始化的时候保证 <span class="math inline">\(\b\theta = \b\theta^-\)</span></li>
<li>对模型按照下述流程训练 <span class="math inline">\(M\)</span> 个 epoch，这里 <span class="math inline">\(M\)</span> 为先前设定的超参
<ul>
<li>假定初始状态 <span class="math inline">\(s_1\)</span>，初始化 <span class="math inline">\(s = s_1\)</span></li>
<li>重复下述流程直到需要中断
<ul>
<li>以超参 <span class="math inline">\(\varepsilon\)</span> 使用 epsilon greedy 策略选取行为 <span class="math inline">\(a := \argmax_{a&#39;} Q(s, a&#39;; \b\theta)\)</span></li>
<li>令 <span class="math inline">\(r\)</span> 表示在状态 <span class="math inline">\(s\)</span> 的条件下执行行为 <span class="math inline">\(a\)</span> 得到的直接收益</li>
<li>令 <span class="math inline">\(s&#39;\)</span> 表示在状态 <span class="math inline">\(s\)</span> 的条件下执行行为 <span class="math inline">\(a\)</span> 转移到的状态</li>
<li>将描述状态转移的元组 <span class="math inline">\((s, a, r; s&#39;)\)</span> 存入 <span class="math inline">\(D\)</span></li>
<li>从 <span class="math inline">\(D\)</span> 中抽取 <span class="math inline">\(B\)</span> 组数据回放，更新 <span class="math inline">\(\b\theta\)</span>，具体流程见下述，这里 <span class="math inline">\(B\)</span> 是先前设定的超参</li>
<li>如果此时 <span class="math inline">\(s&#39;\)</span> 是终止状态，则终止流程，否则令 <span class="math inline">\(s \leftarrow s&#39;\)</span>，转移状态</li>
<li>每经过 <span class="math inline">\(C\)</span> 步，令 <span class="math inline">\(\b\theta^- \leftarrow \b\theta\)</span>，即令 <span class="math inline">\(\hat Q\)</span> 更新至 <span class="math inline">\(Q\)</span>，这里 <span class="math inline">\(C\)</span> 是先前设定的超参</li>
</ul></li>
</ul></li>
</ul>
<p>阐述具体学习过程之前，我们研讨一下引入了 Experience replay 和 Fixed Q targets 之后 loss 的计算。首先由于引入了两个神经网络，计算估计 Q 值和现实 Q 值的网络参数不一致，上述已经通过 <span class="math inline">\(\b\theta, \b\theta^-\)</span> 作出区分。</p>
<p>另外，由于引入了记忆库 <span class="math inline">\(D\)</span> 并且需要从中随机抽取转移对，那么最后的 loss 应该是基于 <span class="math inline">\(D\)</span> 上的均匀分布的期望，所以最后的 loss 为：</p>
<p><span class="math display">\[
\mathcal{L}_{\rm DQN}(\b\theta) := \opE_{(s, a, r; s&#39;) \sim U(D)} \left[r + \gamma\max_{a&#39; \in \Gamma(s&#39;)} \hat Q(s&#39;, a&#39;; \b\theta^-) - Q(s, a; \b\theta)\right]^2
\]</span></p>
<p>具体的学习流程为：</p>
<ul>
<li>从 <span class="math inline">\(D\)</span> 中抽取 <span class="math inline">\(B\)</span> 组转移元组</li>
<li>对每一个元组 <span class="math inline">\((s, a, r; s&#39;)\)</span> 计算 loss</li>
<li>计算梯度将 loss 反向传播到 <span class="math inline">\(\b\theta\)</span> 参数上，完成一次参数更新</li>
<li>根据具体情况调整 epsilon greedy 参数 <span class="math inline">\(\varepsilon\)</span></li>
</ul>
<p>这里使用的网络的数据依赖关系表现为：</p>
<p><img src="/uploads/note-of-rl/1.png" /></p>
<p>现在关注具体的代码实现，我们需要注意到我们采用的网络是两层全连接层：</p>
<figure class="highlight python"><figcaption><span>Python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># $&#123;repo&#125;/DQN/dqn.py - def _build_net</span></span><br><span class="line"></span><br><span class="line">self.state = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_features], name = <span class="string">&quot;state&quot;</span>)</span><br><span class="line">...</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;eval_net&quot;</span>):</span><br><span class="line">    eval_layer = tf.keras.layers.Dense(</span><br><span class="line">        units = <span class="number">20</span>,</span><br><span class="line">        activation = tf.nn.relu,</span><br><span class="line">        kernel_initializer = w_initializer,</span><br><span class="line">        bias_initializer = b_initializer,</span><br><span class="line">        name = <span class="string">&quot;eval_layer&quot;</span></span><br><span class="line">    )(self.state) <span class="comment"># Output: (None, 20)</span></span><br><span class="line">    self.q_eval = tf.keras.layers.Dense(</span><br><span class="line">        units = self.n_actions,</span><br><span class="line">        kernel_initializer = w_initializer,</span><br><span class="line">        bias_initializer = b_initializer,</span><br><span class="line">        name = <span class="string">&quot;q_eval_layer&quot;</span></span><br><span class="line">    )(eval_layer) <span class="comment"># Output: (None, n_actions)</span></span><br></pre></td></tr></table></figure>
<p>其逻辑是接受一个 <code>(batch_size, n_features)</code> 形状的输入，产生一个 <code>(batch_size, n_actions)</code> 形状的输出。<code>n_features</code> 表示需要多少个特征描述一个状态 <span class="math inline">\(s\)</span>，即状态空间维数。最后的输出则是各个动作的 Q 值。</p>
<p>在这样的网络设计之下，我们在求取 loss 的时候需要将 <code>(batch_size, n_actions)</code> 形状的输出中每一行挑出我们实际上选取的行为的 Q 值，压缩为 <code>(batch_size, )</code> 形状的 Q 值向量，再求取平方误差。所以就有下述代码：</p>
<figure class="highlight python"><figcaption><span>Python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># $&#123;repo&#125;/DQN/dqn.py - def _build_net</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;q_target&quot;</span>):</span><br><span class="line">    q_target = self.reward + self.gamma * tf.reduce_max(self.q_next, axis = <span class="number">1</span>, name = <span class="string">&quot;q_max_next_state&quot;</span>)</span><br><span class="line">    self.q_target = tf.stop_gradient(q_target) <span class="comment"># Shape: (None, )</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;q_eval&quot;</span>):</span><br><span class="line">    action_indices = tf.stack([</span><br><span class="line">        tf.<span class="built_in">range</span>(tf.shape(self.action)[<span class="number">0</span>], dtype = tf.int32), <span class="comment"># Index</span></span><br><span class="line">        self.action <span class="comment"># The index of action</span></span><br><span class="line">    ], axis = <span class="number">1</span>)</span><br><span class="line">    self.q_eval_wrt_action = tf.gather_nd(params = self.q_eval, indices = action_indices) <span class="comment"># Shape: (None, )</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;loss&quot;</span>):</span><br><span class="line">    self.loss = tf.reduce_mean(tf.squared_difference(</span><br><span class="line">        self.q_target, self.q_eval_wrt_action,</span><br><span class="line">        name = <span class="string">&quot;error&quot;</span></span><br><span class="line">    ))</span><br></pre></td></tr></table></figure>
<p>这里 <code>q_target</code> 部分只需要直接求取每一行的最大值就可以，而 <code>q_eval</code> 部分需要根据 <code>self.action</code> 压缩网络的输出，得到 <code>q_eval</code>。最后求取平方误差即可。</p>
<h2 id="dqn-enhancement">DQN Enhancement</h2>
<h3 id="double-dqn">Double DQN</h3>
<p>上述讨论的 DQN 其实有 overestimate 的问题，因为我们每次都会使用旧参数 <span class="math inline">\(\b\theta^-\)</span>，采用最大值函数去估计 evaluated Q value。为了解决这个问题，我们需要利用另外一个网络，即参数及时更新的网络来平衡旧参数网络的估计。</p>
<p>传统 DQN 的 loss 计算为：</p>
<p><span class="math display">\[
\mathcal{L}_{\rm DQN} := \left[r + \gamma{\color{red} \max_{a&#39; \in \Gamma(s&#39;)} \hat Q(s&#39;, a&#39;; \b\theta^-)} - Q(s, a; \b\theta)\right]^2
\]</span></p>
<p>这里 <span class="math inline">\(a&#39;\)</span> 的选择仅仅依赖于 <span class="math inline">\(\hat Q\)</span> 网络的情况，我们在 Double DQN 中可以使用 <span class="math inline">\(Q\)</span> 网络来选择 <span class="math inline">\(a&#39;\)</span> 以控制 Q 值的估计：</p>
<p><span class="math display">\[
\mathcal{L}_{\rm DDQN} := \left[r + \gamma{\color{red} \hat Q\left(s&#39;, \argmax_{a&#39; \in \Gamma(s&#39;)} Q(s&#39;, a&#39;; \b\theta); \b\theta^-\right)} - Q(s, a; \b\theta)\right]^2
\]</span></p>
<p>也就是说这里令 <span class="math inline">\(a&#39;\)</span> 是 <span class="math inline">\(Q\)</span> 网络选取出的在 <span class="math inline">\(s&#39;\)</span> 状态下的最佳行为，而非仅通过 <span class="math inline">\(\hat Q\)</span> 网络选择，这样做的平衡就可以防止 <span class="math inline">\(\hat Q\)</span> 给出过高的 Q 值。</p>
<hr />
<p>这里就不给出具体的代码实现了。</p>
<p><code>UPDATE TODO</code></p>
<h3 id="prioritized-experience-replay">Prioritized Experience Replay</h3>
<p>如果我们的环境的 reward 分配比较不均匀，正向反馈较难获取，但是却需要要求机器重点学习正向反馈，我们原先的 replay 策略就会失效，因为在原先的策略内，我们令各个回忆的权重是一致的，这个分配并不符合实际。</p>
<p>我们可以这样估计每个回忆的优先级，和 loss 的计算类似，我们用估计的 Q 值直接减去现实的 Q 值，如果这个值越大，就说明还能提升的空间就越大，从而说明这个回忆的优先级越高。上述 Q 值的差值一般称为 TD error。</p>
<hr />
<p>每次选取回忆的时候，我们都需要根据 TD error 给出的优先级排序各个回忆，但如果每次都排序的话显然会浪费算力，所以我们需要使用 Sum tree 数据结构组织各个回忆。</p>
<p><code>TODO</code></p>
<h1 id="policy-gradients">Policy Gradients</h1>
<p>Policy Gradients 的特点在于其直接输出下一步行为本身，而不是行为的某种评分。这个特点使得 Policy Gradients 可以应对决策空间无限的情况，而 Q Learning 则无法应对无限决策空间。</p>
<p>Policy Gradients 基于随机策略，也就是我们需要给出在某个状态 <span class="math inline">\(s\)</span> 下我们采取行为 <span class="math inline">\(a\)</span> 的概率，这种随机策略一般用参数 <span class="math inline">\(\b\theta\)</span> 建模。</p>
<p>我们把基于参数为 <span class="math inline">\(\b\theta\)</span> 的随机策略 <span class="math inline">\(\pi_{\b\theta}\)</span>，在状态 <span class="math inline">\(s\)</span> 下，我们采取行为 <span class="math inline">\(a\)</span> 的概率表示为 <span class="math inline">\(\pi_{\b\theta}(a \mid s)\)</span>。这里 <span class="math inline">\(s_t, a_t\)</span> 表示在第 <span class="math inline">\(t\)</span> 时刻的状态和行为，后续有类似的 <span class="math inline">\(r_t\)</span>。</p>
<p>显然这样的 <span class="math inline">\(\pi_{\b\theta}(\cdot\mid s)\)</span> 代表了一个行为决策空间 <span class="math inline">\(\mathcal{A}\)</span> 上的一个分布。</p>
<p>我们首先定义累计收益：</p>
<p><span class="math display">\[
G_t(\b\tau) := \sum_{k = 0}^{+\infty} \gamma^k r_{t + k}
\]</span></p>
<p>这里 <span class="math inline">\(\b\tau\)</span> 是一个已知的转移过程，<span class="math inline">\(G_t(\b\tau)\)</span> 就表示沿着已知的转移过程 <span class="math inline">\(\b\tau\)</span>，从时刻 <span class="math inline">\(t\)</span> 开始能获得的累计收益。<span class="math inline">\(\gamma\)</span> 则是先前介绍过的奖励衰减指数，我们将未来所有的收益全部相加作为总收益。</p>
<p>那么我们可以基于此定义出一个行为评估函数：</p>
<p><span class="math display">\[
Q^{\pi_{\b\theta}}(s, a) := \opE_{\b\tau \sim \pi_{\b\theta}} \left[G_t(\b\tau) \mid s_t = s, a_t = a\right]
\]</span></p>
<p>定义的含义也是直观的，这个评估函数的值就是随机取出一个在第 <span class="math inline">\(t\)</span> 时刻，状态和行为分别为 <span class="math inline">\(s, a\)</span> 的转移过程 <span class="math inline">\(\b\tau\)</span>，求取后续能获取的收益的期望。</p>
<p>另外，由于我们知道给定状态 <span class="math inline">\(s\)</span> 的时候行为 <span class="math inline">\(a\)</span> 的分布就是随机策略 <span class="math inline">\(\pi_{\b\theta}\)</span> 指定的，所以我们可以定义一个不需要给定具体行为的，仅针对状态进行评估的状态函数：</p>
<p><span class="math display">\[
V^{\pi_{\b\theta}}(s) := \int_{a \in \mathcal{A}} \pi_{\b\theta}(a \mid s)Q^{\pi_{\b\theta}}(s, a) {\rm d}a = \opE_{\b\tau \sim \pi_{\b\theta}} \left[G_t(\b\tau) \mid s_t = s\right]
\]</span></p>
<p>此外，我们可以注意到这个随机策略实际上定义了一个在状态空间 <span class="math inline">\(\mathcal{S}\)</span> 上的 Markov chain，那么我们就有下述稳态概率的定义：</p>
<p><span class="math display">\[
d^{\pi_{\b\theta}}(s) := \lim_{t \to +\infty} \P_{\pi_{\b\theta}}(s_t = s \mid s_0)
\]</span></p>
<p>稳态概率实际上描述了在给定初始状态 <span class="math inline">\(s_0\)</span> 的条件下，使用随机策略 <span class="math inline">\(\pi_{\b\theta}\)</span> 在状态空间 <span class="math inline">\(\mathcal{S}\)</span> 上随机转移，最终到达稳定状态的时候，位于状态 <span class="math inline">\(s\)</span> 的概率。</p>
<p>基于稳态概率，我们就可以给一个策略 <span class="math inline">\(\pi_{\b\theta}\)</span> 定义价值：</p>
<p><span class="math display">\[
\mathcal{J}(\b\theta) := \int_{s \in \mathcal{S}} d^{\pi_{\b\theta}}(s)V^{\pi_{\b\theta}}(s) {\rm d}s = \int_{s \in \mathcal{S}} d^{\pi_{\b\theta}}(s) \int_{a \in \mathcal{A}} \pi_{\b\theta}(a \mid s)Q^{\pi_{\b\theta}}(s, a) {\rm d}a{\rm d}s
\]</span></p>
<p>容易看出 <span class="math inline">\(d^{\pi_{\b\theta}}(s)\pi_{\b\theta}(a \mid s)\)</span> 给出了 <span class="math inline">\(\mathcal{S} \times \mathcal{A}\)</span> 上的分布：</p>
<p><span class="math display">\[
\int_{s \in \mathcal{S}}\int_{a \in \mathcal{A}} d^{\pi_{\b\theta}}(s)\pi_{\b\theta}(a \mid s) {\rm d}a{\rm d}s = 1
\]</span></p>
<p>那么：</p>
<p><span class="math display">\[
\mathcal{J}(\b\theta) = \opE_{(s, a) \sim \pi_{\b\theta}} [Q^{\pi_{\b\theta}}(s, a)]
\]</span></p>
<p>为了尽可能提高策略的价值，我们采用梯度上升的方式，所以重点就是计算上述价值函数的梯度 <span class="math inline">\(\nabla_{\b\theta}\mathcal{J}(\b\theta)\)</span>。</p>
<hr />
<p>首先我们对 <span class="math inline">\(Q^{\pi_{\b\theta}}(s, a)\)</span> 做一些简单的展开。</p>
<p>我们只需要遍历 <span class="math inline">\(s\)</span> 所有可能的下一步状态 <span class="math inline">\(s&#39;\)</span>，得到其发生转移 <span class="math inline">\(s \to s&#39;\)</span> 的概率，而相应的价值评估也会拆分为直接收益 <span class="math inline">\(r\)</span> 和接下来到达状态 <span class="math inline">\(s&#39;\)</span> 的收益（与 Q Learning 具有类似的结构）。基于上述思路，得到下述展开：</p>
<p><span class="math display">\[
Q^{\pi_{\b\theta}}(s, a) = \int_{s&#39; \in \mathcal{S}} \P_{\mathcal{E}}(s&#39; \mid s, a)\left[r + V^{\pi_{\b\theta}}(s&#39;)\right] {\rm d} s&#39;
\]</span></p>
<p>这里概率符号带有下标 <span class="math inline">\(\mathcal{E}\)</span>，表示这个概率仅仅依赖于环境 <span class="math inline">\(\mathcal{E}\)</span>，和随机策略 <span class="math inline">\(\pi_{\b\theta}\)</span> 并无关系。那么考虑到 <span class="math inline">\(r, \P_{\mathcal{E}}(s&#39; \mid s, a)\)</span> 相对于 <span class="math inline">\(\b\theta\)</span> 都是常数，可以得到：</p>
<p><span class="math display">\[
\nabla_{\b\theta}Q^{\pi_{\b\theta}}(s, a) = \int_{s&#39; \in \mathcal{S}} \P_{\mathcal{E}}(s&#39; \mid s, a)\nabla_{\b\theta}V^{\pi_{\b\theta}}(s&#39;) {\rm d} s&#39;
\]</span></p>
<p>基于此，我们做出如下推理：</p>
<p><span class="math display">\[
\begin{aligned}
&amp;{\color{red} \nabla_{\b\theta}V^{\pi_{\b\theta}}(s)} \\
=&amp; \nabla_{\b\theta}\left[\int_{a \in \mathcal{A}} \pi_{\b\theta}(a \mid s)Q^{\pi_{\b\theta}}(s, a) {\rm d} a\right] \\
=&amp; \int_{a \in \mathcal{A}}\left[Q^{\pi_{\b\theta}}(s, a)\nabla_{\b\theta}\pi_{\b\theta}(a \mid s) + \pi_{\b\theta}(a \mid s)\nabla_{\b\theta}Q^{\pi_{\b\theta}}(s, a)\right] {\rm d} a \\
=&amp; \int_{a \in \mathcal{A}}\left[Q^{\pi_{\b\theta}}(s, a)\nabla_{\b\theta}\pi_{\b\theta}(a \mid s) + \pi_{\b\theta}(a \mid s)\int_{s&#39; \in \mathcal{S}} \P_{\mathcal{E}}(s&#39; \mid s, a)\nabla_{\b\theta}V^{\pi_{\b\theta}}(s&#39;) {\rm d} s&#39;\right] {\rm d} a \\
=&amp; \int_{a \in \mathcal{A}}Q^{\pi_{\b\theta}}(s, a)\nabla_{\b\theta}\pi_{\b\theta}(a \mid s) {\rm d} a + \int_{a \in \mathcal{A}}\left[\pi_{\b\theta}(a \mid s)\int_{s&#39; \in \mathcal{S}} \P_{\mathcal{E}}(s&#39; \mid s, a) {\color{red} \nabla_{\b\theta}V^{\pi_{\b\theta}}(s&#39;)} {\rm d} s&#39;\right] {\rm d} a
\end{aligned}
\]</span></p>
<p>这里我们观测到了 <span class="math inline">\(\nabla_{\b\theta}V^{\pi_{\b\theta}}(s)\)</span> 结构的重复，下面思考该递归结构。</p>
<p>定义记号：</p>
<p><span class="math display">\[
\P_{\pi_{\b\theta}}(s \mathop{\to}^k s_\bot)
\]</span></p>
<p>表示从初态 <span class="math inline">\(s\)</span> 通过随机策略 <span class="math inline">\(\pi_{\b\theta}\)</span> 经过 <span class="math inline">\(k\)</span> 步转移能够到达终态 <span class="math inline">\(s_\bot\)</span> 的概率。这个概率也具有递归的运算特征，即：</p>
<p><span class="math display">\[
\P_{\pi_{\b\theta}}(s \mathop{\to}^{k + 1} s_\bot) = \int_{s&#39; \in \mathcal{S}} \P_{\pi_{\b\theta}}(s \mathop{\to}^k s&#39;) \P_{\pi_{\b\theta}}(s&#39; \mathop{\to}^1 s_\bot) {\rm d}s&#39;
\]</span></p>
<p>此外，根据该概率的定义，我们能确定：</p>
<p><span class="math display">\[
\P_{\pi_{\b\theta}}(s \mathop{\to}^1 s&#39;) = \int_{a \in \mathcal{A}} \pi_{\b\theta}(a \mid s)\P_{\mathcal{E}}(s&#39; \mid s, a) {\rm d}a
\]</span></p>
<p>另外，为了后续讨论简单，定义记号：</p>
<p><span class="math display">\[
\phi(s) := \int_{a \in \mathcal{A}}Q^{\pi_{\b\theta}}(s, a)\nabla_{\b\theta}\pi_{\b\theta}(a \mid s) {\rm d} a
\]</span></p>
<p>从而观察下述展开：</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\nabla_{\b\theta}V^{\pi_{\b\theta}}(s) \\
=&amp; \phi(s) + \int_{a \in \mathcal{A}}\left[\pi_{\b\theta}(a \mid s)\int_{s&#39; \in \mathcal{S}} \P_{\mathcal{E}}(s&#39; \mid s, a) \nabla_{\b\theta}V^{\pi_{\b\theta}}(s&#39;) {\rm d} s&#39;\right] {\rm d} a \\
=&amp; \phi(s) + \int_{s&#39; \in \mathcal{S}} \left[\int_{a \in \mathcal{A}} \pi_{\b\theta}(a \mid s) \P_{\mathcal{E}}(s&#39; \mid s, a) {\rm d} a\right] \nabla_{\b\theta}V^{\pi_{\b\theta}}(s&#39;) {\rm d} s&#39; \\
=&amp; \phi(s) + {\color{red} \int_{s&#39; \in \mathcal{S}} \P_{\pi_{\b\theta}}(s \mathop{\to}^1 s&#39;) \nabla_{\b\theta}V^{\pi_{\b\theta}}(s&#39;) {\rm d} s&#39;} \\
=&amp; \phi(s) + \int_{s&#39; \in \mathcal{S}} \P_{\pi_{\b\theta}}(s \mathop{\to}^1 s&#39;) \left[\phi(s&#39;) + \int_{s&#39;&#39; \in \mathcal{S}} \P_{\pi_{\b\theta}}(s&#39; \mathop{\to}^1 s&#39;&#39;) \nabla_{\b\theta}V^{\pi_{\b\theta}}(s&#39;&#39;) {\rm d} s&#39;&#39;\right] {\rm d} s&#39; \\
=&amp; \phi(s) + \int_{s&#39; \in \mathcal{S}} \P_{\pi_{\b\theta}}(s \mathop{\to}^1 s&#39;)\phi(s&#39;) {\rm d} s&#39; + \int_{s&#39; \in \mathcal{S}} \P_{\pi_{\b\theta}}(s \mathop{\to}^1 s&#39;) \left[\int_{s&#39;&#39; \in \mathcal{S}} \P_{\pi_{\b\theta}}(s&#39; \mathop{\to}^1 s&#39;&#39;) \nabla_{\b\theta}V^{\pi_{\b\theta}}(s&#39;&#39;) {\rm d} s&#39;&#39;\right]{\rm d} s&#39; \\
=&amp; \phi(s) + \int_{s&#39; \in \mathcal{S}} \P_{\pi_{\b\theta}}(s \mathop{\to}^1 s&#39;)\phi(s&#39;) {\rm d} s&#39; + \int_{s&#39;&#39; \in \mathcal{S}}\left[\int_{s&#39; \in \mathcal{S}} \P_{\pi_{\b\theta}}(s \mathop{\to}^1 s&#39;) \P_{\pi_{\b\theta}}(s&#39; \mathop{\to}^1 s&#39;&#39;){\rm d} s&#39;\right] \nabla_{\b\theta}V^{\pi_{\b\theta}}(s&#39;&#39;) {\rm d} s&#39;&#39; \\
=&amp; \phi(s) + {\color{green} \int_{s&#39; \in \mathcal{S}} \P_{\pi_{\b\theta}}(s \mathop{\to}^1 s&#39;)\phi(s&#39;) {\rm d} s&#39;} + {\color{red} \int_{s&#39;&#39; \in \mathcal{S}} \P_{\pi_{\b\theta}}(s \mathop{\to}^2 s&#39;&#39;) \nabla_{\b\theta}V^{\pi_{\b\theta}}(s&#39;&#39;) {\rm d} s&#39;&#39;}
\end{aligned}
\]</span></p>
<p>我们这一次观察到了如上述的递归现象，红色部分始终保持结构一致，并且每展开一次就会得到类似绿色部分的一个常数项。</p>
<p>注意到：</p>
<p><span class="math display">\[
\P_{\pi_{\b\theta}}(s \mathop{\to}^{+\infty} s_\bot) = 0
\]</span></p>
<p>所以上述展开中红色部分在无穷次递归后会变为零，那么上述式子应当在无穷次递归后仅仅留下绿色的常数项的累加：</p>
<p><span class="math display">\[
\nabla_{\b\theta}V^{\pi_{\b\theta}}(s) = \sum_{k = 0}^{+\infty} \int_{s_\bot \in \mathcal{S}} \P_{\pi_{\b\theta}}(s \mathop{\to}^k s_\bot)\phi(s_\bot) {\rm d} s_\bot
\]</span></p>
<p>根据定义，实际上我们有：</p>
<p><span class="math display">\[
\sum_{k = 0}^{+\infty}\P_{\pi_{\b\theta}}(s \mathop{\to}^k s_\bot) = d^{\pi_{\b\theta}}(s_\bot)
\]</span></p>
<p>那么，考虑到对策略的评估实际上就是对该策略下初态价值的评估，我们得到：</p>
<p><span class="math display">\[
\begin{aligned}
\nabla_{\b\theta}\mathcal{J}(\b\theta) &amp;= \nabla_{\b\theta} V^{\pi_{\b\theta}}(s_0) \\
&amp;= \sum_{k = 0}^{+\infty} \int_{s_\bot \in \mathcal{S}} \P_{\pi_{\b\theta}}(s_0 \mathop{\to}^k s_\bot)\phi(s_\bot) {\rm d} s_\bot \\
&amp;= \int_{s_\bot \in \mathcal{S}} \left[\sum_{k = 0}^{+\infty} \P_{\pi_{\b\theta}}(s_0 \mathop{\to}^k s_\bot)\right] \phi(s_\bot) {\rm d} s_\bot \\
&amp;= \int_{s_\bot \in \mathcal{S}} d^{\pi_{\b\theta}}(s_\bot) \phi(s_\bot) {\rm d} s_\bot \\
&amp;= \int_{s_\bot \in \mathcal{S}} d^{\pi_{\b\theta}}(s_\bot) \int_{a \in \mathcal{A}}Q^{\pi_{\b\theta}}(s, a)\nabla_{\b\theta}\pi_{\b\theta}(a \mid s) {\rm d} a {\rm d} s_\bot
\end{aligned}
\]</span></p>
<p>将 <span class="math inline">\(s_\bot\)</span> 替换回 <span class="math inline">\(s\)</span> 就得到 Policy gradient theorem 的公式：</p>
<p><span class="math display">\[
\nabla_{\b\theta}\mathcal{J}(\b\theta) = \int_{s \in \mathcal{S}}\left[d^{\pi_{\b\theta}}(s) \int_{a \in \mathcal{A}}Q^{\pi_{\b\theta}}(s, a)\nabla_{\b\theta}\pi_{\b\theta}(a \mid s) {\rm d} a\right] {\rm d} s
\]</span></p>
<p>为了让计算机便于计算，我们对上述计算式作出进一步变形：</p>
<p><span class="math display">\[
\begin{aligned}
\nabla_{\b\theta}\mathcal{J}(\b\theta) &amp;= \int_{s \in \mathcal{S}}\left[d^{\pi_{\b\theta}}(s) \int_{a \in \mathcal{A}}Q^{\pi_{\b\theta}}(s, a)\nabla_{\b\theta}\pi_{\b\theta}(a \mid s){\rm d} a\right]{\rm d} s \\
&amp;= \int_{s \in \mathcal{S}}\left[d^{\pi_{\b\theta}}(s) \int_{a \in \mathcal{A}}Q^{\pi_{\b\theta}}(s, a)\pi_{\b\theta}(a \mid s)\frac{\nabla_{\b\theta}\pi_{\b\theta}(a \mid s)}{\pi_{\b\theta}(a \mid s)}{\rm d} a\right]{\rm d} s \\
&amp;= \int_{s \in \mathcal{S}}\int_{a \in \mathcal{A}} (d^{\pi_{\b\theta}}(s)\pi_{\b\theta}(a \mid s)) \cdot (Q^{\pi_{\b\theta}}(s, a)\nabla_{\b\theta}\ln \pi_{\b\theta}(a \mid s)){\rm d} a {\rm d} s \\
&amp;= \opE_{(s, a) \sim \pi_{\b\theta}} [Q^{\pi_{\b\theta}}(s, a)\nabla_{\b\theta}\ln \pi_{\b\theta}(a \mid s)]
\end{aligned}
\]</span></p>
<p>为了计算这个期望，我们可以通过 <span class="math inline">\(\pi_{\b\theta}\)</span> 确定转移过程 <span class="math inline">\(\b\tau\)</span>，之后将转移过程上的所有 <span class="math inline">\((s, a)\)</span> 代入：</p>
<p><span class="math display">\[
\nabla_{\b\theta}\mathcal{J}(\b\theta) = \opE_{\b\tau \sim \pi_{\b\theta}} \frac{1}{|\b\tau|} \sum_{(s, a) \in \b\tau} [Q^{\pi_{\b\theta}}(s, a)\nabla_{\b\theta}\ln \pi_{\b\theta}(a \mid s)]
\]</span></p>
<h2 id="代码示例-4">代码示例</h2>
<p>这里使用 Reinforce 方法实现 Policy Gradients，这是最为基本的一种实现方式，是一种回合制更新策略。</p>
<p>首先我们使用两层全连接层作为随机策略，其参数为 <span class="math inline">\(\b\theta\)</span>，网络整体结构为：</p>
<p><img src="/uploads/note-of-rl/2.png" /></p>
<p>之后我们需要在随机策略 <span class="math inline">\(\pi_{\b\theta}\)</span> 指导下得到的一整个回合信息：</p>
<p><span class="math display">\[
s_1, a_1, r_1; s_2, a_2, r_2; s_3, \cdots; s_T, a_T, r_T; {\rm terminal}
\]</span></p>
<p>在将状态、行为、奖励列表传入网络之前，我们需要对奖励列表根据参数 <span class="math inline">\(\gamma\)</span> 作出衰减并累计，同时将其均值归零标准差归一，这样就完成了 reward 的规范化。</p>
<p>下面考虑我们之前计算的梯度：</p>
<p><span class="math display">\[
\nabla_{\b\theta}\mathcal{J}(\b\theta) = \opE_{\b\tau \sim \pi_{\b\theta}} \frac{1}{|\b\tau|} \sum_{(s, a) \in \b\tau} [G_t\nabla_{\b\theta}\ln \pi_{\b\theta}(a \mid s)]
\]</span></p>
<p>这里上述序列就是一个简单的抽样，我们用该抽样近似表示期望，这样就能够得到更新为：</p>
<p><span class="math display">\[
\nabla_{\b\theta}\mathcal{J}(\b\theta) = \frac{1}{T}\sum_{i = 1}^T [G_i \nabla_{\b\theta}\ln \pi_{\b\theta}(a_i \mid s_i)]
\]</span></p>
<p>考虑到 <span class="math inline">\(G_{i}\)</span> 对 <span class="math inline">\(\b\theta\)</span> 是常数，那么：</p>
<p><span class="math display">\[
\nabla_{\b\theta}\mathcal{J}(\b\theta) = \nabla_{\b\theta}\frac{1}{T}\sum_{i = 1}^T [G_{i}\ln \pi_{\b\theta}(a_i \mid s_i)]
\]</span></p>
<p>梯度是容易实现的，我们只需要计算出下述式子（即目标式）：</p>
<p><span class="math display">\[
\frac{1}{T}\sum_{i = 1}^T [G_{i}\ln \pi_{\b\theta}(a_i \mid s_i)]
\]</span></p>
<p>然后将其传入优化器，交由优化器推算其梯度并附加到 <span class="math inline">\(\b\theta\)</span> 上即可。</p>
<p>基于这个式子，我们观察代码中 loss 的计算：</p>
<figure class="highlight python"><figcaption><span>Python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># $&#123;repo&#125;/Policy-gradients/pg.py - def _build_net</span></span><br><span class="line"></span><br><span class="line">all_action = tf.keras.layers.Dense(...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&quot;loss&quot;</span>):</span><br><span class="line">    <span class="comment"># To maximize total reward (log_p * R) is to minimize -(log_p * R)</span></span><br><span class="line">    <span class="comment"># And the tf only have minimize(loss)</span></span><br><span class="line">    neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">        logits = all_action,</span><br><span class="line">        labels = self.tf_action_list) <span class="comment"># This is negative log of chosen action</span></span><br><span class="line"></span><br><span class="line">    loss = tf.reduce_mean(neg_log_prob * self.tf_reward_list) <span class="comment"># Reward guided loss</span></span><br></pre></td></tr></table></figure>
<p>这里用到了 <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code> 函数，这个函数接受的参数中 <code>logits</code> 形状为 <code>[batch_size, n_action]</code>，<code>labels</code> 形状为 <code>[n_action]</code>，<code>labels</code> 参数中的项代表该 batch 给出的分类结果。返回值的形状则为 <code>[n_action]</code>。</p>
<p>对于每一个 batch，该函数首先将 <code>logits</code> 对应行取 softmax，并取出序号为 <code>labels</code> 参数指定的项，取其负对数作为该 batch 的结果。</p>
<p>比如对于某 batch，其 <code>logits</code> 为 <span class="math inline">\(\{L_j\}_{1 \leq j \leq N}\)</span>，这里 <span class="math inline">\(N\)</span> 为 <code>n_action</code>，而该 batch 的 <code>labels</code> 为 <span class="math inline">\(M(1 \leq M \leq N)\)</span>，那么最后对该 batch，该函数返回：</p>
<p><span class="math display">\[
R = -\ln\left(\frac{e^{L_M}}{\sum_{j = 1}^N e^{L_j}}\right)
\]</span></p>
<p>而我们也发现我们也是通过 softmax 来定义动作的概率：</p>
<figure class="highlight python"><figcaption><span>Python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># $&#123;repo&#125;/Policy-gradients/pg.py - def _build_net</span></span><br><span class="line"></span><br><span class="line">self.all_action_prob = tf.nn.softmax(all_action, name = <span class="string">&quot;action_prob&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>也就是说：</p>
<p><span class="math display">\[
\pi_{\b\theta}(a_i \mid s_i) = \frac{e^{L_{a_i}}}{\sum_{j = 1}^N e^{L_j}}
\]</span></p>
<p>所以 <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code> 的返回值实际上就是下述数列构成的向量：</p>
<p><span class="math display">\[
-\ln\pi_{\b\theta}(a_1 \mid s_1), -\ln\pi_{\b\theta}(a_2 \mid s_2), \cdots, -\ln\pi_{\b\theta}(a_i \mid s_i), \cdots, -\ln\pi_{\b\theta}(a_T \mid s_T)
\]</span></p>
<p>将这个数列和规范化后的 reward 数组逐项相乘，求取所有积的平均值就得到了目标式的相反数。这里求出其相反数的目的是通过优化器最小化负梯度，得到最大化梯度从而实现梯度上升。</p>
<p>这样我们就给出了 Policy gradients 的基本实现。</p>
<h1 id="actor-critic">Actor Critic</h1>
<p>我们注意到 Q Learning 无法处理决策空间无限的问题，而 Policy Gradients 又仅能完成回合更新，效率低。为了结合这两者的优点，这里介绍 Actor Critic 网络。</p>
<p>Actor Critic 的基本思路是，利用一个基于概率的 Actor 网络随机给出行为，利用一个基于价值的 Critic 网络给出 Actor 网络行为的评价。具体而言，Actor 网络不断进行梯度上升，而 Critic 网络会判定本次上升是否合理，其会根据其学习到的规则来衰减本次 Actor 的梯度上升。</p>
<p>单步更新还有一个好处，如果一个完整的转移过程 <span class="math inline">\(\b\tau\)</span> 中既需要上升参数，又需要下降参数，这两个都需要细节学习的内容会因为沿着路径累计而消失，从而两者均没有学习到。</p>
<h2 id="数学推理">数学推理</h2>
<p>这里我们聚焦于 Actor Critic 的基本数学原理，我们思考 Policy Gradients 的梯度计算：</p>
<p><span class="math display">\[
\nabla_{\b\theta}\mathcal{J}(\b\theta) = \opE_{\b\tau \sim \pi_{\b\theta}} \frac{1}{|\b\tau|} \sum_{(s, a) \in \b\tau} [Q^{\pi_{\b\theta}}(s, a)\nabla_{\b\theta}\ln \pi_{\b\theta}(a \mid s)]
\]</span></p>
<p>我们下面将其扩展为 Actor 网络的行为。</p>
<p>现在由于引入了 Critic 网络，所以 Actor 的更新也变成了在线单步更新。也就是说我们在逐探索转移过程，在某一时刻到达状态 <span class="math inline">\(s\)</span>，并且根据 Actor 网络 <span class="math inline">\(\pi_{\b\theta}\)</span> 选择了行为 <span class="math inline">\(a\)</span>，那么此时只需要完成下述单步更新即可：</p>
<p><span class="math display">\[
\b\theta \leftarrow \b\theta + \alpha_{\b\theta}Q^{\b w}(s, a)\nabla_{\b\theta}\ln \pi_{\b\theta}(a \mid s)
\]</span></p>
<p>这里 <span class="math inline">\(\alpha_{\b\theta}\)</span> 为 Actor 网络 <span class="math inline">\(\pi_{\b\theta}\)</span> 的学习率，这里 <span class="math inline">\(\b w\)</span> 是 Critic 网络的参数。</p>
<p>注意到这里 <span class="math inline">\(Q\)</span> 的参数是 <span class="math inline">\(\b w\)</span> 而非 <span class="math inline">\(\b\theta\)</span>，这是因为我们需要将 Actor 的评价权力交给 Critic 网络，用 Critic 的输出来控制 Actor 参数的更新。</p>
<p>而 Critic 的数学原理就是 Q Learning 的 TD Error 思想。观察我们对一个状态的价值评估函数：</p>
<p><span class="math display">\[
V^{\b w}(s) = \opE_{\b\tau \sim \b w} \left[G_t(\b\tau) \mid s_t = s\right]
\]</span></p>
<p>而另外一方面：</p>
<p><span class="math display">\[
G_t(\b\tau) = \sum_{k = 0}^{+\infty} \gamma^k r_{t + k + 1} = r_{t + 1} + \gamma\sum_{k = 0}^{+\infty} \gamma^k r_{t + k + 2} = r_{t + 1} + \gamma G_{t + 1}(\b\tau)
\]</span></p>
<p>所以：</p>
<p><span class="math display">\[
\begin{aligned}
V^{\b w}(s) &amp;= \opE_{\b\tau \sim \b w} \left[G_t(\b\tau) \mid s_t = s\right] \\
&amp;= \opE_{\b\tau \sim \b w} \left[r_{t + 1} + \gamma G_{t + 1}(\b\tau) \mid s_t = s\right] \\
&amp;= \opE_{\b\tau \sim \b w} \left[r_{t + 1} \mid s_t = s\right] + \gamma\opE_{\b\tau \sim \b w}\left[G_{t + 1}(\b\tau) \mid s_t = s\right] \\
&amp;= \opE_{\b\tau \sim \b w} \left[r_{t + 1} \mid s_t = s\right] + \gamma\opE_{\b\tau \sim \b w}\left[G_{t + 1}(\b\tau) \mid s_{t + 1} = s&#39;\right] \\
&amp;= \opE_{\b\tau \sim \b w} \left[r_{t + 1} \mid s_t = s\right] + \gamma V^{\b w}(s&#39;)
\end{aligned}
\]</span></p>
<p>这里 <span class="math inline">\(s&#39;\)</span> 为转移过程中 <span class="math inline">\(s\)</span> 的下一个状态。这说明了 <span class="math inline">\(r_{t + 1} + \gamma V^{\b w}(s&#39;) - V^{\b w}(s)\)</span> 构成了一个无偏估计，从而可以用此修正对 <span class="math inline">\(V^{\b w}\)</span> 的估计。而这里 <span class="math inline">\(r_{t + 1} + \gamma V^{\b w}(s&#39;) - V^{\b w}(s)\)</span> 就被称为 TD Error。</p>
<p>上述推理对 <span class="math inline">\(Q^{\b w}\)</span> 依然有效，与之对应的 <span class="math inline">\(r_{t + 1} + \gamma Q^{\b w}(s&#39;, a&#39;) - Q^{\b w}(s, a)\)</span> 也可以称为 TD Error。</p>
<p>从而 Critic 网络的更新策略为首先计算 TD Error：</p>
<p><span class="math display">\[
\delta_t \leftarrow r_{t + 1} + \gamma Q^{\b w}(s&#39;, a&#39;) - Q^{\b w}(s, a)
\]</span></p>
<p>之后用 TD Error 更新 Critic 网络参数：</p>
<p><span class="math display">\[
\b w \leftarrow \b w + \alpha_{\b w}\delta_t\nabla_{\b w}Q^{\b w}(s, a)
\]</span></p>
<p>这也就是最经典的 Actor Critic 算法。</p>
<h1 id="advantage-actor-critic-a2c">Advantage Actor Critic (A2C)</h1>
<p>我们注意到直接让 Critic 学习 Q 值存在一个问题，那就是 Q 值均值很大，但是波动很小。在均值很大的背景下，较小的波动的分布可能被网络忽略，所以相比较于直接使用 Q 值，我们不妨设立 Baseline 函数，替换为使用 Q 值和 Baseline 的差值。这个差值就是 Advantage 函数。</p>
<p>我们首先需要在数学上论证引入 Baseline 并不影响无偏性，也就是说需要证明：</p>
<p><span class="math display">\[
\begin{aligned}
\nabla_{\b\theta}\mathcal{J}(\b\theta) &amp;= \opE_{\b\tau \sim \pi_{\b\theta}} \frac{1}{|\b\tau|} \sum_{(s, a) \in \b\tau} [Q^{\b w}(s, a)\nabla_{\b\theta}\ln \pi_{\b\theta}(a \mid s)] \\
&amp;= \opE_{\b\tau \sim \pi_{\b\theta}} \frac{1}{|\b\tau|} \sum_{(s, a) \in \b\tau} [(Q^{\b w}(s, a) - b(s)) \nabla_{\b\theta}\ln \pi_{\b\theta}(a \mid s)]
\end{aligned}
\]</span></p>
<p>事实上，我们只需要令 Baseline 函数和行为 <span class="math inline">\(a\)</span> 无关，就可以保证上述关系成立。证明思路为考虑下面的式子，如下，首先展开数学期望：</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\opE_{\b\tau \sim \pi_{\b\theta}} \frac{1}{|\b\tau|} \sum_{(s, a) \in \b\tau} [b(s) \nabla_{\b\theta}\ln \pi_{\b\theta}(a \mid s)] \\
=&amp; \opE_{(s, a) \sim \pi_{\b\theta}} [b(s) \nabla_{\b\theta}\ln \pi_{\b\theta}(a \mid s)] \\
=&amp; \int_{s \in \mathcal{S}} d^{\pi_{\b\theta}}(s) \int_{a \in \mathcal{A}} \pi_{\b\theta}(a \mid s) [b(s) \nabla_{\b\theta}\ln \pi_{\b\theta}(a \mid s)] {\rm d}a {\rm d}s \\
\end{aligned}
\]</span></p>
<p>根据：</p>
<p><span class="math display">\[
\pi_{\b\theta}(a \mid s) \cdot \nabla_{\b\theta}\ln \pi_{\b\theta}(a \mid s) = \nabla_{\b\theta}\pi_{\b\theta}(a \mid s)
\]</span></p>
<p>得到下述：</p>
<p><span class="math display">\[
{\rm LHS} = \int_{s \in \mathcal{S}} d^{\pi_{\b\theta}}(s) \int_{a \in \mathcal{A}} b(s) \nabla_{\b\theta} \pi_{\b\theta}(a \mid s) {\rm d}a {\rm d}s \\
\]</span></p>
<p>考虑到 <span class="math inline">\(b(s)\)</span> 和 <span class="math inline">\(a\)</span> 的无关性，继续展开得到：</p>
<p><span class="math display">\[
\begin{aligned}
{\rm LHS} &amp;= \int_{s \in \mathcal{S}} d^{\pi_{\b\theta}}(s)b(s) \nabla_{\b\theta}\left[\int_{a \in \mathcal{A}} \pi_{\b\theta}(a \mid s) {\rm d}a\right] {\rm d}s \\
&amp;= \int_{s \in \mathcal{S}} d^{\pi_{\b\theta}}(s)b(s) (\nabla_{\b\theta} 1) {\rm d}s \\
&amp;= 0
\end{aligned}
\]</span></p>
<p>据此显然就能得到无偏性依然保持。</p>
<h2 id="bellman-equation">Bellman equation</h2>
<p>我们一般选取的 Baseline 函数就是该状态的评价函数 <span class="math inline">\(V^{\b w}(s)\)</span>，所以此时我们定义 Advantage 函数：</p>
<p><span class="math display">\[
A^{\b w}(s, a) := Q^{\b w}(s, a) - V^{\b w}(s)
\]</span></p>
<p>之前我们得到过下述结论：</p>
<p><span class="math display">\[
Q^{\b w}(s, a) = \int_{s&#39; \in \mathcal{S}} \P_{\mathcal{E}}(s&#39; \mid s, a)\left[r + V^{\b w}(s&#39;)\right] {\rm d}s&#39; = \opE_{s&#39; \sim \b w} \left[r + V^{\b w}(s&#39;)\right]
\]</span></p>
<p>那么我们不妨把 Advantage 函数写为：</p>
<p><span class="math display">\[
A^{\b w}(s, a) := r + V^{\b w}(s&#39;) - V^{\b w}(s)
\]</span></p>
<p>而 Critic 就需要学习 Advantage 函数，相应的 Actor 的梯度计算更新为：</p>
<p><span class="math display">\[
\nabla_{\b\theta}\mathcal{J}(\b\theta) = \opE_{(s, a) \sim \pi_{\b\theta}} [A^{\b w}(s, a)\nabla_{\b\theta}\ln \pi_{\b\theta}(a \mid s)]
\]</span></p>
<h2 id="代码示例-5">代码示例</h2>
<p>这里采用这样的网络结构：</p>
<p><img src="/uploads/note-of-rl/3.png" /></p>
<p>Critic 网络的结构很简单，其接受一个状态，输出这个状态的价值，即 Advantage 函数的作用。而其 loss 的计算即为 TD Error 的平方。这样我们训练 Critic 的方法就很明确，只需要不断在状态转移过程中计算 TD Error 后平方并将其反向传播即可：</p>
<figure class="highlight python"><figcaption><span>Python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># $&#123;repo&#125;/Actor-Critic/ac.py - class Critic def __init__</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;critic_squared_td_error&quot;</span>):</span><br><span class="line">    self.td_error = self.reward + self.gamma * self.next_state_value - self.state_value</span><br><span class="line">    self.loss = tf.square(self.td_error)</span><br></pre></td></tr></table></figure>
<p>Critic 引导 Actor 的方式为将 TD Error 传递给 Actor，与梯度求积后进行反向传播：</p>
<figure class="highlight python"><figcaption><span>Python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># $&#123;repo&#125;/Actor-Critic/ac.py - class Actor def __init__</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;action_value&quot;</span>):</span><br><span class="line">    log_prob = tf.log(self.action_prob[<span class="number">0</span>, self.action])</span><br><span class="line">    <span class="comment"># TD Error - scalar</span></span><br><span class="line">    self.action_value = tf.reduce_mean(log_prob * self.td_error) <span class="comment"># Advantage (TD_error) guided loss</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;actor_train&quot;</span>):</span><br><span class="line">    self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.action_value)</span><br></pre></td></tr></table></figure>
<h1 id="deep-deterministic-policy-gradients-ddpg">Deep Deterministic Policy Gradients (DDPG)</h1>
<p>DDPG 的主要思想是将 DQN 的双网络思想借鉴到 Actor Critic 结构中。在 DDPG 中，Actor 具有两个相同结构的网络，Critic 也具有两个相同结构的网络。</p>
<p>在具体介绍 DDPG 之前，我们首先简单介绍 Deterministic Policy Gradients，即 DPG。这里 Deterministic 的含义就是区别于一般的 Policy Gradients 最后输出的是行为空间 <span class="math inline">\(\mathcal{A}\)</span> 上的概率分布，DPG 会直接给出某个状态下应当采取何种行为。其决策函数我们标记为：</p>
<p><span class="math display">\[
a = \mu_{\b\theta}(s)
\]</span></p>
<p>我们用符号 <span class="math inline">\(\mu\)</span> 表示确定性策略，与随机策略 <span class="math inline">\(\pi\)</span> 区分。</p>
<p>我们首先观察随机策略 <span class="math inline">\(\pi_{\b\theta}\)</span> 下的评价函数表达：</p>
<p><span class="math display">\[
\mathcal{J}_{\pi}(\b\theta) = \int_{s \in \mathcal{S}} d^{\pi_{\b\theta}}(s) \int_{a \in \mathcal{A}} \pi_{\b\theta}(a \mid s)Q^{\pi_{\b\theta}}(s, a) {\rm d}a {\rm d}s
\]</span></p>
<p>两者实际上都表达出了同样的含义，即策略的评价函数就是 Q 值的数学期望：</p>
<p><span class="math display">\[
\mathcal{J}_{\pi}(\b\theta) = \opE_{(s, a) \sim \pi_{\b\theta}} Q^{\pi_{\b\theta}}(s, a)
\]</span></p>
<p>那么在确定性策略下，策略的评价函数应当定义为：</p>
<p><span class="math display">\[
\mathcal{J}_{\mu}(\b\theta) := \opE_{s \sim \mu_{\b\theta}} Q^{\mu_{\b\theta}}(s, \mu_{\b\theta}(s)) = \int_{s \in \mathcal{S}} d^{\mu_{\b\theta}}(s)Q^{\mu_{\b\theta}}(s, \mu_{\b\theta}(s)) {\rm d}s
\]</span></p>
<p>这里梯度的推理则较为简单，直接使用复合函数的导数法则即可：</p>
<p><span class="math display">\[
\begin{aligned}
\nabla_{\b\theta}\mathcal{J}_{\mu}(\b\theta) &amp;= \int_{s \in \mathcal{S}} d^{\mu_{\b\theta}}(s) \nabla_{\b\theta}Q^{\mu_{\b\theta}}(s, \mu_{\b\theta}(s)) {\rm d}s \\
&amp;= \int_{s \in \mathcal{S}} d^{\mu_{\b\theta}}(s) \nabla_{a} \left.Q^{\mu_{\b\theta}}(s, a)\right|_{a = \mu_{\b\theta}(s)} \nabla_{\b\theta}\mu_{\b\theta}(s) {\rm d}s \\
&amp;= \opE_{s \sim \mu_{\b\theta}} \left[\nabla_{a} \left.Q^{\mu_{\b\theta}}(s, a)\right|_{a = \mu_{\b\theta}(s)} \nabla_{\b\theta}\mu_{\b\theta}(s)\right]
\end{aligned}
\]</span></p>
<p>基于此，我们在 DPG 的基础上引入 DQN 的双网络思路。我们记 Actor 网络中在线更新网络的参数为 <span class="math inline">\(\b\theta\)</span>，目标网络的参数为 <span class="math inline">\(\b\theta^-\)</span>。Critic 网络中在线更新网络的参数为 <span class="math inline">\(\b w\)</span>，目标网络的参数为 <span class="math inline">\(\b w^-\)</span>。初始化时应当令 <span class="math inline">\(\b\theta = \b\theta^-, \b w = \b w^-\)</span>。</p>
<p>DDPG 的算法流程完全基于 DQN 算法，这里简单介绍如下：</p>
<ul>
<li>首先初始化一个容量为 <span class="math inline">\(N\)</span> 的记忆库 <span class="math inline">\(D\)</span> 用于存放先前的经验，这里 <span class="math inline">\(N\)</span> 是先前设定的超参</li>
<li>随机初始化结构一致的 Critic 网络 <span class="math inline">\(Q, \hat Q\)</span>，其初始化参数分别为 <span class="math inline">\(\b\theta, \b\theta^-\)</span>，初始化的时候保证 <span class="math inline">\(\b\theta = \b\theta^-\)</span></li>
<li>随机初始化结构一致的 Actor 网络 <span class="math inline">\(\mu, \hat\mu\)</span>，其初始化参数分别为 <span class="math inline">\(\b w, \b w^-\)</span>，初始化的时候保证 <span class="math inline">\(\b w = \b w^-\)</span></li>
<li>对模型按照下述流程训练 <span class="math inline">\(M\)</span> 个 epoch，这里 <span class="math inline">\(M\)</span> 为先前设定的超参
<ul>
<li>初始化随机数发生器 <span class="math inline">\(\mathcal{N}\)</span></li>
<li>假定初始状态 <span class="math inline">\(s_1\)</span>，初始化 <span class="math inline">\(s = s_1\)</span></li>
<li>推演下述状态转移 <span class="math inline">\(T\)</span> 次，这里 <span class="math inline">\(T\)</span> 为先前设定的超参，下述过程的 <span class="math inline">\(t\)</span> 表示目前转移的次数
<ul>
<li>使用 Actor 网络选取行为 <span class="math inline">\(a_t := \mu_{\b\theta}(s_t) + \mathcal{N}(t)\)</span></li>
<li>令 <span class="math inline">\(r_t\)</span> 表示在状态 <span class="math inline">\(s_t\)</span> 的条件下执行行为 <span class="math inline">\(a_t\)</span> 得到的直接收益</li>
<li>令 <span class="math inline">\(s_{t + 1}\)</span> 表示在状态 <span class="math inline">\(s_t\)</span> 的条件下执行行为 <span class="math inline">\(a_t\)</span> 转移到的状态</li>
<li>将描述状态转移的元组 <span class="math inline">\((s_t, a_t, r_t; s_{t + 1})\)</span> 存入 <span class="math inline">\(D\)</span></li>
<li>从 <span class="math inline">\(D\)</span> 中抽取 <span class="math inline">\(B\)</span> 组数据，更新 <span class="math inline">\(\b\theta, \b w\)</span>，具体流程见下述，这里 <span class="math inline">\(B\)</span> 是先前设定的超参</li>
<li>令 <span class="math inline">\(\b\theta^- \leftarrow \tau\b\theta + (1 - \tau)\b\theta^-\)</span>，这里 <span class="math inline">\(\tau\)</span> 是先前设定的超参</li>
<li>令 <span class="math inline">\(\b w^- \leftarrow \tau\b w + (1 - \tau)\b w^-\)</span>，这里 <span class="math inline">\(\tau\)</span> 是先前设定的超参</li>
</ul></li>
</ul></li>
</ul>
<p>具体需要说明的是如何更新 <span class="math inline">\(\b\theta, \b w\)</span>。首先观察 Critic 网络，其几乎完全就是 DQN 更新方式。下述为 DQN 的 loss 计算：</p>
<p><span class="math display">\[
\mathcal{L}_{\rm DQN}(\b\theta) := \opE_{(s, a, r; s&#39;) \sim U(D)} \left[r + \gamma\max_{a&#39; \in \Gamma(s&#39;)} \hat Q(s&#39;, a&#39;; \b\theta^-) - Q(s, a; \b\theta)\right]^2
\]</span></p>
<p>而 DDPG 中 Critic 的 loss 则为：</p>
<p><span class="math display">\[
\mathcal{L}_{\rm DDPG(Critic)}(\b w) := \opE_{(s, a, r; s&#39;) \sim U(D)} \left[r + \gamma\hat Q(s&#39;, \hat\mu_{\b\theta^-}(s&#39;); \b w^-) - Q(s, a; \b w)\right]^2
\]</span></p>
<p>显然，仅仅是将通过贪心获取 <span class="math inline">\(s&#39;\)</span> 状态下的下一行为修改为通过 Actor 网络来获取 <span class="math inline">\(s&#39;\)</span> 状态下的下一行为，这样的设计使得 Critic 更为现实化。</p>
<p>Actor 网络的梯度则先前已经证明：</p>
<p><span class="math display">\[
\nabla_{\b\theta}\mathcal{J}_{\mu}(\b\theta) = \opE_{s \sim \mu_{\b\theta}} \left[\nabla_{a} \left.Q(s, a; \b w)\right|_{a = \mu_{\b\theta}(s)} \nabla_{\b\theta}\mu_{\b\theta}(s)\right]
\]</span></p>
<p>这里仅仅将严格的 Q 值函数替换为 Critic 网络给出的估计值。</p>
<p>可以看出 DDPG 和 DQN 不同的地方仅在于：</p>
<ul>
<li>选取行为是通过 Actor 网络完成的，而非使用 epsilon greedy 策略，使用神经网络一定程度上也保证了行为的随机性</li>
<li>选取行为的时候加入了全局随机数发生器增加探索未知状态的可能</li>
<li>每一回合均更新目标网络的参数，但是使用 Soft update，即通过衰减系数 <span class="math inline">\(\tau\)</span> 进行更新，而非 DQN 中完全同步</li>
<li>loss 的计算融入了 Policy Gradients 的思路</li>
</ul>
<p>这种能够打乱学习顺序的方式，很好地解决了传统 Actor Critic 中由于经常抽样到相同的转移路径导致反复学习无用知识的问题。</p>
<h2 id="代码示例-6">代码示例</h2>
<p>这里的网络结构为：</p>
<p><img src="/uploads/note-of-rl/4.png" /></p>
<p>此时的网络结构图并不一定清晰，所以我们基于代码完成分析。</p>
<p>这里直接把构建网络的代码贴过来：</p>
<figure class="highlight python"><figcaption><span>Python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># $&#123;repo&#125;/DDPG/ddpg.py - class DDPG def __init__</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Placeholders</span></span><br><span class="line">self.state = tf.placeholder(tf.float32, [<span class="literal">None</span>, n_feature], <span class="string">&quot;state&quot;</span>)</span><br><span class="line">self.next_state = tf.placeholder(tf.float32, [<span class="literal">None</span>, n_feature], <span class="string">&quot;next_state&quot;</span>)</span><br><span class="line">self.reward = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>], <span class="string">&quot;reward&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;actor&quot;</span>):</span><br><span class="line">    self.action = self._build_actor(self.state, scope = <span class="string">&quot;eval&quot;</span>, trainable = <span class="literal">True</span>)</span><br><span class="line">    next_action = self._build_actor(self.next_state, scope = <span class="string">&quot;target&quot;</span>, trainable = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;critic&quot;</span>):</span><br><span class="line">    <span class="comment"># Assign self.action = action in memory when calculating Q value for td_error,</span></span><br><span class="line">    <span class="comment"># Otherwise the self.action is from Actor when updating Actor</span></span><br><span class="line">    q_value = self._build_critic(self.state, self.action, scope = <span class="string">&quot;eval&quot;</span>, trainable = <span class="literal">True</span>)</span><br><span class="line">    next_q_value = self._build_critic(self.next_state, next_action, scope = <span class="string">&quot;target&quot;</span>, trainable = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Networks parameters</span></span><br><span class="line">self.actor_eval_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = <span class="string">&quot;actor/eval&quot;</span>)</span><br><span class="line">self.actor_target_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = <span class="string">&quot;actor/target&quot;</span>)</span><br><span class="line">self.critic_eval_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = <span class="string">&quot;critic/eval&quot;</span>)</span><br><span class="line">self.critic_target_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = <span class="string">&quot;critic/target&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Target net replacement</span></span><br><span class="line">self.soft_replace = [</span><br><span class="line">    tf.assign(t, (<span class="number">1</span> - self.tau) * t + self.tau * e)</span><br><span class="line">        <span class="keyword">for</span> t, e <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">            self.actor_target_params + self.critic_target_params,</span><br><span class="line">            self.actor_eval_params + self.critic_eval_params</span><br><span class="line">        )</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">q_target = self.reward + self.gamma * next_q_value</span><br><span class="line">td_error = tf.losses.mean_squared_error(labels = q_target, predictions = q_value)</span><br><span class="line">self.critic_train = tf.train.AdamOptimizer(critic_lr).minimize(td_error, var_list = self.critic_eval_params)</span><br><span class="line"></span><br><span class="line">actor_loss = -tf.reduce_mean(q_value)</span><br><span class="line">self.actor_train = tf.train.AdamOptimizer(actor_lr).minimize(actor_loss, var_list = self.actor_eval_params)</span><br></pre></td></tr></table></figure>
<p>这里 <code>self._build_actor</code> 以及 <code>self._build_critic</code> 是构建 Actor 和 Critic 网络具体结构的函数，这里省略。</p>
<p>可以看到，首先需要构建 Tensorflow Placeholder，之后构建 Actor 的 Eval 和 Target 网络，Critic 的 Eval 和 Target 网络。其中 Actor 网络接受的输入是当前状态，给出的输出是行为选择。而 Critic 网络接受的输入是当前状态和 Actor 网络给出的行为选择，给出的输出是行为的评估值。</p>
<p>之后需要编写软替换算子，直接遍历所有网络参数，对其施以软替换即可。</p>
<p>最后是 loss 的计算。Critic 网络的 loss 较为简单，公式为：</p>
<p><span class="math display">\[
\mathcal{L}_{\rm DDPG(Critic)}(\b w) := \opE_{(s, a, r; s&#39;) \sim U(D)} \left[r + \gamma\hat Q(s&#39;, \hat\mu_{\b\theta^-}(s&#39;); \b w^-) - Q(s, a; \b w)\right]^2
\]</span></p>
<p>而代码中也只是简单将这个公式转换为代码。</p>
<p>之后是 Actor，我们注意到评价函数就是 Q 值的期望，所以只需要把 <code>q_value</code> 求取平均送入优化器求取其梯度即可。</p>
<p>后面的具体运行学习过程的代码和 DQN 类似，从记忆库中抽样，之后将其送入网络运算即可。</p>
<p>实际上 DDPG 就是 AC 和 DQN 代码的结合，基本上的原理并没有很大的差别。</p>
<p>使用上述代码获得的 Reward 曲线为：</p>
<p><img src="/uploads/note-of-rl/5.png" /></p>
<h1 id="asynchronous-advantage-actor-critic-a3c">Asynchronous Advantage Actor Critic (A3C)</h1>
<p>A3C 相较于 A2C 多出的特征是 Asynchronous，即异步。其核心思想是让多个网络尝试不同的行为，获得各类反馈传播到主网络。这样主网络由于接收到多个副网络的更新请求，就不会发生连续更新导致连续学习无用知识的问题，这样就可以抛弃 DQN 的记忆库策略解决这个问题。此外，主网络对副网络的指导也能够加速收敛的速度。</p>
<p>这里并不想过多解释 A3C 代码，因为其核心思路并没有发生变化，这里直接给出训练结果：</p>
<p><img src="/uploads/note-of-rl/6.png" /></p>
<h1 id="trust-region-policy-optimization-trpo">Trust Region Policy Optimization (TRPO)</h1>
<p>TRPO 是引入最后的 DPPO 算法之前，我们需要事先了解的一个简单的算法。</p>
<h2 id="kl-散度">KL 散度</h2>
<p>KL 散度是一种度量两个随机分布之间距离的一种方式，对于空间 <span class="math inline">\(X\)</span> 上的概率分布 <span class="math inline">\(P, Q\)</span>，其 KL 散度定义为：</p>
<p><span class="math display">\[
D_{\rm KL}(P \parallel Q) := \opE_{x \sim P}\left[\ln \frac{P(x)}{Q(x)}\right] = \int_{x \in X} P(x)\ln \frac{P(x)}{Q(x)} {\rm d}x
\]</span></p>
<p>KL 度量的意义在于，如果我们将 <span class="math inline">\(P\)</span> 认为是标准分布，即需要学习的概率分布。而 <span class="math inline">\(Q\)</span> 是机器给出的预测分布，那么 KL 散度实际上给出了当前机器给出的预测和真实分布的距离，机器应当尽可能缩小 KL 散度来逼近真实分布。</p>
<p>可以证明 <span class="math inline">\(D_{\rm KL}(P \parallel Q) = 0\)</span> 当且仅当 <span class="math inline">\(P \equiv Q\)</span>。</p>
<p>这是显然的：</p>
<p><span class="math display">\[
\begin{aligned}
D_{\rm KL}(P \parallel Q) &amp;= \int_{x \in X} P(x)\ln \frac{P(x)}{Q(x)} {\rm d}x = -\int_{x \in X} P(x) \ln \frac{Q(x)}{P(x)} {\rm d}x \\
&amp;\geq -\int_{x \in X} P(x) \left(\frac{Q(x)}{P(x)} - 1\right) {\rm d}x = -\int_{x \in X} (Q(x) - P(x)) {\rm d}x \\
&amp;= 0
\end{aligned}
\]</span></p>
<p>显然等号仅仅在 <span class="math inline">\(P \equiv Q\)</span> 的时候得到。</p>
<p>另外，我们需要说明 KL 散度并不是对称的，也就是说 KL 散度虽然直观上可以理解为一种度量，但是实际上并不满足数学上度量的定义：</p>
<p><span class="math display">\[
D_{\rm KL}(P \parallel Q) \neq D_{\rm KL}(Q \parallel P)
\]</span></p>
<p>在先前提到的机器学习场合下，往往把 <span class="math inline">\(D_{\rm KL}(P \parallel Q)\)</span> 称为前向 KL 散度，而 <span class="math inline">\(D_{\rm KL}(Q \parallel P)\)</span> 则是后向 KL 散度。</p>
<hr />
<p>此外，我们需要说明的是，如果分布 <span class="math inline">\(Q\)</span> 是参数 <span class="math inline">\(\b\theta\)</span> 建模的，那么最小化前向 KL 散度得到的参数 <span class="math inline">\(\b\theta = \argmin_{\b\theta} D_{\rm KL}(P \parallel Q_{\b\theta})\)</span> 等价于极大似然估计。</p>
<p>在此之前，我们需要对机器学习领域中的极大似然估计作出说明。对于参数 <span class="math inline">\(\b\theta\)</span> 建模的，空间 <span class="math inline">\(X\)</span> 上的随机分布 <span class="math inline">\(A\)</span>，我们观测到了样本 <span class="math inline">\(x\)</span>，那么似然函数定义为：</p>
<p><span class="math display">\[
\mathcal{L}(\b\theta) := A_{\b\theta}(x)
\]</span></p>
<p>即似然函数描述了在不同的分布参数之下，我们观测到该样本的概率。那么极大似然估计的思想就是最大化似然函数，作为该样本观测下的参数估计值。</p>
<p>而如果我们观测到了样本列 <span class="math inline">\(\{x_n\} \in X^n\)</span>，那么似然定义为：</p>
<p><span class="math display">\[
\mathcal{L}(\b\theta) := \prod_{k = 1}^n A_{\b\theta}(x_k)
\]</span></p>
<p>机器学习中为了防止连乘下溢，会使用对数似然：</p>
<p><span class="math display">\[
\mathcal{L}&#39;(\b\theta) := \sum_{k = 1}^n \ln A_{\b\theta}(x_k)
\]</span></p>
<p>在这种条件下最大似然估计就是：</p>
<p><span class="math display">\[
\b\theta := \argmax_{\b\theta} \sum_{k = 1}^n \ln A_{\b\theta}(x_k) = \argmax_{\b\theta} \frac{1}{n}\sum_{k = 1}^n \ln A_{\b\theta}(x_k) = \argmax_{\b\theta} \opE_{x \sim \mathcal{D}_A}[\ln A_{\b\theta}(x)]
\]</span></p>
<p>这里 <span class="math inline">\(\mathcal{D}_A\)</span> 是在分布 <span class="math inline">\(A\)</span> 下采样得到的采样分布。</p>
<p>那么回到原先的问题，我们对 KL 散度作变形：</p>
<p><span class="math display">\[
\begin{aligned}
\b\theta &amp;= \argmin_{\b\theta} D_{\rm KL}(P \parallel Q_{\b\theta}) = \argmin_{\b\theta} \opE_{x \sim P}\left[\ln \frac{P(x)}{Q_{\b\theta}(x)}\right] \\
&amp;= \argmin_{\b\theta} \left(\opE_{x \sim P}\left[\ln P(x)\right] - \opE_{x \sim P}\left[\ln Q_{\b\theta}(x)\right]\right) \\
&amp;= \argmin_{\b\theta} \left(-\opE_{x \sim P}\left[\ln Q_{\b\theta}(x)\right]\right) = \argmax_{\b\theta} \opE_{x \sim P}\left[\ln Q_{\b\theta}(x)\right] \\
&amp;\approx \argmax_{\b\theta} \opE_{x \sim \mathcal{D}_P}\left[\ln Q_{\b\theta}(x)\right]
\end{aligned}
\]</span></p>
<p>这里基于 <span class="math inline">\(\mathcal{D}_P \approx P\)</span>，即采样分布近似表示原先分布得到了最后的结果。所以我们证明了最小化 KL 散度得到的参数等价于根据真实分布 <span class="math inline">\(P\)</span> 的样本对分布 <span class="math inline">\(Q\)</span> 作极大似然估计。</p>
<h2 id="off-policy-policy-gradients">Off-policy Policy gradients</h2>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/06/10/paper-2022-08/" rel="prev" title="2022 年 8 月论文笔记">
                  <i class="fa fa-chevron-left"></i> 2022 年 8 月论文笔记
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/07/07/grade-3-spring-conclusion/" rel="next" title="大三春季学期学期总结">
                  大三春季学期学期总结 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments"><div id="twikoo-comments"></div></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ashitemaru</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">398k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">6:02</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://unpkg.com/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{&quot;enable&quot;:true,&quot;theme&quot;:&quot;default&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;unpkg.com&#x2F;mermaid@8.9.3&#x2F;dist&#x2F;mermaid.min.js&quot;,&quot;integrity&quot;:&quot;sha256-OyJHvRcZHaRR6Ig73ppxF4QXk8HzvfgTprRWkulCkfY&#x3D;&quot;}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;unpkg.com&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;,&quot;integrity&quot;:&quot;sha256-ncNI9OXOS5Ek4tzVYiOMmN&#x2F;KKCPZ6V0Cpv2P&#x2F;zHntiA&#x3D;&quot;}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="twikoo" type="application/json">{&quot;enable&quot;:true,&quot;visitor&quot;:true,&quot;envId&quot;:&quot;https:&#x2F;&#x2F;vercel-deploy-two.vercel.app&quot;,&quot;el&quot;:&quot;#twikoo-comments&quot;}</script>
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.twikoo.el)
    .then(() => NexT.utils.getScript(
      'https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js',
      { condition: window.twikoo }
    ))
    .then(() => {
      twikoo.init(CONFIG.twikoo);
    });
});
</script>

</body>
</html>
