<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic%7CMonaco:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://unpkg.com/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;ashitemaru.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.5.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;width&quot;:280,&quot;display&quot;:&quot;always&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:true,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:&quot;disqusjs&quot;,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;manual&quot;,&quot;top_n_per_article&quot;:1,&quot;unescape&quot;:false,&quot;preload&quot;:false}}</script><script src="/js/config.js"></script>
<meta name="description" content="目前实验室的项目和强化学习关系比较大，然而自己确实没有什么强化学习的基础。 实验室的学长似乎有一个不错的强化学习入门学习仓库，感觉可以照着这个慢慢学来。 \[ \newcommand{\b}{\boldsymbol} \newcommand{\argmax}{\mathop{\rm argmax}} \newcommand{\P}{\mathbb{P}} \newcommand{\E}{\math">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习入门学习笔记">
<meta property="og:url" content="https://ashitemaru.github.io/2022/06/30/note-of-rl/index.html">
<meta property="og:site_name" content="Ashitemaru">
<meta property="og:description" content="目前实验室的项目和强化学习关系比较大，然而自己确实没有什么强化学习的基础。 实验室的学长似乎有一个不错的强化学习入门学习仓库，感觉可以照着这个慢慢学来。 \[ \newcommand{\b}{\boldsymbol} \newcommand{\argmax}{\mathop{\rm argmax}} \newcommand{\P}{\mathbb{P}} \newcommand{\E}{\math">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/note-of-rl/1.png">
<meta property="article:published_time" content="2022-06-30T20:44:56.000Z">
<meta property="article:modified_time" content="2022-06-30T20:44:56.000Z">
<meta property="article:author" content="Ashitemaru">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ashitemaru.github.io/uploads/note-of-rl/1.png">


<link rel="canonical" href="https://ashitemaru.github.io/2022/06/30/note-of-rl/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;ashitemaru.github.io&#x2F;2022&#x2F;06&#x2F;30&#x2F;note-of-rl&#x2F;&quot;,&quot;path&quot;:&quot;2022&#x2F;06&#x2F;30&#x2F;note-of-rl&#x2F;&quot;,&quot;title&quot;:&quot;强化学习入门学习笔记&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>强化学习入门学习笔记 | Ashitemaru</title>
  



<link rel="stylesheet" href="https://www.unpkg.com/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://www.unpkg.com/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://www.unpkg.com/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
<script src="https://www.unpkg.com/twikoo@1.4.1/dist/twikoo.all.min.js"></script>
<script src="https://cdn.bootcdn.net/ajax/libs/jquery/3.6.0/jquery.js"></script>
<script>
function getURL(e) {
    http = e.slice(0,4)
    https = e.slice(0,5)
    if (http == "http" || https == "https" ) {
        return e
    } else if (e == "" || e == null || e == undefined) {
        return e
    } else {
        e = 'http://' + e
        return e
    }
}

function newcomment() {
    twikoo.getRecentComments({
        envId: 'https://vercel-deploy-two.vercel.app',
        pageSize: 7,
        includeReply: false
    }).then(function (res) {
        var hotComments = $("#hot-comments");
        for (var i = 0; i < res.length; i++) {
            var nick = res[i].nick;
            var content = res[i].commentText;
            var newcontent = content.substring(0, 50);
            var url = res[i].url;
            var avatar = res[i].avatar;
            var link = getURL(res[i].link);
            var updatedAt = res[i].relativeTime;
            var commentId = '#' + res[i].id;
            hotComments.append(
                `<li class="px1 pb2 flex items-center">
                    <img style="width:40px;height:40px" class="circle mx1 listavatar" src="${avatar}">
                    <div style="display:flex;flex-direction:column;width:100%;">
                        <div style="display:flex;justify-content:space-between;flex-direction:row;align-items:center;">
                            <div class="h5 listauthor overflow-hidden" title="${nick}">
                                <a target="_blank" rel="noopener external nofollow noreferrer" href="${link}">${nick}</a>
                            </div>
                            <div class="h6 mr1 listdate wenzi hang1" style="color:#777777;">${updatedAt}</div>
                        </div>
                        <div style="display:flex;flex-direction:row;width:100%;">
                            <a class="h5 list-comcontent" style="overflow:hidden;display:flex;border-bottom:0px;text-overflow:ellipsis;line-height:1.5;text-align:left" href="${url}${commentId}">${newcontent}</a>
                        </div>
                    </div>
                </li>`
            );
        }
    }).catch(function (err) {
        console.error(err);
    });
}

$(function () {
    newcomment();
});
</script>

<!-- CSS -->
<link href="https://fonts.googleapis.com/css2?family=Mulish&amp;display=swap" rel="stylesheet" media="all" onload="this.media=&quot;all&quot;">
<link href="https://cdn.jsdelivr.net/gh/heson10/pic@master/css/app.min.css" rel="stylesheet">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Ashitemaru</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">11</span></a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">31</span></a></li>
        <li class="menu-item menu-item-skill-docs"><a href="https://sast-skill-docers.github.io/sast-skill-docs/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i>技能引导文档</a></li>
        <li class="menu-item menu-item-thuinfo"><a href="https://www.thuinfo.net/" rel="noopener" target="_blank"><i class="fa fa-sitemap fa-fw"></i>THUInfo</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">1.</span> <span class="nav-text">强化学习的基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%9A%84%E5%88%86%E7%B1%BB"><span class="nav-number">1.1.</span> <span class="nav-text">强化学习方法的分类</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#q-learning"><span class="nav-number">2.</span> <span class="nav-text">Q Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="nav-number">2.1.</span> <span class="nav-text">代码示例</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#sarsa"><span class="nav-number">3.</span> <span class="nav-text">Sarsa</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B-1"><span class="nav-number">3.1.</span> <span class="nav-text">代码示例</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#sarsa-lambda"><span class="nav-number">4.</span> <span class="nav-text">Sarsa lambda</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B-2"><span class="nav-number">4.1.</span> <span class="nav-text">代码示例</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#deep-q-network-dqn"><span class="nav-number">5.</span> <span class="nav-text">Deep Q Network (DQN)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B-3"><span class="nav-number">5.1.</span> <span class="nav-text">代码示例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dqn-enhancement"><span class="nav-number">5.2.</span> <span class="nav-text">DQN Enhancement</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#double-dqn"><span class="nav-number">5.2.1.</span> <span class="nav-text">Double DQN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#prioritized-experience-replay"><span class="nav-number">5.2.2.</span> <span class="nav-text">Prioritized Experience Replay</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#policy-gradients"><span class="nav-number">6.</span> <span class="nav-text">Policy Gradients</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B-4"><span class="nav-number">6.1.</span> <span class="nav-text">代码示例</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ashitemaru"
      src="/uploads/avatar.png">
  <p class="site-author-name" itemprop="name">Ashitemaru</p>
  <div class="site-description" itemprop="description">A cat that likes Sakana.</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">31</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Ashitemaru" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Ashitemaru" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:qhd19@mails.tsinghua.edu.cn" title="E-Mail → mailto:qhd19@mails.tsinghua.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Friends
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.unidy.cn/" title="https:&#x2F;&#x2F;www.unidy.cn" rel="noopener" target="_blank">UNIDY</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.dilant.cn/" title="https:&#x2F;&#x2F;www.dilant.cn" rel="noopener" target="_blank">Dilant</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://zcy.moe/" title="https:&#x2F;&#x2F;zcy.moe" rel="noopener" target="_blank">猫猫</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://c7w.tech/" title="https:&#x2F;&#x2F;c7w.tech" rel="noopener" target="_blank">c7w</a>
        </li>
    </ul>
  </div>
<div class="sidebar-1 mybox relative">
    <div class="p2">
        <i class="fab fa-facebook-messenger mr1"></i>
        Latest Comments
    </div>
    <div id="hot-comments"></div>
</div>
          </div>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/Ashitemaru" class="github-corner" title="Come here for fun." aria-label="Come here for fun." rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ashitemaru.github.io/2022/06/30/note-of-rl/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.png">
      <meta itemprop="name" content="Ashitemaru">
      <meta itemprop="description" content="A cat that likes Sakana.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ashitemaru">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          强化学习入门学习笔记
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-30 20:44:56" itemprop="dateCreated datePublished" datetime="2022-06-30T20:44:56+00:00">2022-06-30</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2022-06-30 20:44:56" itemprop="dateModified" datetime="2022-06-30T20:44:56+00:00">2022-06-30</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%AE%A1%E7%AE%97%E6%9C%BA/" itemprop="url" rel="index"><span itemprop="name">【学习笔记】计算机</span></a>
        </span>
    </span>

  
    <span id="/2022/06/30/note-of-rl/" class="post-meta-item twikoo_visitors" data-flag-title="强化学习入门学习笔记" title="阅读次数">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="twikoo_visitors"></span>
    </span>
  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>16k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>15 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>目前实验室的项目和强化学习关系比较大，然而自己确实没有什么强化学习的基础。</p>
<p>实验室的学长似乎有一个不错的强化学习入门学习仓库，感觉可以照着这个慢慢学来。</p>
<p><span class="math display">\[
\newcommand{\b}{\boldsymbol}
\newcommand{\argmax}{\mathop{\rm argmax}}
\newcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\opE}{\mathop{\mathbb{E}}}
\newcommand{\relmiddle}[1]{\mathrel{}\middle#1\mathrel{}}
\]</span></p>
<span id="more"></span>
<p>本笔记也有参考代码 Repo，虽然大部分是从别人那边搬过来的，但是我自己还是稍微做了一些调整，增加了一些可读性然后修复了一些 deprecated 的问题。</p>
<p>代码 Repo 的地址：<a target="_blank" rel="noopener" href="https://github.com/Ashitemaru/RL-tutorial">Reinforce learning code repository</a>。</p>
<h1 id="强化学习的基本概念">强化学习的基本概念</h1>
<p>强化学习和常见的监督学习有一定的类似，但是也有区别。监督学习我们给出的监督往往是数据集上的标签，但是强化学习我们能给出的提示是给机器的行为作出评分。即，在强化学习中，我们会有一个不断给机器的行为评分的监督者，我们希望机器能够通过这个评分监督者学习到如何作出高分行为。</p>
<p>监督学习的标签往往需要事先由人类标注出来，也就是说监督学习学习的是人类标签的模式，但是强化学习不一定需要事先标注，他需要学习我们给出的评分策略的模式。</p>
<h2 id="强化学习方法的分类">强化学习方法的分类</h2>
<p>根据强化学习方法是否需要理解环境，可以将强化学习算法分为 model-based 和 model-free 两类。</p>
<p>model-free 方法不需要机器去理解环境，他每一步决策都需要等待真实环境的反馈才能进行下一步。而 model-based 方法可以通过 model 来模拟现实环境，这样机器可以通过这个模拟现实环境的 model 来预测自己的行为对环境的影响，甚至分析行为不同分支的影响，从而做到对现实环境的理解。</p>
<p>model-free 方法包括 Q Learning、Sarsa、Policy gradients。</p>
<hr />
<p>根据强化学习方法最后选择行为的依据，可以将强化学习算法分为基于概率（policy-based）和基于价值（value-based）两类。</p>
<p>基于概率的强化学习算法是最自然的，其会根据其感官接收到的信息，给出其下一步各个行为的概率，然后根据这个概率随机选择下一步的行为。这些算法包括 Policy gradients。</p>
<p>基于价值的强化学习算法则会测算各个行动的价值，并且选择价值最高的作为自己下一步的行为。这类算法包括 Sarsa、Q Learning。</p>
<p>基于概率的方法的一个优势在于能够根据概率分布作出连续动作的选择，而基于价值的方法不得不在选择一步动作后才能作出下一步动作的价值评估。</p>
<p>当然现在也有将两者结合起来的算法，即 Actor-Critic 方法，这个方法内 Actor 会基于概率给出动作，而 Critic 会给出价值评估，这样的话能够在 Policy gradients 的基础上加快训练速度。</p>
<hr />
<p>根据其更新参数的策略，可以将强化学习算法分为回合制更新（Monte-Carlo update）和单步更新（temporal difference update）。</p>
<p>Monte-Carlo learning 和基础版本的 Policy gradients 方法都是回合制更新的，然而 Q Learning、Sarsa 和升级版本的 Policy gradients 都会基于更为现实和有效的单步更新。</p>
<hr />
<p>另外还有在线算法（on policy）和离线算法（off policy）的区别。离线算法的优势就是我可以在脱离交互环境的条件下，学习先前的交互记录。而在线算法必须要在交互环境内更新参数。</p>
<p>最典型的在线算法是 Sarsa 和强化过后的 Sarsa lambda。而典型的离线算法为 Q Learning 和强化过的 Deep Q network。</p>
<h1 id="q-learning">Q Learning</h1>
<p>首先引入 Q 表这个概念，Q 表可以看作一个函数 <span class="math inline">\(Q(s, a)\)</span>，这个函数值的含义为在状态 <span class="math inline">\(s\)</span> 的条件下，作出行为 <span class="math inline">\(a\)</span> 的潜在收益。而决策过程就是选择当前状态的所有可选行为中潜在收益最高的行为。</p>
<p>Q 表的更新是在进行决策之后开始的，如果我们原先在状态 <span class="math inline">\(s\)</span>，并且我们通过行为 <span class="math inline">\(a\)</span> 到达新状态 <span class="math inline">\(s&#39;\)</span>，这个时候我们需要重新估算 <span class="math inline">\(Q(s, a)\)</span> 的值。这个估计值新增的部分应当包含两个部分，即我们采取这个行为立刻能得到的奖励和我们在 <span class="math inline">\(s&#39;\)</span> 状态处能够达到的最大可能收益。</p>
<p>我们的更新策略如下：</p>
<p><span class="math display">\[
Q&#39;(s, a) := Q(s, a) + \alpha\left[r + \gamma\max_{a&#39; \in \Gamma(s&#39;)} Q(s&#39;, a&#39;) - Q(s, a)\right]
\]</span></p>
<p>这里 <span class="math inline">\(\Gamma(s&#39;)\)</span> 表示在状态 <span class="math inline">\(s&#39;\)</span> 处能够选择的所有行为构成的集合，<span class="math inline">\(\alpha\)</span> 表示<strong>学习率</strong>。</p>
<p>这样我们就能看到调整估计值的逻辑。我们首先获取在新状态 <span class="math inline">\(s&#39;\)</span> 处所有行为可能得到的最大收益 <span class="math inline">\(\max_{a&#39; \in \Gamma(s&#39;)} Q(s&#39;, a&#39;)\)</span>，乘以一个衰减 <span class="math inline">\(\gamma\)</span>，之后加上采取行为 <span class="math inline">\(a\)</span> 后立刻能得到的收益 <span class="math inline">\(r\)</span> 计算出行为 <span class="math inline">\(a\)</span> 的潜在收益。之后我们将这个估计值和原先的值作差得到差距，乘以学习率 <span class="math inline">\(\alpha\)</span> 之后就得到了我们需要更新到 <span class="math inline">\(Q(s, a)\)</span> 上的增量。</p>
<p>实际上的 Q Learning 在作出决策的时候其实也不一定完全按照最高价值选择，而是可能通过 epsilon greedy 的策略。即提前设定一个 <span class="math inline">\(\varepsilon \in (0, 1)\)</span>，以 <span class="math inline">\(\varepsilon\)</span> 概率按照 Q 表最优价值选择行为，以 <span class="math inline">\(1 - \varepsilon\)</span> 概率随机选择行为。这里 <span class="math inline">\(\varepsilon\)</span> 也被称为<strong>贪婪度</strong>。</p>
<p>在学习过程的初期，我们希望机器能够随机探索环境，所以此时 <span class="math inline">\(\varepsilon\)</span> 参数会设定较小。而后期我们在已经具有较为可靠的 Q 表并且希望得到最优解的时候，就可以适当调整到较高的 <span class="math inline">\(\varepsilon\)</span>。</p>
<hr />
<p>如果我们分析参数 <span class="math inline">\(\gamma\)</span> 在这个学习过程中的作用，我们注意到第一步的奖励会以 <span class="math inline">\(\gamma\)</span> 衰减，而第二步的奖励会以 <span class="math inline">\(\gamma^2\)</span> 衰减，以此类推，越远的奖励衰减的次数越高。</p>
<p>这就说明，如果令 <span class="math inline">\(\gamma = 0\)</span>，就意味着除了最近的直接奖励，其余奖励均会清空，此时机器只会关注最近的直接奖励。而如果令 <span class="math inline">\(\gamma = 1\)</span>，则意味着机器会平等地对待所有可能的奖励。而一般条件下 <span class="math inline">\(0 &lt; \gamma &lt; 1\)</span>，此时机器会按照指数衰减的方式对待从近到远的奖励。所以 <span class="math inline">\(\gamma\)</span> 也被称为<strong>奖励衰减指数</strong>。</p>
<h2 id="代码示例">代码示例</h2>
<p>这里在 <code>$&#123;repo&#125;/Q-learning</code> 下写了一个简单的通过 Q Learning 学习走迷宫的小程序，这里稍微介绍一下。</p>
<p>其实主要关注主函数之中的主循环就可以知道 Q Learning 是如何进行的了：</p>
<figure class="highlight python"><figcaption><span>Python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># $&#123;repo&#125;/Q-learning/main.py</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>():</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="comment"># Initial state</span></span><br><span class="line">        state = env.reset()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># Refresh the canvas</span></span><br><span class="line">            env.render()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL choose action based on state</span></span><br><span class="line">            action = RL.choose_action(<span class="built_in">str</span>(state))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL take action and get next state and reward</span></span><br><span class="line">            next_state, reward, done = env.step(action)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL learn from this transition</span></span><br><span class="line">            RL.learn(<span class="built_in">str</span>(state), action, reward, <span class="built_in">str</span>(next_state))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Swap state, move ahead</span></span><br><span class="line">            state = next_state</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Break while loop when end of this episode</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># End of game</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Game over&quot;</span>)</span><br><span class="line">    env.destroy()</span><br></pre></td></tr></table></figure>
<p>这里的 <code>RL</code> 相当于我们学习认识环境的机器，其具备的接口包括选择下一步行动（即 <code>RL.choose_action</code>）和根据环境的反馈做出学习（即 <code>RL.learn</code>）。而 <code>env</code> 则模拟了机器需要去认识的环境，其实质上是一个方形地图，地图上有两个地狱和一个代表最终结果的天堂，机器应该尝试着去学习一个绕过地狱到达天堂的路径。</p>
<p>其实主逻辑十分简单。我们需要训练 100 个 epoch，对于每个 epoch 我们首先初始化环境和机器所在的位置，然后在主循环中重复进行下述动作：</p>
<ul>
<li>令机器选择一个行为</li>
<li>将这个行为传递给环境，令环境给出反馈（包括奖励函数、是否终止、机器即将转到的状态）</li>
<li>机器通过环境的反馈进行学习</li>
<li>令机器采取该行为</li>
</ul>
<p>这里机器选择一个行为即我们提到的 epsilon greedy 策略，而其学习方式就是 Q 表的更新策略也已经叙述过。环境给出反馈则是简单判定机器是否走到了地狱或者天堂，并给出相应的结果。</p>
<p>只需要这样简单的代码，我们就可以看见一个红色方块仅依靠着环境的反馈学习到了如何登上天堂。</p>
<h1 id="sarsa">Sarsa</h1>
<p>Sarsa 和 Q Learning 的策略选择方面是一致的，也就是使用 epsilon greedy 策略选择自己下一步实际的路径，但是 Sarsa 在更新 Q 表的策略上和 Q Learning 是不同的。</p>
<p>我们观察 Q Learning 的更新策略：</p>
<p><span class="math display">\[
Q&#39;_{\rm QL}(s, a) := Q(s, a) + \alpha\left[r + \gamma{\color{red} \max_{a&#39; \in \Gamma(s&#39;)} Q(s&#39;, a&#39;)} - Q(s, a)\right]
\]</span></p>
<p>而 Sarsa 的更新策略为：</p>
<p><span class="math display">\[
Q&#39;_{\rm Sarsa}(s, a) := Q(s, a) + \alpha\left[r + \gamma{\color{red} Q(s&#39;, {\rm EG}(s&#39;))} - Q(s, a)\right]
\]</span></p>
<p>这里 <span class="math inline">\(a = {\rm EG}(s)\)</span> 表示的是在状态 <span class="math inline">\(s\)</span> 处使用 epsilon greedy 策略选择行为 <span class="math inline">\(a\)</span>。</p>
<p>除去这里更新策略的不同之外，Sarsa 在到达新状态 <span class="math inline">\(s&#39;\)</span> 后决定此时需要采取的行动的时候，就会采取之前更新策略中的 <span class="math inline">\({\rm EG}(s&#39;)\)</span> 直接作为下一步。</p>
<hr />
<p>Sarsa 的策略意味着，其在评估新状态 <span class="math inline">\(s&#39;\)</span> 的潜在收益的时候，并不会像 Q Learning 一样贪心选择最大的可能收益，而是更为现实地选择下一步行为，并且一定会采取这一步行为。</p>
<p>而 Q Learning 这里使用最大值贪心的更新策略也意味着其会完全沿着局部最优的路径前行，而无视可能造成的负面效果。Sarsa 则相对而言较为实际且保守。</p>
<p>另外，Q Learning 由于可以脱离环境，所以是 off policy 的。而 Sarsa 需要边转移状态边更新 Q 表，所以是 on policy 的。</p>
<h2 id="代码示例-1">代码示例</h2>
<p>这里不具体展示代码了，代码位于 <code>$&#123;repo&#125;/Sarsa</code> 目录之下。</p>
<p>需要注意和 Q Learning 算法不同的地方在于，在主循环里面我们除了需要保存一个 <code>state</code> 变量，我们还需要一个 <code>action</code> 变量保存即将需要采取的动作。</p>
<p>如果比对两种算法的效率，其实可以发现 Sarsa 算法的收敛速度明显慢于 Q Learning 算法。</p>
<h1 id="sarsa-lambda">Sarsa lambda</h1>
<p>Sarsa lambda 相对于 Sarsa 的优化点在于，机器最初探索环境的时候所采取的那些行为很有可能和最终结果无关，但是在经典的 Sarsa 算法中，每一步都会以同等的权重更新 Q 表，这很有可能导致后续真正导致较高奖励函数的行为没有匹配到较高权重。</p>
<p>所以我们可以采取近似于回合制更新的方式取代经典的 Sarsa 单步更新策略。我们令到达最终结果的最后一步以原始权重更新 Q 表，倒数第二步需要乘以常数 <span class="math inline">\(\lambda\)</span> 再更新到 Q 表，以此类推，倒数第 <span class="math inline">\(n\)</span> 步需要以权重 <span class="math inline">\(\lambda^{n - 1}\)</span> 更新到 Q 表。</p>
<p>我们可以注意到 <span class="math inline">\(\lambda = 0\)</span> 的时候，只有最后一步被更新到 Q 表，这就是最经典的回合更新。而 <span class="math inline">\(\lambda = 1\)</span> 的时候就是经典的 Sarsa 单步更新。</p>
<p>这里的 <span class="math inline">\(\lambda\)</span> 就被称为<strong>路径衰减指数</strong>。</p>
<h2 id="代码示例-2">代码示例</h2>
<p>这里只需要在 Sarsa 的基础上加上一个 E 表来累计每一步的 Q 表更新值即可，每走一步就需要将 E 表以 <span class="math inline">\(\lambda\)</span> 权重衰减一次。</p>
<p>主要观察 E 表在模型学习过程中的作用：</p>
<figure class="highlight python"><figcaption><span>Python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># $&#123;repo&#125;/Sarsa/model.py - class SarsaLambdaTable</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, state, action, reward, next_state, next_action</span>):</span></span><br><span class="line">    self.check_state_exist(next_state)</span><br><span class="line"></span><br><span class="line">    q_predict = self.q_table.loc[state, action]</span><br><span class="line">    <span class="keyword">if</span> next_state != <span class="string">&quot;terminal&quot;</span>:</span><br><span class="line">        q_target = reward + self.gamma * self.q_table.loc[next_state, next_action]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        q_target = reward</span><br><span class="line">        </span><br><span class="line">    error = q_target - q_predict</span><br><span class="line"></span><br><span class="line">    self.e_table.loc[state, :] *= <span class="number">0</span></span><br><span class="line">    self.e_table.loc[state, action] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    self.q_table += self.lr * error * self.e_table</span><br><span class="line">    self.e_table *= self.gamma * self.lambda_ <span class="comment"># Decay</span></span><br></pre></td></tr></table></figure>
<p>前半部分和经典 Sarsa 是完全一致的，但是注意后续的两个更新。在说明具体更新之前，我们可以说明 E 表的含义，E 表实际上就是将具体值更新到 Q 表时的权重表。</p>
<p>首先是将当前状态对应的 E 表行清零并将当前动作对应的列设为 <code>1</code>，即更新 E 表。之后以当前权重将误差更新到 Q 表，最后对权重作出衰减。</p>
<div class="note info"><p>我觉得他这个代码框架存在一个问题，就是当机器走到角落的时候，实际上它的决策空间会变小，但是代码里面没有体现这一点，只是简单的让机器不动。比如机器在左上角选择向左走，那么只是简单地让机器停在左上角。</p>
<p>这是一个并不好的处理，因为如果此时机器还没有碰巧到达天堂产生正向激励的话，机器完全会缩在角落里来避免地狱的反向激励。</p>
<p>有时间给他代码改改吧。</p>
</div>
<h1 id="deep-q-network-dqn">Deep Q Network (DQN)</h1>
<p>在我们面对更大的解空间的时候，存储过大的 Q 表会导致内存不够使用，所以此时我们需要令 <span class="math inline">\(Q(s, a)\)</span> 使用其他的方法计算，比如说我们可以引入深度神经网络来计算 <span class="math inline">\(Q(s, a)\)</span>。</p>
<p>这里我们可以引入这样的一个深度神经网络，该网络接受的输入是当前机器的状态 <span class="math inline">\(s\)</span>，该网络给出的输出则是各种可行的行为以及其评估值，我们只需要按照最基本的 Q Learning 原则根据神经网络的输出给出。</p>
<p>在这个给定的决策过程下，我们需要给出神经网络的训练策略。实际上我们只需要给出网络 loss 的计算方式就可以。我们思考 Q Learning 之中的训练策略，我们可以得知，在经典的 Q Learning 算法中我们会求出估计的 Q 值，将其与现实的 Q 值做差之后乘以学习率叠加到原先的 Q 表上。</p>
<p>而如果将这样的策略和神经网络联系起来，我们会发现估计的 Q 值和现实的 Q 值之间的差距实际上就表现出了 loss，我们只需要将这个 loss 对网络各个参数的梯度乘以学习率叠加到网络上就可以。</p>
<p>基于这样的认识，我们给出下述 loss 计算，使用平方误差的基本思想：</p>
<p><span class="math display">\[
\mathcal{L} := \left[r + \gamma\max_{a&#39; \in \Gamma(s&#39;)} Q(s&#39;, a&#39;) - Q(s, a)\right]^2
\]</span></p>
<p>这里在状态 <span class="math inline">\(s\)</span> 下选择动作 <span class="math inline">\(a\)</span> 会转移到状态 <span class="math inline">\(s&#39;\)</span>。</p>
<hr />
<p>另外，我们可以使用两种方法强化 DQN，这两种方法包括 Experience replay 和 Fixed Q targets。</p>
<p>前者实际上基于 Q Learning 的 off policy 特性，我们可以使用以往的训练数据乃至其他机器的经验数据来训练网络。</p>
<p>后者的含义是使用两个同样结构但是参数不同的网络，一个使用较早的参数来给出估计的 Q 值，而且这个网络的参数几乎不会在训练过程中更新，这个网络代表了过去的经验。一个使用较新的参数来给出现实的 Q 值，这个网络会在训练过程中更新参数并且会被应用到实际场景之中。这样做的原因是取消数据的相关性和提高训练的稳定性，具体的原因后续研讨。</p>
<p>在引入了这两个强化方法之后，loss 的具体计算方式会有所不同，具体的讨论见下方。</p>
<h2 id="代码示例-3">代码示例</h2>
<p>这里我们使用 Tensorflow 实现 DQN，这里我们使用的 DQN 具体算法描述为：</p>
<ul>
<li>首先初始化一个容量为 <span class="math inline">\(N\)</span> 的记忆库 <span class="math inline">\(D\)</span> 用于存放先前的经验，这里 <span class="math inline">\(N\)</span> 是先前设定的超参</li>
<li>随机初始化两个结构一致的神经网络 <span class="math inline">\(Q, \hat Q\)</span>，其初始化参数分别为 <span class="math inline">\(\b\theta, \b\theta^-\)</span>，初始化的时候保证 <span class="math inline">\(\b\theta = \b\theta^-\)</span></li>
<li>对模型按照下述流程训练 <span class="math inline">\(M\)</span> 个 epoch，这里 <span class="math inline">\(M\)</span> 为先前设定的超参
<ul>
<li>假定初始状态 <span class="math inline">\(s_1\)</span>，初始化 <span class="math inline">\(s = s_1\)</span></li>
<li>重复下述流程直到需要中断
<ul>
<li>以超参 <span class="math inline">\(\varepsilon\)</span> 使用 epsilon greedy 策略选取行为 <span class="math inline">\(a := \argmax_{a&#39;} Q(s, a&#39;; \b\theta)\)</span></li>
<li>令 <span class="math inline">\(r\)</span> 表示在状态 <span class="math inline">\(s\)</span> 的条件下执行行为 <span class="math inline">\(a\)</span> 得到的直接收益</li>
<li>令 <span class="math inline">\(s&#39;\)</span> 表示在状态 <span class="math inline">\(s\)</span> 的条件下执行行为 <span class="math inline">\(a\)</span> 转移到的状态</li>
<li>将描述状态转移的元组 <span class="math inline">\((s, a, r; s&#39;)\)</span> 存入 <span class="math inline">\(D\)</span></li>
<li>从 <span class="math inline">\(D\)</span> 中抽取 <span class="math inline">\(B\)</span> 组数据回放，更新 <span class="math inline">\(\b\theta\)</span>，具体流程见下述，这里 <span class="math inline">\(B\)</span> 是先前设定的超参</li>
<li>如果此时 <span class="math inline">\(s&#39;\)</span> 是终止状态，则终止流程，否则令 <span class="math inline">\(s \leftarrow s&#39;\)</span>，转移状态</li>
<li>每经过 <span class="math inline">\(C\)</span> 步，令 <span class="math inline">\(\b\theta^- \leftarrow \b\theta\)</span>，即令 <span class="math inline">\(\hat Q\)</span> 更新至 <span class="math inline">\(Q\)</span>，这里 <span class="math inline">\(C\)</span> 是先前设定的超参</li>
</ul></li>
</ul></li>
</ul>
<p>阐述具体学习过程之前，我们研讨一下引入了 Experience replay 和 Fixed Q targets 之后 loss 的计算。首先由于引入了两个神经网络，计算估计 Q 值和现实 Q 值的网络参数不一致，上述已经通过 <span class="math inline">\(\b\theta, \b\theta^-\)</span> 作出区分。</p>
<p>另外，由于引入了记忆库 <span class="math inline">\(D\)</span> 并且需要从中随机抽取转移对，那么最后的 loss 应该是基于 <span class="math inline">\(D\)</span> 上的均匀分布的期望，所以最后的 loss 为：</p>
<p><span class="math display">\[
\mathcal{L}_{\rm DQN}(\b\theta) := \opE_{(s, a, r; s&#39;) \sim U(D)} \left[r + \gamma\max_{a&#39; \in \Gamma(s&#39;)} \hat Q(s&#39;, a&#39;; \b\theta^-) - Q(s, a; \b\theta)\right]^2
\]</span></p>
<p>具体的学习流程为：</p>
<ul>
<li>从 <span class="math inline">\(D\)</span> 中抽取 <span class="math inline">\(B\)</span> 组转移元组</li>
<li>对每一个元组 <span class="math inline">\((s, a, r; s&#39;)\)</span> 计算 loss</li>
<li>计算梯度将 loss 反向传播到 <span class="math inline">\(\b\theta\)</span> 参数上，完成一次参数更新</li>
<li>根据具体情况调整 epsilon greedy 参数 <span class="math inline">\(\varepsilon\)</span></li>
</ul>
<p>这里使用的网络的数据依赖关系表现为：</p>
<p><img src="/uploads/note-of-rl/1.png" /></p>
<p>现在关注具体的代码实现，我们需要注意到我们采用的网络是两层全连接层：</p>
<figure class="highlight python"><figcaption><span>Python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># $&#123;repo&#125;/DQN/dqn.py - def _build_net</span></span><br><span class="line"></span><br><span class="line">self.state = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_features], name = <span class="string">&quot;state&quot;</span>)</span><br><span class="line">...</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;eval_net&quot;</span>):</span><br><span class="line">    eval_layer = tf.keras.layers.Dense(</span><br><span class="line">        units = <span class="number">20</span>,</span><br><span class="line">        activation = tf.nn.relu,</span><br><span class="line">        kernel_initializer = w_initializer,</span><br><span class="line">        bias_initializer = b_initializer,</span><br><span class="line">        name = <span class="string">&quot;eval_layer&quot;</span></span><br><span class="line">    )(self.state) <span class="comment"># Output: (None, 20)</span></span><br><span class="line">    self.q_eval = tf.keras.layers.Dense(</span><br><span class="line">        units = self.n_actions,</span><br><span class="line">        kernel_initializer = w_initializer,</span><br><span class="line">        bias_initializer = b_initializer,</span><br><span class="line">        name = <span class="string">&quot;q_eval_layer&quot;</span></span><br><span class="line">    )(eval_layer) <span class="comment"># Output: (None, n_actions)</span></span><br></pre></td></tr></table></figure>
<p>其逻辑是接受一个 <code>(batch_size, n_features)</code> 形状的输入，产生一个 <code>(batch_size, n_actions)</code> 形状的输出。<code>n_features</code> 表示需要多少个特征描述一个状态 <span class="math inline">\(s\)</span>，即状态空间维数。最后的输出则是各个动作的 Q 值。</p>
<p>在这样的网络设计之下，我们在求取 loss 的时候需要将 <code>(batch_size, n_actions)</code> 形状的输出中每一行挑出我们实际上选取的行为的 Q 值，压缩为 <code>(batch_size, )</code> 形状的 Q 值向量，再求取平方误差。所以就有下述代码：</p>
<figure class="highlight python"><figcaption><span>Python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># $&#123;repo&#125;/DQN/dqn.py - def _build_net</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;q_target&quot;</span>):</span><br><span class="line">    q_target = self.reward + self.gamma * tf.reduce_max(self.q_next, axis = <span class="number">1</span>, name = <span class="string">&quot;q_max_next_state&quot;</span>)</span><br><span class="line">    self.q_target = tf.stop_gradient(q_target) <span class="comment"># Shape: (None, )</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;q_eval&quot;</span>):</span><br><span class="line">    action_indices = tf.stack([</span><br><span class="line">        tf.<span class="built_in">range</span>(tf.shape(self.action)[<span class="number">0</span>], dtype = tf.int32), <span class="comment"># Index</span></span><br><span class="line">        self.action <span class="comment"># The index of action</span></span><br><span class="line">    ], axis = <span class="number">1</span>)</span><br><span class="line">    self.q_eval_wrt_action = tf.gather_nd(params = self.q_eval, indices = action_indices) <span class="comment"># Shape: (None, )</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;loss&quot;</span>):</span><br><span class="line">    self.loss = tf.reduce_mean(tf.squared_difference(</span><br><span class="line">        self.q_target, self.q_eval_wrt_action,</span><br><span class="line">        name = <span class="string">&quot;error&quot;</span></span><br><span class="line">    ))</span><br></pre></td></tr></table></figure>
<p>这里 <code>q_target</code> 部分只需要直接求取每一行的最大值就可以，而 <code>q_eval</code> 部分需要根据 <code>self.action</code> 压缩网络的输出，得到 <code>q_eval</code>。最后求取平方误差即可。</p>
<h2 id="dqn-enhancement">DQN Enhancement</h2>
<h3 id="double-dqn">Double DQN</h3>
<p>上述讨论的 DQN 其实有 overestimate 的问题，因为我们每次都会使用旧参数 <span class="math inline">\(\b\theta^-\)</span>，采用最大值函数去估计 evaluated Q value。为了解决这个问题，我们需要利用另外一个网络，即参数及时更新的网络来平衡旧参数网络的估计。</p>
<p>传统 DQN 的 loss 计算为：</p>
<p><span class="math display">\[
\mathcal{L}_{\rm DQN} := \left[r + \gamma{\color{red} \max_{a&#39; \in \Gamma(s&#39;)} \hat Q(s&#39;, a&#39;; \b\theta^-)} - Q(s, a; \b\theta)\right]^2
\]</span></p>
<p>这里 <span class="math inline">\(a&#39;\)</span> 的选择仅仅依赖于 <span class="math inline">\(\hat Q\)</span> 网络的情况，我们在 Double DQN 中可以使用 <span class="math inline">\(Q\)</span> 网络来选择 <span class="math inline">\(a&#39;\)</span> 以控制 Q 值的估计：</p>
<p><span class="math display">\[
\mathcal{L}_{\rm DDQN} := \left[r + \gamma{\color{red} \hat Q\left(s&#39;, \argmax_{a&#39; \in \Gamma(s&#39;)} Q(s&#39;, a&#39;; \b\theta); \b\theta^-\right)} - Q(s, a; \b\theta)\right]^2
\]</span></p>
<p>也就是说这里令 <span class="math inline">\(a&#39;\)</span> 是 <span class="math inline">\(Q\)</span> 网络选取出的在 <span class="math inline">\(s&#39;\)</span> 状态下的最佳行为，而非仅通过 <span class="math inline">\(\hat Q\)</span> 网络选择，这样做的平衡就可以防止 <span class="math inline">\(\hat Q\)</span> 给出过高的 Q 值。</p>
<hr />
<p>这里就不给出具体的代码实现了。</p>
<p><code>UPDATE TODO</code></p>
<h3 id="prioritized-experience-replay">Prioritized Experience Replay</h3>
<p>如果我们的环境的 reward 分配比较不均匀，正向反馈较难获取，但是却需要要求机器重点学习正向反馈，我们原先的 replay 策略就会失效，因为在原先的策略内，我们令各个回忆的权重是一致的，这个分配并不符合实际。</p>
<p>我们可以这样估计每个回忆的优先级，和 loss 的计算类似，我们用估计的 Q 值直接减去现实的 Q 值，如果这个值越大，就说明还能提升的空间就越大，从而说明这个回忆的优先级越高。上述 Q 值的差值一般称为 TD error。</p>
<hr />
<p>每次选取回忆的时候，我们都需要根据 TD error 给出的优先级排序各个回忆，但如果每次都排序的话显然会浪费算力，所以我们需要使用 Sum tree 数据结构组织各个回忆。</p>
<p><code>TODO</code></p>
<h1 id="policy-gradients">Policy Gradients</h1>
<p>Policy Gradients 的特点在于其直接输出下一步行为本身，而不是行为的某种评分。这个特点使得 Policy Gradients 可以应对决策空间无限的情况，而 Q Learning 则无法应对无限决策空间。</p>
<p>Policy Gradients 基于随机策略，也就是我们需要给出在某个状态 <span class="math inline">\(s\)</span> 下我们采取行为 <span class="math inline">\(a\)</span> 的概率，这种随机策略一般用参数 <span class="math inline">\(\b\theta\)</span> 建模。</p>
<p>我们把基于参数为 <span class="math inline">\(\b\theta\)</span> 的随机策略 <span class="math inline">\(\pi_{\b\theta}\)</span>，在状态 <span class="math inline">\(s\)</span> 下，我们采取行为 <span class="math inline">\(a\)</span> 的概率表示为 <span class="math inline">\(\pi_{\b\theta}(a \mid s)\)</span>。这里 <span class="math inline">\(s_t, a_t\)</span> 表示在第 <span class="math inline">\(t\)</span> 时刻的状态和行为，后续有类似的 <span class="math inline">\(r_t\)</span>。</p>
<p>显然这样的 <span class="math inline">\(\pi_{\b\theta}\)</span> 代表了一个行为决策空间 <span class="math inline">\(\mathcal{A}\)</span> 上的一个分布。</p>
<p>那么我们可以类似地定义出一个行为评估函数（这里由于 MathJAX 的渲染问题，使用冒号代替表示条件概率的竖线）：</p>
<p><span class="math display">\[
Q^{\pi_{\b\theta}}(s, a) := \opE_{a \sim \pi_{\b\theta}} \left[\sum_{k = 0}^{+\infty} \gamma^k r_{t + k + 1}: s_t = s, a_t = a\right]
\]</span></p>
<p>定义的含义也是直观的，这个评估函数的值就是在第 <span class="math inline">\(t\)</span> 时刻，状态和行为分别为 <span class="math inline">\(s, a\)</span> 的时候，后续能获取的收益的期望。这里 <span class="math inline">\(\gamma\)</span> 是先前介绍过的奖励衰减指数，我们将未来所有的收益全部相加作为总收益。</p>
<p>另外，由于我们知道给定状态 <span class="math inline">\(s\)</span> 的时候行为 <span class="math inline">\(a\)</span> 的分布，所以我们可以定义一个不需要给定具体行为的，仅针对状态进行评估的状态函数：</p>
<p><span class="math display">\[
V^{\pi_{\b\theta}}(s) := \sum_{a \in \mathcal{A}} \pi_{\b\theta}(a \mid s)Q^{\pi_{\b\theta}}(s, a) = \opE_{a \sim \pi_{\b\theta}} \left[\sum_{k = 0}^{+\infty} \gamma^k r_{t + k + 1}: s_t = s\right]
\]</span></p>
<p>此外，我们可以注意到这个随机策略实际上定义了一个在状态空间 <span class="math inline">\(\mathcal{S}\)</span> 上的 Markov chain，那么我们就有下述稳态概率的定义：</p>
<p><span class="math display">\[
d^{\pi_{\b\theta}}(s) := \lim_{t \to +\infty} \P_{\pi_{\b\theta}}(s_t = s \mid s_0)
\]</span></p>
<p>稳态概率实际上描述了在给定初始状态 <span class="math inline">\(s_0\)</span> 的条件下，使用随机策略 <span class="math inline">\(\pi_{\b\theta}\)</span> 在状态空间 <span class="math inline">\(\mathcal{S}\)</span> 上随机转移，最终到达稳定状态的时候，位于状态 <span class="math inline">\(s\)</span> 的概率。</p>
<p>基于稳态概率，我们就可以给一个策略 <span class="math inline">\(\pi_{\b\theta}\)</span> 定义价值：</p>
<p><span class="math display">\[
\mathcal{J}(\b\theta) := \sum_{s \in \mathcal{S}} d^{\pi_{\b\theta}}(s)V^{\pi_{\b\theta}}(s) = \sum_{s \in \mathcal{S}} d^{\pi_{\b\theta}}(s) \left(\sum_{a \in \mathcal{A}} \pi_{\b\theta}(a \mid s)Q^{\pi_{\b\theta}}(s, a)\right)
\]</span></p>
<p>为了尽可能提高策略的价值，我们采用梯度上升的方式，所以重点就是计算上述价值函数的梯度 <span class="math inline">\(\nabla_{\b\theta}\mathcal{J}(\b\theta)\)</span>。</p>
<hr />
<p>首先我们对 <span class="math inline">\(Q^{\pi_{\b\theta}}(s, a)\)</span> 做一些简单的展开。</p>
<p>我们只需要遍历 <span class="math inline">\(s\)</span> 所有可能的下一步状态 <span class="math inline">\(s&#39;\)</span>，得到其发生转移 <span class="math inline">\(s \to s&#39;\)</span> 的概率，而相应的价值评估也会拆分为直接收益 <span class="math inline">\(r\)</span> 和接下来到达状态 <span class="math inline">\(s&#39;\)</span> 的收益（与 Q Learning 具有类似的结构）。基于上述思路，得到下述展开：</p>
<p><span class="math display">\[
Q^{\pi_{\b\theta}}(s, a) = \sum_{s&#39; \in \mathcal{S}} \P_{\pi_{\b\theta}}(s&#39; \mid s, a)\left[r + V^{\pi_{\b\theta}}(s&#39;)\right]
\]</span></p>
<p>考虑到 <span class="math inline">\(r, \P_{\pi_{\b\theta}}(s&#39; \mid s, a)\)</span> 相对于 <span class="math inline">\(\b\theta\)</span> 都是常数，可以得到：</p>
<p><span class="math display">\[
\nabla_{\b\theta}Q^{\pi_{\b\theta}}(s, a) = \sum_{s&#39; \in \mathcal{S}} \P_{\pi_{\b\theta}}(s&#39; \mid s, a)\nabla_{\b\theta}V^{\pi_{\b\theta}}(s&#39;)
\]</span></p>
<p>基于此，我们做出如下推理：</p>
<p><span class="math display">\[
\begin{aligned}
\nabla_{\b\theta}V^{\pi_{\b\theta}}(s) &amp;= \nabla_{\b\theta}\left[\sum_{a \in \mathcal{A}} \pi_{\b\theta}(a \mid s)Q^{\pi_{\b\theta}}(s, a)\right] \\
&amp;= \sum_{a \in \mathcal{A}}\left[Q^{\pi_{\b\theta}}(s, a)\nabla_{\b\theta}\pi_{\b\theta}(a \mid s) + \pi_{\b\theta}(a \mid s)\nabla_{\b\theta}Q^{\pi_{\b\theta}}(s, a)\right] \\
&amp;= \sum_{a \in \mathcal{A}}\left[Q^{\pi_{\b\theta}}(s, a)\nabla_{\b\theta}\pi_{\b\theta}(a \mid s) + \pi_{\b\theta}(a \mid s)\sum_{s&#39; \in \mathcal{S}} \P_{\pi_{\b\theta}}(s&#39; \mid s, a)\nabla_{\b\theta}V^{\pi_{\b\theta}}(s&#39;)\right] \\
&amp;= \sum_{a \in \mathcal{A}}Q^{\pi_{\b\theta}}(s, a)\nabla_{\b\theta}\pi_{\b\theta}(a \mid s) + \sum_{a \in \mathcal{A}}\left[\pi_{\b\theta}(a \mid s)\sum_{s&#39; \in \mathcal{S}} \P_{\pi_{\b\theta}}(s&#39; \mid s, a)\nabla_{\b\theta}V^{\pi_{\b\theta}}(s&#39;)\right]
\end{aligned}
\]</span></p>
<p>这里我们观测到了 <span class="math inline">\(\nabla_{\b\theta}V^{\pi_{\b\theta}}(s)\)</span> 结构的重复，我们将其无限展开。这里每次展开留下的常数项为 <span class="math inline">\(\sum_{a \in \mathcal{A}}Q^{\pi_{\b\theta}}(s, a)\nabla_{\b\theta}\pi_{\b\theta}(a \mid s)\)</span>。</p>
<p>而每展开一步需要累计的线性系数为 <span class="math inline">\(\pi_{\b\theta}(a \mid s)\P_{\pi_{\b\theta}}(s&#39; \mid s, a)\)</span>，这一系数实际上代表的是在状态 <span class="math inline">\(s\)</span> 下一步转移到 <span class="math inline">\(s&#39;\)</span> 的概率，那么其的累计就是在若干步下到达指定状态的概率。</p>
<p>那么上述式子无限展开后就应当为：</p>
<p><span class="math display">\[
\nabla_{\b\theta}V^{\pi_{\b\theta}}(s) = \sum_{s_\bot \in \mathcal{S}}\left[\sum_{k = 0}^{+\infty}\P_{\pi_{\b\theta}}(s \mathop{\to}^k s_\bot) \cdot \sum_{a \in \mathcal{A}}Q^{\pi_{\b\theta}}(s_\bot, a)\nabla_{\b\theta}\pi_{\b\theta}(a \mid s_\bot)\right]
\]</span></p>
<p>这里 <span class="math inline">\(\P_{\pi_{\b\theta}}(s \to^k\!\! s_\bot)\)</span> 表示在策略 <span class="math inline">\(\pi_{\b\theta}\)</span> 下，从状态 <span class="math inline">\(s\)</span> 用 <span class="math inline">\(k\)</span> 步到达终态 <span class="math inline">\(s_\bot\)</span> 的概率。</p>
<p>根据定义，实际上我们有：</p>
<p><span class="math display">\[
\sum_{k = 0}^{+\infty}\P_{\pi_{\b\theta}}(s \mathop{\to}^k s_\bot) = d^{\pi_{\b\theta}}(s_\bot)
\]</span></p>
<p>那么，考虑到对策略的评估实际上就是对该策略下初态价值的评估，我们得到：</p>
<p><span class="math display">\[
\begin{aligned}
\nabla_{\b\theta}\mathcal{J}(\b\theta) &amp;= \nabla_{\b\theta} V^{\pi_{\b\theta}}(s_0) \\
&amp;= \sum_{s_\bot \in \mathcal{S}}\left[d^{\pi_{\b\theta}}(s_\bot) \sum_{a \in \mathcal{A}}Q^{\pi_{\b\theta}}(s_\bot, a)\nabla_{\b\theta}\pi_{\b\theta}(a \mid s_\bot)\right] \\
\end{aligned}
\]</span></p>
<p>将 <span class="math inline">\(s_\bot\)</span> 替换回 <span class="math inline">\(s\)</span> 就得到 Policy gradient theorem 的公式：</p>
<p><span class="math display">\[
\nabla_{\b\theta}\mathcal{J}(\b\theta) = \sum_{s \in \mathcal{S}}\left[d^{\pi_{\b\theta}}(s) \sum_{a \in \mathcal{A}}Q^{\pi_{\b\theta}}(s, a)\nabla_{\b\theta}\pi_{\b\theta}(a \mid s)\right]
\]</span></p>
<p>为了让计算机便于计算，我们对上述计算式作出进一步变形：</p>
<p><span class="math display">\[
\begin{aligned}
\nabla_{\b\theta}\mathcal{J}(\b\theta) &amp;= \sum_{s \in \mathcal{S}}\left[d^{\pi_{\b\theta}}(s) \sum_{a \in \mathcal{A}}Q^{\pi_{\b\theta}}(s, a)\nabla_{\b\theta}\pi_{\b\theta}(a \mid s)\right] \\
&amp;= \sum_{s \in \mathcal{S}}\left[d^{\pi_{\b\theta}}(s) \sum_{a \in \mathcal{A}}Q^{\pi_{\b\theta}}(s, a)\pi_{\b\theta}(a \mid s)\frac{\nabla_{\b\theta}\pi_{\b\theta}(a \mid s)}{\pi_{\b\theta}(a \mid s)}\right] \\
&amp;= \sum_{s \in \mathcal{S}}\sum_{a \in \mathcal{A}} (d^{\pi_{\b\theta}}(s)\pi_{\b\theta}(a \mid s)) \cdot (Q^{\pi_{\b\theta}}(s, a)\nabla_{\b\theta}\ln \pi_{\b\theta}(a \mid s))
\end{aligned}
\]</span></p>
<p>容易看出 <span class="math inline">\(d^{\pi_{\b\theta}}(s)\pi_{\b\theta}(a \mid s)\)</span> 给出了 <span class="math inline">\(\mathcal{S} \times \mathcal{A}\)</span> 上的分布：</p>
<p><span class="math display">\[
\sum_{s \in \mathcal{S}}\sum_{a \in \mathcal{A}} d^{\pi_{\b\theta}}(s)\pi_{\b\theta}(a \mid s) = 1
\]</span></p>
<p>那么：</p>
<p><span class="math display">\[
\nabla_{\b\theta}\mathcal{J}(\b\theta) = \opE_{(s, a) \sim \pi_{\b\theta}} [Q^{\pi_{\b\theta}}(s, a)\nabla_{\b\theta}\ln \pi_{\b\theta}(a \mid s)]
\]</span></p>
<p>当然，我们还可以继续展开评估函数：</p>
<p><span class="math display">\[
\nabla_{\b\theta}\mathcal{J}(\b\theta) = \opE_{(s, a) \sim \pi_{\b\theta}} \left[\nabla_{\b\theta}\ln \pi_{\b\theta}(a \mid s) \cdot \E\left[\sum_{k = 0}^{+\infty} \gamma^k r_{t + k + 1}: s_t = s, a_t = a\right]\right]
\]</span></p>
<p>也就是：</p>
<p><span class="math display">\[
\nabla_{\b\theta}\mathcal{J}(\b\theta) = \opE_{(s, a) \sim \pi_{\b\theta}} \left[\nabla_{\b\theta}\ln \pi_{\b\theta}(a \mid s) \cdot \sum_{k = 0}^{+\infty} \gamma^k r_{t + k + 1}\right]
\]</span></p>
<h2 id="代码示例-4">代码示例</h2>
<p>这里使用 Reinforce 方法实现 Policy Gradients，这是最为基本的一种实现方式，是一种回合制更新策略。</p>
<p>首先我们需要在随机策略 <span class="math inline">\(\pi_{\b\theta}\)</span> 指导下得到的一整个回合信息：</p>
<p><span class="math display">\[
s_1, a_1, r_2; s_2, a_2, r_3; s_3, \cdots; s_{T - 1}, a_{T - 1}, r_T; {\rm terminal}
\]</span></p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/06/22/paper-3/" rel="prev" title="《Comyco - Quality-Aware Adaptive Video Streaming via Imitation Learning》论文笔记">
                  <i class="fa fa-chevron-left"></i> 《Comyco - Quality-Aware Adaptive Video Streaming via Imitation Learning》论文笔记
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/07/07/paper-4/" rel="next" title="《Neural Adaptive Video Streaming with Pensieve》论文笔记">
                  《Neural Adaptive Video Streaming with Pensieve》论文笔记 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments"><div id="twikoo-comments"></div></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ashitemaru</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">243k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">3:40</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://unpkg.com/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{&quot;enable&quot;:true,&quot;theme&quot;:&quot;default&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;unpkg.com&#x2F;mermaid@8.9.3&#x2F;dist&#x2F;mermaid.min.js&quot;,&quot;integrity&quot;:&quot;sha256-OyJHvRcZHaRR6Ig73ppxF4QXk8HzvfgTprRWkulCkfY&#x3D;&quot;}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;unpkg.com&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;,&quot;integrity&quot;:&quot;sha256-ncNI9OXOS5Ek4tzVYiOMmN&#x2F;KKCPZ6V0Cpv2P&#x2F;zHntiA&#x3D;&quot;}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="twikoo" type="application/json">{&quot;enable&quot;:true,&quot;visitor&quot;:true,&quot;envId&quot;:&quot;https:&#x2F;&#x2F;vercel-deploy-two.vercel.app&quot;,&quot;el&quot;:&quot;#twikoo-comments&quot;}</script>
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.twikoo.el)
    .then(() => NexT.utils.getScript(
      'https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js',
      { condition: window.twikoo }
    ))
    .then(() => {
      twikoo.init(CONFIG.twikoo);
    });
});
</script>

</body>
</html>
