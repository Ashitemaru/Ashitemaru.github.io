<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic%7CNoto+Serif+KR:300,300italic,400,400italic,700,700italic%7CMS+PMincho:300,300italic,400,400italic,700,700italic%7CFira+Code:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext" referrerpolicy="no-referrer">

<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://unpkg.com/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;ashitemaru.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Muse&quot;,&quot;version&quot;:&quot;8.5.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;width&quot;:320,&quot;display&quot;:&quot;always&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:true,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:&quot;disqusjs&quot;,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;manual&quot;,&quot;top_n_per_article&quot;:1,&quot;unescape&quot;:false,&quot;preload&quot;:false}}</script><script src="/js/config.js"></script>
<meta name="description" content="这学期选了一门深度强化学习，也正好趁此机会系统性质地把强化学习理论基础过一下，如果可以的话可能还打算过一下深度学习基础扩展一下视野之类的。">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习入门笔记">
<meta property="og:url" content="https://ashitemaru.github.io/2023/02/22/note-of-rl/index.html">
<meta property="og:site_name" content="Ashitemaru">
<meta property="og:description" content="这学期选了一门深度强化学习，也正好趁此机会系统性质地把强化学习理论基础过一下，如果可以的话可能还打算过一下深度学习基础扩展一下视野之类的。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ashitemaru.github.io/uploads/note-of-rl/1.webp">
<meta property="article:published_time" content="2023-02-22T17:39:55.000Z">
<meta property="article:modified_time" content="2023-02-22T17:39:55.000Z">
<meta property="article:author" content="Ashitemaru">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ashitemaru.github.io/uploads/note-of-rl/1.webp">


<link rel="canonical" href="https://ashitemaru.github.io/2023/02/22/note-of-rl/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;ashitemaru.github.io&#x2F;2023&#x2F;02&#x2F;22&#x2F;note-of-rl&#x2F;&quot;,&quot;path&quot;:&quot;2023&#x2F;02&#x2F;22&#x2F;note-of-rl&#x2F;&quot;,&quot;title&quot;:&quot;强化学习入门笔记&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>强化学习入门笔记 | Ashitemaru</title>
  



<link rel="stylesheet" href="https://www.unpkg.com/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://www.unpkg.com/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://www.unpkg.com/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
<script src="https://www.unpkg.com/twikoo@1.4.1/dist/twikoo.all.min.js"></script>
<script src="/js/jquery.min.js"></script>
<script>
function getURL(e) {
    var http = e.slice(0, 4)
    var https = e.slice(0, 5)
    if (http == "http" || https == "https") {
        return e
    } else if (e == "" || e == null || e == undefined) {
        return e
    } else {
        e = 'http://' + e
        return e
    }
}

function newComment() {
    twikoo.getRecentComments({
        envId: 'https://vercel-deploy-two.vercel.app',
        pageSize: 10,
        includeReply: false
    }).then(function (res) {
        var hotComments = $("#hot-comments");
        for (var i = 0; i < res.length; i++) {
            var nick = res[i].nick;
            var content = res[i].commentText;
            var newcontent = content.substring(0, 50);
            var url = res[i].url;
            var avatar = res[i].avatar;
            var link = getURL(res[i].link);
            var updatedAt = res[i].relativeTime;
            var commentId = '#' + res[i].id;
            hotComments.append(
                `<li class="px1 pb2 flex items-center">
                    <img style="width:40px;height:40px" class="circle mx1 listavatar" src="${avatar}">
                    <div style="display:flex;flex-direction:column;width:100%;">
                        <div style="display:flex;justify-content:space-between;flex-direction:row;align-items:center;">
                            <div class="h5 listauthor overflow-hidden" title="${nick}">
                                <a target="_blank" rel="noopener external nofollow noreferrer" href="${link}">${nick}</a>
                            </div>
                            <div class="h6 mr1 listdate wenzi hang1" style="color:#777777;">${updatedAt}</div>
                        </div>
                        <div style="display:flex;flex-direction:row;width:100%;">
                            <a class="h5 list-comcontent" style="overflow:hidden;display:flex;border-bottom:0px;text-overflow:ellipsis;line-height:1.5;text-align:left" href="${url}${commentId}">${newcontent}</a>
                        </div>
                    </div>
                </li>`
            );
        }
    }).catch(function (err) {
        console.error(err);
    });
}

function replaceRuby() {
    $('code')
        .filter((_, node) => {
            var list = $(node).text().split(' ');
            return list.length === 3 && list[0] === "@";
        })
        .replaceWith((_, text) => {
            var list = text.split(' ');
            var written = list[1];
            var read = list[2];
            return $(`<ruby>${written}<rp>(</rp><rt>${read}</rt><rp>)</rp></ruby>`);
        });
}

$(function () {
    newComment();
    replaceRuby();
});
</script>

<!-- CSS -->
<link href="/css/app.min.css" rel="stylesheet">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Ashitemaru</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">11</span></a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">53</span></a></li>
        <li class="menu-item menu-item-skill-docs"><a href="https://sast-skill-docers.github.io/sast-skill-docs/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i>技能引导文档</a></li>
        <li class="menu-item menu-item-thuinfo"><a href="https://www.thuinfo.net/" rel="noopener" target="_blank"><i class="fa fa-sitemap fa-fw"></i>THUInfo</a></li>
        <li class="menu-item menu-item-se-index"><a href="https://thuse-course.github.io/course-index/" rel="noopener" target="_blank"><i class="fa fa-calendar fa-fw"></i>软工课程主页</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%9F%BA%E6%9C%AC%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA"><span class="nav-number">1.</span> <span class="nav-text">强化学习的基本概念与基本优化理论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E4%B8%8E%E7%AD%96%E7%95%A5"><span class="nav-number">1.1.</span> <span class="nav-text">环境与策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%B4%AF%E8%AE%A1%E6%94%B6%E7%9B%8A%E4%B8%8E%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.</span> <span class="nav-text">累计收益与价值函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA"><span class="nav-number">1.3.</span> <span class="nav-text">基本优化理论</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%AE%8C%E5%85%A8%E7%8E%AF%E5%A2%83%E7%9F%A5%E8%AF%86%E7%9A%84-rl-%E6%96%B9%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">基于完全环境知识的 RL 方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#policy-iteration-pi"><span class="nav-number">2.1.</span> <span class="nav-text">Policy Iteration (PI)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#value-iteration-vi"><span class="nav-number">2.2.</span> <span class="nav-text">Value Iteration (VI)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%89%E5%85%B3-pi-%E5%92%8C-vi-%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AF%B9%E6%AF%94"><span class="nav-number">2.3.</span> <span class="nav-text">有关 PI 和 VI 的一些对比</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%89%E5%85%B3-bellman-%E7%AE%97%E5%AD%90%E7%9A%84%E6%8B%93%E5%B1%95"><span class="nav-number">2.4.</span> <span class="nav-text">有关 Bellman 算子的拓展</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BA%92%E7%9A%84-rl-%E6%96%B9%E6%B3%95"><span class="nav-number">3.</span> <span class="nav-text">基于环境交互的 RL 方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0"><span class="nav-number">3.1.</span> <span class="nav-text">策略评估</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#monte-carlo-mc"><span class="nav-number">3.1.1.</span> <span class="nav-text">Monte-Carlo (MC)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#temporal-difference-td"><span class="nav-number">3.1.2.</span> <span class="nav-text">Temporal-Difference (TD)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E6%AD%A5-td-target-%E4%B8%8E-tdlambda"><span class="nav-number">3.1.2.1.</span> <span class="nav-text">多步 TD target 与 TD(\(\lambda\))</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#backward-view-tdlambda-eligibility-traces"><span class="nav-number">3.1.2.2.</span> <span class="nav-text">Backward-view TD(\(\lambda\)) &amp; Eligibility Traces</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%94%B9%E8%BF%9B"><span class="nav-number">3.2.</span> <span class="nav-text">策略改进</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%8E%A7%E5%88%B6"><span class="nav-number">4.</span> <span class="nav-text">时序差分控制</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%8C%E8%BD%A8%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%8E%A7%E5%88%B6sarsa"><span class="nav-number">4.1.</span> <span class="nav-text">同轨时序差分控制（SARSA）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%82%E8%BD%A8%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%8E%A7%E5%88%B6q-learning"><span class="nav-number">4.2.</span> <span class="nav-text">异轨时序差分控制（Q learning）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%8E%A7%E5%88%B6"><span class="nav-number">5.</span> <span class="nav-text">蒙特卡洛控制</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%8C%E8%BD%A8%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%8E%A7%E5%88%B6"><span class="nav-number">5.1.</span> <span class="nav-text">同轨蒙特卡洛控制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%82%E8%BD%A8%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%8E%A7%E5%88%B6"><span class="nav-number">5.2.</span> <span class="nav-text">异轨蒙特卡洛控制</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%87%BD%E6%95%B0%E4%BC%B0%E8%AE%A1%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="nav-number">6.</span> <span class="nav-text">函数估计与深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#deep-q-network-dqn"><span class="nav-number">6.1.</span> <span class="nav-text">Deep Q Network (DQN)</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ashitemaru"
      src="/uploads/avatar.png">
  <p class="site-author-name" itemprop="name">Ashitemaru</p>
  <div class="site-description" itemprop="description">Trying to be an observer of this world.</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">53</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Ashitemaru" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Ashitemaru" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:qhd19@mails.tsinghua.edu.cn" title="E-Mail → mailto:qhd19@mails.tsinghua.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/Ashitemaru" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;Ashitemaru" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Friends
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.unidy.cn/" title="https:&#x2F;&#x2F;www.unidy.cn" rel="noopener" target="_blank">UNIDY</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.dilant.cn/" title="https:&#x2F;&#x2F;www.dilant.cn" rel="noopener" target="_blank">Dilant</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://zcy.moe/" title="https:&#x2F;&#x2F;zcy.moe" rel="noopener" target="_blank">猫猫</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://c7w.tech/" title="https:&#x2F;&#x2F;c7w.tech" rel="noopener" target="_blank">c7w</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.leenldk.top/" title="http:&#x2F;&#x2F;www.leenldk.top&#x2F;" rel="noopener" target="_blank">leenldk (20)</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://sun80449.github.io/" title="https:&#x2F;&#x2F;sun80449.github.io" rel="noopener" target="_blank">lcr</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://pwe.cat/" title="https:&#x2F;&#x2F;pwe.cat" rel="noopener" target="_blank">索尔</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://kokic.github.io/" title="https:&#x2F;&#x2F;kokic.github.io&#x2F;" rel="noopener" target="_blank">kokic</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://sjfh.top/" title="https:&#x2F;&#x2F;sjfh.top" rel="noopener" target="_blank">sjfh</a>
        </li>
    </ul>
  </div>
<div class="sidebar-1 mybox relative">
    <div class="p2">
        <i class="fab fa-facebook-messenger mr1"></i>
        Latest Comments
    </div>
    <div id="hot-comments"></div>
</div>
          </div>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/Ashitemaru" class="github-corner" title="Come here for fun." aria-label="Come here for fun." rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ashitemaru.github.io/2023/02/22/note-of-rl/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.png">
      <meta itemprop="name" content="Ashitemaru">
      <meta itemprop="description" content="Trying to be an observer of this world.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ashitemaru">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          强化学习入门笔记
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-02-22 17:39:55" itemprop="dateCreated datePublished" datetime="2023-02-22T17:39:55+00:00">2023-02-22</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2023-02-22 17:39:55" itemprop="dateModified" datetime="2023-02-22T17:39:55+00:00">2023-02-22</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%AE%A1%E7%AE%97%E6%9C%BA/" itemprop="url" rel="index"><span itemprop="name">【学习笔记】计算机</span></a>
        </span>
    </span>

  
    <span id="/2023/02/22/note-of-rl/" class="post-meta-item twikoo_visitors" data-flag-title="强化学习入门笔记" title="阅读次数">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="twikoo_visitors"></span>
    </span>
  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>29k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>26 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>这学期选了一门深度强化学习，也正好趁此机会系统性质地把强化学习理论基础过一下，如果可以的话可能还打算过一下深度学习基础扩展一下视野之类的。</p>
<span id="more"></span>
<p><span class="math display">\[
\newcommand{\Pe}{\mathbb{P}_{\mathcal{E}}}
\newcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\S}{\mathcal{S}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\Eop}{\mathop{\mathbb{E}}}
\newcommand{\b}{\boldsymbol}
\newcommand{\d}{\mathrm{d}}
\newcommand{\argmax}{\mathop{\rm argmax}}
\]</span></p>
<h1 id="强化学习的基本概念与基本优化理论">强化学习的基本概念与基本优化理论</h1>
<h2 id="环境与策略">环境与策略</h2>
<p>强化学习的基本框架为一个智能体和环境交互，环境对智能体的决策给出反馈，即收益信号并转移到下一状态，智能体需要学会在不同的状态下应该做出何种决策以收获最高总收益。</p>
<p>这里环境常常建模为 Markov 决策过程，即 MDP <span class="math inline">\((\S, \A, \Pe, r, \gamma)\)</span>。所谓 Markov 过程，指的是状态转移和历史状态无关，仅仅和当前状态有关，可以简单描述为 <span class="math inline">\(\P(s_{t + 1} \mid s_t) = \P(s_{t + 1} \mid s_t, s_{t - 1}, \cdots, s_1)\)</span>。MDP 中各项指的是：</p>
<ul>
<li>状态空间 <span class="math inline">\(\S\)</span>。即环境可以表现出的所有状态构成的集合。</li>
<li>决策空间 <span class="math inline">\(\A\)</span>。即环境允许智能体做出的所有决策构成的集合。</li>
<li>状态转移概率函数 <span class="math inline">\(\Pe\)</span>。<span class="math inline">\(\Pe(s&#39; \mid s, a)\)</span> 表示环境在状态 <span class="math inline">\(s \in \S\)</span> 且智能体做出决策 <span class="math inline">\(a \in \A\)</span> 的时候环境下一步转移到状态 <span class="math inline">\(s&#39; \in \S\)</span> 的概率。这里下标 <span class="math inline">\(\mathcal{E}\)</span> 表示该概率仅与环境 <span class="math inline">\(\mathcal{E}\)</span> 本身有关。其一般称为 model 或者 dynamics。</li>
<li>收益函数 <span class="math inline">\(r\)</span>。收益函数为从 <span class="math inline">\(\S \times \A \times \S\)</span> 到 <span class="math inline">\(\R\)</span> 的映射。<span class="math inline">\(r(s, a, s&#39;)\)</span> 表示环境在状态 <span class="math inline">\(s \in \S\)</span> 且智能体做出决策 <span class="math inline">\(a \in \A\)</span> 并转移到新状态 <span class="math inline">\(s&#39; \in \S\)</span> 的时候环境给予智能体的收益。</li>
<li>收益衰减系数 <span class="math inline">\(\gamma\)</span>。这里 <span class="math inline">\(\gamma \in [0, 1]\)</span>。该参数代表收益随着时间步衰减的系数，具体含义将会在后续定义累计收益的时候展开。</li>
</ul>
<p>另外需要说明的是，状态 <span class="math inline">\(s \in \S\)</span> 可能并不是完全可以观测的，如果 <span class="math inline">\(s\)</span> 不能完全观测，我们会将 <span class="math inline">\(s\)</span> 中能观测的部分记作 <span class="math inline">\(o\)</span>，称为<strong>观测</strong>，即 observation。所有观测构成的集合记作 <span class="math inline">\(\mathcal{O}\)</span>。这种部分可以观测的 MDP 称为 <strong>P</strong>artially <strong>O</strong>bservable MDP，缩写 POMDP。</p>
<p>而智能体内部所使用的策略一般表现为 <span class="math inline">\(\pi(a \mid s)\)</span>，其表示了智能体在该策略条件下，在状态 <span class="math inline">\(s\)</span> 时做出决策 <span class="math inline">\(a\)</span> 的概率。</p>
<p>上述的策略往往称为<strong>非确定性策略</strong>，因为其对于某个状态给出的是做出某一种决策的概率，是不确定的。而另外一类策略，即<strong>确定性策略</strong>，会对于某个状态给出具体的某一种决策。确定性策略的记号，为了和非确定性策略的 <span class="math inline">\(\pi\)</span> 区别，往往使用 <span class="math inline">\(\mu\)</span>。而 <span class="math inline">\(a := \mu(s)\)</span> 就表示了智能体在该策略条件下，在状态 <span class="math inline">\(s\)</span> 时会做出决策 <span class="math inline">\(a\)</span>。</p>
<p>一条<strong>轨迹（Trajectory）</strong>，指的是智能体和环境不断交互的过程的记录。具体而言，环境首先位于状态 <span class="math inline">\(s_0\)</span>，智能体做出决策 <span class="math inline">\(a_0\)</span>，环境返回收益 <span class="math inline">\(r_1\)</span> 后转入新状态 <span class="math inline">\(s_1\)</span>。以此类推形成 <span class="math inline">\(s_0, a_0, r_1; s_1, a_1, r_2; \cdots\)</span>。轨迹常常使用记号 <span class="math inline">\(\tau\)</span> 表示。</p>
<p>显然根据 Markov 的特性，我们可以得到在策略 <span class="math inline">\(\pi\)</span> 的条件下某一条轨迹 <span class="math inline">\(\tau = (s_0, a_0, r_1; s_1, a_1, r_2; \cdots)\)</span> 出现的概率为：</p>
<p><span class="math display">\[
\P_\pi(\tau) = \Pe(s_0)\prod_{t = 0}^{+\infty} \pi(a_t \mid s_t)\Pe(s_{t + 1} \mid s_t, a_t)
\]</span></p>
<p>这里 <span class="math inline">\(\Pe(s_0)\)</span> 表示环境初始状态为 <span class="math inline">\(s_0\)</span> 的概率。</p>
<p>我们表示轨迹 <span class="math inline">\(\tau\)</span> 服从上述概率分布的时候，可以写成多种符号，包括 <span class="math inline">\(\tau \sim \P_\pi(\tau)\)</span> 或者 <span class="math inline">\(\tau \sim \pi\)</span> 等等，本文会使用第二种表达。</p>
<h2 id="累计收益与价值函数">累计收益与价值函数</h2>
<p>对一条轨迹，<strong>累计收益（Utility）</strong>的定义为：</p>
<p><span class="math display">\[
G_t(\tau) := \sum_{k = 0}^{+\infty} \gamma^k r_{t + k + 1}
\]</span></p>
<p>其中 <span class="math inline">\(t = 0\)</span> 时，<span class="math inline">\(G_0\)</span> 可以简写为 <span class="math inline">\(G\)</span>。</p>
<p>这里注意到我们累加收益的时候需要不断使用衰减因子进行衰减，这一操作主要是为了拟合现实中距离现在越远的行为对当前决策的影响越小的特征，这一操作同时也保证了收敛性。</p>
<p>之后，我们进一步定义<strong>价值函数</strong>。首先需要定义<strong>状态行为价值函数</strong>：</p>
<p><span class="math display">\[
Q^\pi(s, a) := \Eop_{\tau \sim \pi}[G(\tau) \mid s_0 = s, a_0 = a]
\]</span></p>
<p>进一步即可有<strong>状态价值函数</strong>：</p>
<p><span class="math display">\[
V^\pi(s) := \Eop_{\tau \sim \pi}[G(\tau) \mid s_0 = s]
\]</span></p>
<p>我们可以发现状态行为价值函数描述了在状态 <span class="math inline">\(s\)</span> 下做出决策 <span class="math inline">\(a\)</span> 后期望的累计收益，也就是说该价值函数评价了给定状态下的某个决策的期望收益，所以我们常常使用状态行为价值函数辅助决策。而状态价值函数则和具体的决策无关，是衡量从某一个状态出发，能够获得的期望收益。</p>
<p>显然这两个价值函数之间存在相互推出关系，而这种关系描述如下：</p>
<div class="note info no-icon"><p><strong>Theorem 1.01</strong> 对于任何 MDP 与其上的策略 <span class="math inline">\(\pi\)</span>，该策略的状态价值函数与状态行为价值函数满足：</p>
<p><span class="math display">\[
\begin{aligned}
{\color{red} V^\pi(s)} &amp;= \sum_{a \in \A} \pi(a \mid s){\color{red} Q^\pi(s, a)} \\
{\color{red} Q^\pi(s, a)} &amp;= \sum_{s&#39; \in \S} \Pe(s&#39; \mid s, a)[r(s, a, s&#39;) + \gamma {\color{red} V^\pi(s&#39;)}] \\
\end{aligned}
\]</span></p>
</div>
<p>第一个等式是由 <span class="math inline">\(Q^\pi\)</span> 推出 <span class="math inline">\(V^\pi\)</span> 的路径，证明也是简单的：</p>
<p><span class="math display">\[
\begin{aligned}
V^\pi(s) &amp;= \Eop_{\tau \sim \pi}[G(\tau) \mid s_0 = s] \\
&amp;= \sum_{\tau} \P_\pi(\tau \mid s_0 = s)G(\tau) \\
&amp;= \sum_{\tau} \left(\sum_{a \in \A} \P_\pi(\tau, a_0 = a \mid s_0 = s)\right)G(\tau) \\
&amp;= \sum_{\tau} \left(\sum_{a \in \A} \P_\pi(\tau \mid s_0 = s, a_0 = a)\P_\pi(a_0 = a \mid s_0 = s)\right)G(\tau) \\
&amp;= \sum_{a \in \A} \P_\pi(a_0 = a \mid s_0 = s) \left(\sum_{\tau} \P_\pi(\tau \mid s_0 = s, a_0 = a) G(\tau)\right) \\
&amp;= \sum_{a \in \A} \pi(a \mid s)\Eop_{\tau \sim \pi}[G(\tau) \mid s_0 = s, a_0 = a] \\
&amp;= \sum_{a \in \A} \pi(a \mid s)Q^\pi(s, a) \\
\end{aligned}
\]</span></p>
<p>第二个等式是由 <span class="math inline">\(V^\pi\)</span> 推出 <span class="math inline">\(Q^\pi\)</span> 的路径，其涉及到了后续状态，从而实际上描述出了价值函数之间的递推关系。</p>
<p>我们考虑其证明，首先我们显然有这个等式：</p>
<p><span class="math display">\[
r(s, a, s&#39;) = \Eop_{\tau \sim \pi}[r_1 \mid s_0 = s, a_0 = a, s_1 = s&#39;]
\]</span></p>
<p>之后考虑对状态行为价值函数的单步展开：</p>
<p><span class="math display">\[
\begin{aligned}
Q^\pi(s) &amp;= \Eop_{\tau \sim \pi}[G(\tau) \mid s_0 = s, a_0 = a] \\
&amp;= \Eop_{\tau \sim \pi}\left[\sum_{k = 0}^{+\infty} \gamma^k r_{k + 1} \middle| s_0 = s, a_0 = a\right] \\
&amp;= \Eop_{\tau \sim \pi}\left[r_1 + \sum_{k = 1}^{+\infty} \gamma^k r_{k + 1} \middle| s_0 = s, a_0 = a\right] \\
&amp;= \Eop_{\tau \sim \pi}\left[r_1 + \gamma\sum_{k = 0}^{+\infty} \gamma^k r_{k + 2} \middle| s_0 = s, a_0 = a\right] \\
&amp;= \Eop_{\tau \sim \pi}[r_1 \mid s_0 = s, a_0 = a] + \Eop_{\tau \sim \pi}\left[\gamma\sum_{k = 0}^{+\infty} \gamma^k r_{k + 2} \middle| s_0 = s, a_0 = a\right] \\
&amp;= \Eop_{\tau \sim \pi}[r_1 \mid s_0 = s, a_0 = a] + \gamma\Eop_{\tau \sim \pi}[G_1(\tau) \mid s_0 = s, a_0 = a] \\
\end{aligned}
\]</span></p>
<p>之后我们使用全期望公式处理条件期望中的条件：</p>
<p><span class="math display">\[
\begin{aligned}
\Eop_{\tau \sim \pi}[r_1 \mid s_0 = s, a_0 = a] &amp;= \sum_{s&#39; \in \S} \Pe(s&#39; \mid s, a) \Eop_{\tau \sim \pi}[r_1 \mid s_0 = s, a_0 = a, s_1 = s&#39;] \\
&amp;= \sum_{s&#39; \in \S} \Pe(s&#39; \mid s, a)r(s, a, s&#39;) \\
\Eop_{\tau \sim \pi}[G_1(\tau) \mid s_0 = s, a_0 = a] &amp;= \sum_{s&#39; \in \S} \Pe(s&#39; \mid s, a) \Eop_{\tau \sim \pi}[G_1(\tau) \mid s_0 = s, a_0 = a, s_1 = s&#39;] \\
&amp;= \sum_{s&#39; \in \S} \Pe(s&#39; \mid s, a) \Eop_{\tau \sim \pi}[G_1(\tau) \mid s_1 = s&#39;] \\
&amp;= \sum_{s&#39; \in \S} \Pe(s&#39; \mid s, a)V^\pi(s&#39;) \\
\end{aligned}
\]</span></p>
<p>这里注意到处理第二个条件期望的时候，直接删去了 <span class="math inline">\(s_0 = s\)</span> 和 <span class="math inline">\(a_0 = a\)</span> 两个条件，这是因为 <span class="math inline">\(G_1(\tau)\)</span> 与 <span class="math inline">\(s_0, a_0\)</span> 无关，可以直接删去而不影响最后的期望。</p>
<p>将此代入原有展开式即可证明完毕。</p>
<p>事实上，基于这两个价值函数的互相推出，我们可以得到 <span class="math inline">\(Q^\pi\)</span> 推出 <span class="math inline">\(Q^\pi\)</span> 的公式：</p>
<p><span class="math display">\[
{\color{red} Q^\pi(s, a)} = \sum_{s&#39; \in \S} \Pe(s&#39; \mid s, a) \left[r(s, a, s&#39;) + \gamma\sum_{a&#39; \in \A} \pi(a&#39; \mid s&#39;){\color{red} Q^\pi(s&#39;, a&#39;)}\right]
\]</span></p>
<p>以及 <span class="math inline">\(V^\pi\)</span> 推出 <span class="math inline">\(V^\pi\)</span> 的公式：</p>
<p><span class="math display">\[
{\color{red} V^\pi(s)} = \sum_{a \in \A} \pi(a \mid s) \sum_{s&#39; \in \S} \Pe(s&#39; \mid s, a)[r(s, a, s&#39;) + \gamma {\color{red} V^\pi(s&#39;)}]
\]</span></p>
<p>这两个公式直接表现出了价值函数自身具有的迭代结构，也就是当前强化学习中最为重要的方程之一，目前大部分强化学习方法的核心就是拟合该方程。该方程即 <strong>Bellman 方程（Bellman Equation）</strong>。</p>
<h2 id="基本优化理论">基本优化理论</h2>
<p>现在我们讨论如何获取一个最有价值的策略。</p>
<p>首先，我们定义<strong>最优状态行为价值函数</strong>和<strong>最优状态价值函数</strong>：</p>
<p><span class="math display">\[
\begin{aligned}
Q^\star(s, a) &amp;:= \max_\pi Q^\pi(s, a) \\
V^\star(s) &amp;:= \max_\pi V^\pi(s) \\
\end{aligned}
\]</span></p>
<p>基于最优性，我们可以定义策略之间的序。我们称策略 <span class="math inline">\(\pi\)</span> 优于 <span class="math inline">\(\pi&#39;\)</span>，记作 <span class="math inline">\(\pi \geq \pi&#39;\)</span>，当且仅当对于所有 <span class="math inline">\(s \in \S\)</span> 都满足 <span class="math inline">\(\pi\)</span> 的状态函数优于 <span class="math inline">\(\pi&#39;\)</span>，即 <span class="math inline">\(V^\pi(s) \geq V^{\pi&#39;}(s)\)</span>。</p>
<p>我们有下述基本优化理论：</p>
<div class="note info no-icon"><p><strong>Theorem 1.02</strong> 对于任何 MDP，都存在一个最优策略 <span class="math inline">\(\pi^\star\)</span> 满足对于任何 <span class="math inline">\(\pi\)</span> 都有 <span class="math inline">\(\pi^\star \geq \pi\)</span>。并且该最优策略 <span class="math inline">\(\pi^\star\)</span> 在在任何状态与任何决策下均实现了最优价值，即对于任何 <span class="math inline">\(s \in \S, a \in \A\)</span> 都有 <span class="math inline">\(V^{\pi^\star}(s) = V^\star(s), Q^{\pi^\star}(s, a) = Q^\star(s, a)\)</span>。</p>
</div>
<p>本文不加证明地认为该理论成立。另外，这里给出最优策略的构造。可见最优策略是确定性策略：</p>
<p><span class="math display">\[
\pi^\star(a \mid s) := \begin{cases}
1 &amp; a = \argmax_{a \in \A} Q^\star(s, a) \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>基于该定理我们能够得到两个最优价值函数之间存在的关系：</p>
<div class="note info no-icon"><p><strong>Theorem 1.03</strong> 对于任何 MDP，其最优状态价值函数与最优状态行为价值函数满足：</p>
<p><span class="math display">\[
\begin{aligned}
{\color{red} V^\star(s)} &amp;= \max_{a \in \A} {\color{red} Q^\star(s, a)} \\
{\color{red} Q^\star(s, a)} &amp;= \sum_{s&#39; \in \S} \Pe(s&#39; \mid s, a)[r(s, a, s&#39;) + \gamma {\color{red} V^\star(s&#39;)}] \\
\end{aligned}
\]</span></p>
</div>
<p>第一个等式是简单的：</p>
<p><span class="math display">\[
V^\star(s) = V^{\pi^\star}(s) = \sum_{a \in \A} \pi^\star(a \mid s)Q^{\pi^\star}(s, a) = \max_{a \in \A} Q^{\pi^\star}(s, a) = \max_{a \in \A} Q^\star(s, a)
\]</span></p>
<p>第二个等式直接对 <span class="math inline">\(\pi^\star\)</span> 应用一般的 Bellman 方程即可得到。</p>
<p>从而还有下述<strong>最优 Bellman 方程（Bellman Optimality Equation）</strong>：</p>
<p><span class="math display">\[
{\color{red} Q^\star(s, a)} = \sum_{s&#39; \in \S} \Pe(s&#39; \mid s, a) \left[r(s, a, s&#39;) + \gamma\max_{a&#39; \in \A} {\color{red} Q^\star(s&#39;, a&#39;)}\right]
\]</span></p>
<p>以及：</p>
<p><span class="math display">\[
{\color{red} V^\star(s)} = \max_{a \in \A} \left[\sum_{s&#39; \in \S} \Pe(s&#39; \mid s, a)[r(s, a, s&#39;) + \gamma {\color{red} V^\star(s&#39;)}]\right]
\]</span></p>
<p>理论上我们可以求解最优 Bellman 方程得到 <span class="math inline">\(V^\star\)</span>，从而就能够反推出 <span class="math inline">\(\pi^\star\)</span>。然而求解最优 Bellman 方程是计算困难的，现行的强化学习方法就是在尝试近似求解最优 Bellman 方程。</p>
<h1 id="基于完全环境知识的-rl-方法">基于完全环境知识的 RL 方法</h1>
<p>如果我们能够完全掌握环境，即完全掌握转移概率 <span class="math inline">\(\Pe\)</span> 和收益函数 <span class="math inline">\(r\)</span>，那么我们可以使用<strong>动态规划（Dynamic Programming, DP）</strong>来求解最优策略。DP 方法有两种类别，分别是<strong>策略迭代（Policy Iteration, PI）</strong>和<strong>值迭代（Value Iteration, VI）</strong>。</p>
<h2 id="policy-iteration-pi">Policy Iteration (PI)</h2>
<p>如果我们已经对环境有了完整的建模，我们完全可以使用迭代的方式求解最优 Bellman 方程。</p>
<p>具体的迭代过程分为两步，分别为<strong>策略评估（Policy Evaluation）</strong>和<strong>策略提升（Policy Improvement）</strong>，这两步会交替进行。具体而言，策略评估是在已知 <span class="math inline">\(\pi\)</span> 的条件下计算其状态价值函数 <span class="math inline">\(V^\pi\)</span>。而策略提升为在得知状态价值函数的基础上优化策略 <span class="math inline">\(\pi\)</span>。</p>
<p>首先阐述策略评估的过程，这里举出一个迭代计算的方式。</p>
<p>我们记 <span class="math inline">\(V_k\)</span> 是第 <span class="math inline">\(k\)</span> 轮迭代的时候的状态价值函数，那么我们在初始化 <span class="math inline">\(V_0\)</span> 之后不断进行下述迭代直到状态价值函数收敛：</p>
<p><span class="math display">\[
V_{k + 1}(s) \leftarrow \sum_{a \in \A} \pi(a \mid s) \sum_{s&#39; \in \S} \Pe(s&#39; \mid s, a)[r(s, a, s&#39;) + \gamma V_k(s&#39;)]
\]</span></p>
<p>收敛后，应当有 <span class="math inline">\(V_{+\infty} = V^\pi\)</span>，此时我们就得到了策略 <span class="math inline">\(\pi\)</span> 所对应的价值函数。</p>
<p>策略评估除了使用迭代的方式计算，也可以使用求解线性方程组等方式计算，这些方法我们不具体展开。</p>
<p>之后阐述策略提升的方法。</p>
<p>我们在得知了策略 <span class="math inline">\(\pi\)</span> 的状态价值函数 <span class="math inline">\(V^\pi\)</span> 之后，我们可以贪心地构造一个确定性策略 <span class="math inline">\(\pi&#39;\)</span>，保证策略更优。保证这一点能够成立的是下述定理：</p>
<div class="note info no-icon"><p><strong>Theorem 2.01 (Policy Improvement Theorem)</strong> 对于已知的策略 <span class="math inline">\(\pi\)</span>，定义确定性策略 <span class="math inline">\(\pi&#39;\)</span>：</p>
<p><span class="math display">\[
\pi&#39;(a \mid s) := \begin{cases}
1 &amp; a = \argmax_{a \in \A} Q^\pi(s, a) \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>那么 <span class="math inline">\(\pi&#39; \geq \pi\)</span>，并且等号成立当且仅当 <span class="math inline">\(\pi = \pi&#39; = \pi^\star\)</span>。</p>
</div>
<p>证明是简单的，首先我们可以注意到：</p>
<p><span class="math display">\[
Q^\pi(s, \pi&#39;(s)) = \max_{a \in \A} Q^\pi(s, a) \geq \sum_{a \in \A} \pi(a \mid s)Q^\pi(s, a) = V^\pi(s)
\]</span></p>
<p>我们从而能得到下述不等式。这里事实上进行了单步展开，进一步就可以尝试利用迭代结构构造不等式链，具体细节则在后续证明中说明：</p>
<p><span class="math display">\[
\begin{aligned}
V^\pi(s) &amp;\leq Q^\pi(s, \pi&#39;(s)) \\
&amp;= \sum_{s&#39; \in \S} \Pe(s&#39; \mid s, \pi&#39;(s))[r(s, \pi&#39;(s), s&#39;) + \gamma V^\pi(s&#39;)] \\
&amp;= \sum_{s&#39; \in \S} \Pe(s&#39; \mid s, \pi&#39;(s))r(s, \pi&#39;(s), s&#39;) + \gamma\sum_{s&#39; \in \S} \Pe(s&#39; \mid s, \pi&#39;(s)) V^\pi(s&#39;) \\
\end{aligned}
\]</span></p>
<p>另外，我们可以证明：</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\sum_{s&#39; \in \S} \Pe(s&#39; \mid s, \pi&#39;(s))r(s, \pi&#39;(s), s&#39;) \\
=&amp; \sum_{s&#39; \in \S} \Pe(s&#39; \mid s, \pi&#39;(s))\Eop_{\tau \sim \pi&#39;}[r_1 \mid s_0 = s, a_0 = \pi&#39;(s), s_1 = s&#39;] \\
=&amp; \sum_{a \in \A} \pi&#39;(a \mid s)\sum_{s&#39; \in \S} \Pe(s&#39; \mid s, a)\Eop_{\tau \sim \pi&#39;}[r_1 \mid s_0 = s, a_0 = a, s_1 = s&#39;] \\
=&amp; \Eop_{\tau \sim \pi&#39;}[r_1 \mid s_0 = s]
\end{aligned}
\]</span></p>
<p>即得到：</p>
<p><span class="math display">\[
V^\pi(s) \leq \Eop_{\tau \sim \pi&#39;}[r_1 \mid s_0 = s] + \gamma\sum_{s&#39; \in \S} \Pe(s&#39; \mid s, \pi&#39;(s)) V^\pi(s&#39;)
\]</span></p>
<p>对这个不等式迭代展开：</p>
<p><span class="math display">\[
\begin{aligned}
V^\pi(s) &amp;\leq \Eop_{\tau \sim \pi&#39;}[r_1 \mid s_0 = s] + \gamma\sum_{s&#39; \in \S} \Pe(s&#39; \mid s, \pi&#39;(s)) V^\pi(s&#39;) \\
&amp;\leq \Eop_{\tau \sim \pi&#39;}[r_1 \mid s_0 = s] + \gamma\sum_{s&#39; \in \S} \Pe(s&#39; \mid s, \pi&#39;(s)) \left[\Eop_{\tau \sim \pi&#39;}[r_2 \mid s_1 = s&#39;] + \gamma\sum_{s&#39;&#39; \in \S} \Pe(s&#39;&#39; \mid s&#39;, \pi&#39;(s&#39;)) V^\pi(s&#39;&#39;)\right] \\
&amp;= \Eop_{\tau \sim \pi&#39;}[r_1 \mid s_0 = s] + \gamma{\color{red} \sum_{s&#39; \in \S} \Pe(s&#39; \mid s, \pi&#39;(s))\Eop_{\tau \sim \pi&#39;}[r_2 \mid s_1 = s&#39;]} + \gamma^2\sum_{s&#39; \in \S}\sum_{s&#39;&#39; \in \S}\Pe(s&#39;&#39; \mid s&#39;, \pi&#39;(s&#39;)) V^\pi(s&#39;&#39;) \\
\end{aligned}
\]</span></p>
<p>对于红色部分，我们容易发现 <span class="math inline">\(s_0 = s\)</span> 对于 <span class="math inline">\(r_2\)</span> 的期望是无效条件，所以：</p>
<p><span class="math display">\[
\sum_{s&#39; \in \S} \Pe(s&#39; \mid s, \pi&#39;(s))\Eop_{\tau \sim \pi&#39;}[r_2 \mid s_1 = s&#39;] = \sum_{s&#39; \in \S} \Pe(s&#39; \mid s, \pi&#39;(s))\Eop_{\tau \sim \pi&#39;}[r_2 \mid s_0 = s, s_1 = s&#39;] = \Eop_{\tau \sim \pi&#39;}[r_2 \mid s_0 = s]
\]</span></p>
<p>所以：</p>
<p><span class="math display">\[
V^\pi(s) \leq \Eop_{\tau \sim \pi&#39;}[{\color{red} r_1 + \gamma r_2} \mid s_0 = s] + {\color{green} \gamma^2\sum_{s&#39; \in \S}\sum_{s&#39;&#39; \in \S}\Pe(s&#39;&#39; \mid s&#39;, \pi&#39;(s&#39;)) V^\pi(s&#39;&#39;)}
\]</span></p>
<p>我们接下来展开绿色部分的时候，可以注意到我们需要为 <span class="math inline">\(r_3\)</span> 的条件期望补上 <span class="math inline">\(s_0 = s\)</span> 和 <span class="math inline">\(s_1 = s&#39;\)</span> 两个无效条件，然后两个求和号通过全期望公式消去 <span class="math inline">\(s_1 = s&#39;\)</span> 以及 <span class="math inline">\(s_2 = s&#39;&#39;\)</span> 两个条件，故剩余的条件仅有 <span class="math inline">\(s_0 = s\)</span>，从而可以进一步合并到红色部分。以此类推，我们就能够说明该不等式的迭代展开是可行的，从而得到：</p>
<p><span class="math display">\[
V^\pi(s) \leq \Eop_{\tau \sim \pi&#39;}[{\color{red} r_1 + \gamma r_2 + \gamma^2r_3 + \cdots} \mid s_0 = s] = V^{\pi&#39;}(s)
\]</span></p>
<p>从而欲证明的不等式成立。</p>
<p>而根据证明过程，该不等式取等当且仅当满足 <span class="math inline">\(V^\pi(s) = Q^\pi(s, \pi&#39;(s)) = \max_{a \in \A}Q^\pi(s, a)\)</span>，而这正是最优 Bellman 方程。也就是说我们使得 <span class="math inline">\(V^\pi = V^\star\)</span>。此外，<span class="math inline">\(\pi\)</span> 目前也必须是一个确定性策略，从而也就保证了 <span class="math inline">\(\pi = \pi^\star\)</span>。</p>
<p>综合策略评估和策略提升，交替进行两者我们就可以保证能够获取到最优策略。然而这个方法并不现实，因为理论上我们需要对环境有完全把握，至少我们需要得到整个环境状态转移矩阵才能进行。</p>
<h2 id="value-iteration-vi">Value Iteration (VI)</h2>
<p>同样，在已经完全得知环境的条件下，除了策略迭代，我们还可以用值迭代求解最优策略。相较于策略迭代是迭代策略本身，值迭代则是在迭代求解状态价值函数。</p>
<p>考虑最优 Bellman 方程：</p>
<p><span class="math display">\[
{\color{red} V^\star(s)} = \max_{a \in \A} \left[\sum_{s&#39; \in \S} \Pe(s&#39; \mid s, a)[r(s, a, s&#39;) + \gamma {\color{red} V^\star(s&#39;)}]\right]
\]</span></p>
<p>也就是说我们完全可以直接先随意初始化一个 <span class="math inline">\(V_0 \equiv 0\)</span>，然后不断使用上述方程做迭代直到收敛，此时 <span class="math inline">\(V_{+\infty} = V^\star\)</span>。迭代的流程描述为：</p>
<p><span class="math display">\[
{\color{red} V_{k + 1}(s)} \leftarrow \max_{a \in \A} \left[\sum_{s&#39; \in \S} \Pe(s&#39; \mid s, a)[r(s, a, s&#39;) + \gamma {\color{red} V_k(s&#39;)}]\right]
\]</span></p>
<p>重复该迭代直到收敛，即可获得最优状态价值函数。基于最优状态价值函数，可以推知最优策略，从而算法求解完毕。</p>
<p>这里需要说明的一点是，使用值迭代求解的时候，策略是比价值函数先收敛的。这一点是显然的，因为通过价值函数获取对应的策略是贪心的。从而，在价值函数还未实际完全收敛的时候，各个状态的价值大小关系很有可能已经确定且不变化，从而从这一时间点开始，推知的策略都不会发生变化，理论上都是最优策略。</p>
<p>值迭代的时间复杂度为 <span class="math inline">\(O(|\S|^2|\A|)\)</span>。</p>
<h2 id="有关-pi-和-vi-的一些对比">有关 PI 和 VI 的一些对比</h2>
<p>事实上基于 Bellman 方程而言，VI 方法是最直观也是最容易想到的，因为通过迭代法求解有递归结构的方程确实是一种很常见的方法。然而 VI 的两个特点也是限制其运用的缺陷，即<strong>策略早于价值函数收敛</strong>和<strong>复杂度较高</strong>。</p>
<p>前者事实上是 VI 的一个致命缺陷，因为从根本上而言我们需要获取的是最优策略，具体的价值函数可能并不关注，或者至少不是最主要目标。另外，高复杂度也让 VI 的实际运算难度较高，而这也一定程度上催生了 PI 方法。</p>
<p>VI 在运算过程中有一步涉及到了 <span class="math inline">\(\max_{a \in \A}\)</span>，这需要遍历行为空间却没有有效利用遍历的信息（仅取了最大值）。那么一个显然的性能换时间的操作就是我们取一个并非最优但足够优的策略 <span class="math inline">\(\pi: \S \to \A\)</span>，直接用 <span class="math inline">\(a := \pi(s)\)</span> 代替掉 <span class="math inline">\(a := \argmax_{a \in \A}\)</span>，这样复杂度直接就降低到 <span class="math inline">\(O(|\S|^2)\)</span>。</p>
<p>为了保证 <span class="math inline">\(\pi\)</span> 能够足够优，那我们可以两路并进，一路优化 <span class="math inline">\(\pi\)</span>，一路更新 <span class="math inline">\(V^\pi\)</span>，从而我们就得到了 PI 方法。PI 方法在部分条件下会比 VI 方法收敛快很多，并且我们依然可以证明 PI 理论可以得到最优策略。</p>
<h2 id="有关-bellman-算子的拓展">有关 Bellman 算子的拓展</h2>
<p>如果仔细观察 Bellman 方程，其事实上就是一个线性方程：</p>
<p><span class="math display">\[
{\color{red} V^\pi(s)} = \sum_{a \in \A} \pi(a \mid s) \sum_{s&#39; \in \S} \Pe(s&#39; \mid s, a)[r(s, a, s&#39;) + \gamma {\color{red} V^\pi(s&#39;)}]
\]</span></p>
<p>首先作一些记号简化：</p>
<p><span class="math display">\[
\begin{aligned}
\P^\pi(s&#39; \mid s) &amp;:= \sum_{a \in \A} \pi(a \mid s)\Pe(s&#39; \mid s, a) \\
r^\pi(s) &amp;:= \sum_{a \in \A} \pi(a \mid s) \sum_{s&#39; \in \S} \Pe(s&#39; \mid s, a)r(s, a, s&#39;) \\
\end{aligned}
\]</span></p>
<p>得到：</p>
<p><span class="math display">\[
{\color{red} V^\pi(s)} = r^\pi(s) + \gamma\sum_{s&#39; \in \S} \P^\pi(s&#39; \mid s){\color{red} V^\pi(s&#39;)}
\]</span></p>
<p>如果记 <span class="math inline">\(\S = \{s_1, s_2, \cdots, s_n\}, n = |\S|\)</span>，并定义矩阵：</p>
<p><span class="math display">\[
\begin{aligned}
\b{V}^\pi &amp;:= (v^\pi_i)^T_{n} \in \R^n &amp;&amp; v^\pi_i := V^\pi(s_i) \\
\b{R}^\pi &amp;:= (r^\pi_i)^T_{n} \in \R^n &amp;&amp; r^\pi_i := r^\pi(s_i) \\
\b{P}^\pi &amp;:= (p^\pi_{ij})_{n \times n} \in \R^{n \times n} &amp;&amp; p^\pi_{ij} := \P^\pi(s_j \mid s_i) \\
\end{aligned}
\]</span></p>
<p>那么 Bellman 方程等价于：</p>
<p><span class="math display">\[
\b{V}^\pi = \b{R}^\pi + \gamma\b{P}^\pi\b{V}^\pi \Rightarrow (I - \gamma\b{P}^\pi)\b{V}^\pi = \b{R}^\pi
\]</span></p>
<p>这里就可以引入 Bellman 算子：</p>
<p><span class="math display">\[
\mathcal{T}^\pi(\b{x}) := \b{R}^\pi + \gamma\b{P}^\pi\b{x}
\]</span></p>
<p>如果状态空间足够小，那么在 PI 的策略评估阶段，事实上完全可以通过求解上述线性方程组解的方式得到策略的价值函数。</p>
<p>除了 Bellman 算子，还可以从最优 Bellman 方程引入最优 Bellman 算子。首先依然是定义矩阵：</p>
<p><span class="math display">\[
\begin{aligned}
\b{V}^\star &amp;:= (v^\star_i)^T_{n} \in \R^n &amp;&amp; v^\star_i := V^\star(s_i) \\
\b{R}(a) &amp;:= (r_i(a))^T_{n} \in \R^n &amp;&amp; r_i(a) := \sum_{s_j \in \S} \Pe(s_j \mid s_i, a)r(s_i, a, s_j) \\
\b{P}(a) &amp;:= (p_{ij}(a))_{n \times n} \in \R^{n \times n} &amp;&amp; p_{ij}(a) := \Pe(s_j \mid s_i, a) \\
\end{aligned}
\]</span></p>
<p>那么最优 Bellman 算子就定义为：</p>
<p><span class="math display">\[
\mathcal{T}^\star(\b{x}) := \max_{a \in \A}(\b{R}(a) + \gamma\b{P}(a)\b{x})
\]</span></p>
<p>现在我们需要证明一个我们并没有严格说明的引理，即使用迭代法求解价值函数的合法性，无论是 PI 中策略评估还是 VI 求最优价值函数本身。事实上，这两者本质相同，其都在求解 Bellman 算子的不动点，只不过一个是一般 Bellman 算子，一个是最优 Bellman 算子。这里以最优 Bellman 算子为例进行证明。</p>
<p>证明的主要思想就是 Banach 空间不动点定理。</p>
<div class="note info no-icon"><p><strong>Theorem 2.02 (Banach Fixed-point Theorem)</strong> 给定完备度量空间 <span class="math inline">\(X\)</span> 以及其范数 <span class="math inline">\(d\)</span>，假设 <span class="math inline">\(f: X \to X\)</span> 是一个压缩映射，即存在 <span class="math inline">\(\gamma \in [0, 1)\)</span> 令 <span class="math inline">\(\forall x_1, x_2 \in X\)</span> 都有：</p>
<p><span class="math display">\[
d(f(x_1), f(x_2)) \leq \gamma d(x_1, x_2)
\]</span></p>
<p>那么 <span class="math inline">\(f\)</span> 在 <span class="math inline">\(X\)</span> 上存在唯一不动点 <span class="math inline">\(x^*\)</span>。</p>
<p>该不动点满足，任取 <span class="math inline">\(x_0 \in X\)</span>，定义序列 <span class="math inline">\(\{x_n\}_{n \geq 0}\)</span>，其中 <span class="math inline">\(x_{n + 1} = f(x_n), n \geq 0\)</span>，那么该序列必然收敛到 <span class="math inline">\(x^*\)</span>。</p>
</div>
<p>证明是简单的。首先我们任取 <span class="math inline">\(x_0 \in X\)</span>，考虑对 <span class="math inline">\(m &gt; n\)</span>：</p>
<p><span class="math display">\[
\begin{aligned}
d(f^{(m)}(x_0), f^{(n)}(x_0)) &amp;\leq d(f^{(m)}(x_0), f^{(m - 1)}(x_0)) + d(f^{(m - 1)}(x_0), f^{(m - 2)}(x_0)) + \cdots + d(f^{(n + 1)}(x_0), f^{(n)}(x_0)) \\
&amp;\leq \gamma^{m - 1}d(f(x_0), x_0) + \gamma^{m - 2}d(f(x_0), x_0) + \cdots + \gamma^nd(f(x_0), x_0) \\
&amp;\leq \dfrac{\gamma^n}{1 - \gamma}d(f(x_0), x_0) \\
\end{aligned}
\]</span></p>
<p>也就是无论给多紧的界，我总能令 <span class="math inline">\(n\)</span> 充分大满足上界约束，从而该序列是 Cauchy 序列，在完备度量空间上其自然是收敛的。</p>
<p>假定这个序列收敛到 <span class="math inline">\(x^*\)</span>，我们需要说明 <span class="math inline">\(x^*\)</span> 是 <span class="math inline">\(f\)</span> 的不动点以及其唯一性。其是不动点是容易的，考虑：</p>
<p><span class="math display">\[
0 \leq d(x_{n + 1}, f(x^*)) = d(f(x_n), f(x^*)) \leq \gamma d(x_n, x^*)
\]</span></p>
<p>由于 <span class="math inline">\(n \to \infty\)</span> 时 <span class="math inline">\(d(x_n, x^*) \to 0\)</span>，这说明 <span class="math inline">\(d(x_{n + 1}, f(x^*)) \to 0\)</span>，也就说明 <span class="math inline">\(f(x^*)\)</span> 也是序列 <span class="math inline">\(\{x_n\}_{n \geq 0}\)</span> 的极限，据极限的唯一性，不动点得证。</p>
<p>唯一性则据反证。若存在第二个不动点 <span class="math inline">\(f(y) = y\)</span>，那么：</p>
<p><span class="math display">\[
d(x^*, y) = d(f(x^*), f(y)) \leq \gamma d(x^*, y) \Rightarrow (1 - \gamma)d(x^*, y) \leq 0
\]</span></p>
<p>上述约束仅有 <span class="math inline">\(y = x^*\)</span> 时成立，唯一性得证。</p>
<hr />
<p>基于该引理，我们只需要说明最优 Bellman 算子也满足引理所述条件即可。在空间 <span class="math inline">\(\S\)</span> 上取其 <span class="math inline">\(L^{\infty}\)</span> 范数（即各分量最大值）。我们证明最优 Bellman 算子是压缩的：</p>
<p><span class="math display">\[
\begin{aligned}
\|\mathcal{T}^\star(\b{V}_1) - \mathcal{T}^\star(\b{V}_2)\|_\infty &amp;= \left\|\max_{a \in \A}(\b{R}(a) + \gamma\b{P}(a)\b{V}_1) - \max_{a \in \A}(\b{R}(a) + \gamma\b{P}(a)\b{V}_2)\right\|_\infty \\
&amp;\leq \left\|\max_{a \in \A}(\b{R}(a) + \gamma\b{P}(a)\b{V}_1 - \b{R}(a) - \gamma\b{P}(a)\b{V}_2)\right\|_\infty \\
&amp;= \gamma\left\|\max_{a \in \A}(\b{P}(a)(\b{V}_1 - \b{V}_2))\right\|_\infty \\
&amp;\leq \gamma\|\b{V}_1 - \b{V}_2\|_\infty
\end{aligned}
\]</span></p>
<p>最后一个不等号源于 <span class="math inline">\(\b{P}(a)\)</span> 矩阵中，每一行的元素之和必然为 <span class="math inline">\(1\)</span>。如果假设 <span class="math inline">\(\b{V}_1 - \b{V}_2\)</span> 各元素中最大为 <span class="math inline">\(p\)</span>（亦其 <span class="math inline">\(L^\infty\)</span> 范数），那么 <span class="math inline">\(\b{P}(a)(\b{V}_1 - \b{V}_2)\)</span> 每一个元素都不会大于 <span class="math inline">\(p\)</span>。</p>
<p>从而得证。</p>
<p>事实上我们有下述定理，描述了迭代法求解的正确性。对于任意初始化的 <span class="math inline">\(\b{V}_0\)</span>：</p>
<p><span class="math display">\[
\lim_{n \to \infty} \mathcal{T}^{\pi(n)}\b{V}_0 = \b{V}^\pi, \lim_{n \to \infty} \mathcal{T}^{\star(n)}\b{V}_0 = \b{V}^\star
\]</span></p>
<h1 id="基于环境交互的-rl-方法">基于环境交互的 RL 方法</h1>
<p>如果我们无法掌握环境的信息，我们就需要令智能体与环境交互以收集信息。而这也是当前绝大多数 RL 问题需要采用的方法，因为我们完全能掌握的环境几乎不存在。</p>
<p>这里简单提一下<strong>无模型（Model-free）</strong>与<strong>基于模型（Model-based）</strong>的概念。在智能体和环境交互的过程中，事实上我们有两种选择，其一是让智能体去和真实的环境作交互，这就是无模型方法。其二是我们事先使用另外一个模型去拟合环境，让智能体和这个模型交互，这就是基于模型方法。基于模型方法的优势在于，如果智能体和真实的环境交互成本高昂或者环境响应较慢，则可以用于大幅降低实验成本。但相应地，基于模型方法也需要一个相当优越的和环境契合的模型才能让智能体真正学习到最优策略。</p>
<p>基于环境交互的 RL 方法的流程事实上近似于策略迭代，也被称为<strong>广义策略迭代（General Policy Iteration, GPI）</strong>。GPI 的流程是首先初始化一个价值函数和策略 <span class="math inline">\(V, \pi\)</span>，并不断重复下述两个操作：</p>
<ul>
<li>（策略评估）更新价值函数使得其符合当前的策略，即 <span class="math inline">\(V = V^\pi\)</span></li>
<li>（策略改进）根据当前价值函数贪心地更新策略，即 <span class="math inline">\(\pi = {\rm greedy}(V)\)</span></li>
</ul>
<p>当然，这里是以状态价值函数 <span class="math inline">\(V\)</span> 作为比方，事实上状态行为价值函数 <span class="math inline">\(Q\)</span> 也可以应用到上述流程中。</p>
<p>具体讲解之前，首先引入<strong>同轨（On-policy）</strong>和<strong>异轨（Off-policy）</strong>的概念。这类 RL 方法中智能体所执行的策略一般有两种用途，其一是用于与环境交互收集转移轨迹，这个策略称为<strong>采样策略（Sample Policy）</strong>或者<strong>行为策略（Behavioral Policy）</strong>，其二是用于在环境中获取收益，这个策略称为<strong>目标策略（Target Policy）</strong>。如果一个智能体的行为策略和目标策略是不一致的，那这个方法就是异轨的，相对应地，行为策略和目标策略一致的时候这个方法是同轨的。</p>
<p>如果采用同轨方法，那么策略评估、策略改进的流程就是简单的，也就是按照描述迭代即可。然而如果采用异轨方法，通用的方法是：</p>
<ul>
<li>策略评估时应当估算<strong>目标策略</strong>的价值函数</li>
<li>策略改进时应当贪心地生成<strong>行为策略</strong></li>
</ul>
<h2 id="策略评估">策略评估</h2>
<h3 id="monte-carlo-mc">Monte-Carlo (MC)</h3>
<p>MC 方法计算目标策略的价值函数的方法非常简单，即使用行为策略直接采样一整条轨迹，利用这一条轨迹上的信息更新价值函数。更新方式也相对简单，直接使用算术平均值或者利用学习率更新即可。设定固定的学习率 <span class="math inline">\(\alpha\)</span>，若行为策略采样得到的轨迹为 <span class="math inline">\(\tau\)</span>，对该轨迹上任何一个状态 <span class="math inline">\(s_t\)</span>，MC 方法的更新公式为：</p>
<p><span class="math display">\[
V(s_t) \leftarrow V(s_t) + \alpha(G_t(\tau) - V(s_t))
\]</span></p>
<p>理论而言，MC 方法相当简易且不涉及到 bootstrap。这里 bootstrap 指的是一类“利用估计值本身更新估计值”的更新策略，在后续 TD 方法中展开说明。另一方面，MC 方法也因为低效和并没有充分利用 Bellman 方程而具有缺陷。</p>
<div class="note info no-icon"><p>GPI 中使用 MC 方法做策略评估的算法一般称为 <strong>蒙特卡洛控制算法</strong>。</p>
</div>
<h3 id="temporal-difference-td">Temporal-Difference (TD)</h3>
<p>TD 方法与 MC 方法目标是一致的，也是尽力取得价值函数的准确估计。但是两者的核心差别在于，TD 方法是一个更新力度更小的 bootstrap 方法，TD 方法会在每一次决策后进行更新。这种逐步更新的问题在于我们无法精确获取某一个状态的累计收益，所以我们需要虚化一个更新目标，这也就是所谓的 TD target。</p>
<p>假设我们在时刻 <span class="math inline">\(t\)</span> 时位于状态 <span class="math inline">\(s_t\)</span>，我们通过决策行为 <span class="math inline">\(a_t\)</span> 迁移到了状态 <span class="math inline">\(s_{t + 1}\)</span>，并且获取收益 <span class="math inline">\(r_{t + 1}\)</span>。另外，我们保存有一个所有状态的价值函数估计值表 <span class="math inline">\(V(s), s \in \S\)</span>。那么 TD target 指的是：</p>
<p><span class="math display">\[
{\rm TD\ target} := r_{t + 1} + \gamma V(s_{t + 1})
\]</span></p>
<p>可以注意到，TD target 本身是利用类似 Bellman 方程中的步进展开方法，通过 <span class="math inline">\(V(s_{t + 1})\)</span> 对 <span class="math inline">\(V(s_t)\)</span> 作出估计，并以此作为 <span class="math inline">\(V(s_t)\)</span> 更新的目标。这和 MC 使用真实采样获取到真实的累计收益作为更新目标完全不同，这也就是 bootstrap 的含义。</p>
<p>在获取 TD target 后，就可以计算目标和当前估计值的差，即 TD error：</p>
<p><span class="math display">\[
{\rm TD\ error} = \delta_t := r_{t + 1} + \gamma V(s_{t + 1}) - V(s_t)
\]</span></p>
<p>之后，设定一个适当的学习率 <span class="math inline">\(\alpha\)</span>，即可得到 TD 方法的核心更新策略，下述这种简单的更新策略也被称为 TD(0) 方法：</p>
<p><span class="math display">\[
V(s_t) \leftarrow V(s_t) + \alpha\delta_t = V(s_t) + \alpha(r_{t + 1} + \gamma V(s_{t + 1}) - V(s_t))
\]</span></p>
<p>从理论上比较 TD 方法与 MC 方法。MC 方法使用的累计收益 <span class="math inline">\(G_t(\tau)\)</span> 是对价值函数 <span class="math inline">\(V^\pi(s)\)</span> 的无偏估计，而 TD 方法使用的 TD target 则相应是有偏估计。然而就估计方差而言，TD 方法由于仅仅涉及到一步决策，其方差显然较 MC 方法小。就初始值方面而言，MC 方法对估计的初始值设置并不敏感，然而 TD 由于基于 bootstrap，故对初始值设置极为敏感。</p>
<p>如果简单总结上述方法，可以使用下述图：</p>
<p><img src="/uploads/note-of-rl/1.webp" /></p>
<p>简而言之，MC 方法较 TD 方法而言有着更深的探索。而 DP 方法（尤其是 VI）则是遍历所有可能的后继状态进行更新，从而具有更广泛的视角，也就拥有广度。与这三者均不同的则是搜索，其需要完整探索整个决策树，但这自然是相当浪费资源的。</p>
<div class="note info no-icon"><p>GPI 中使用 TD 方法做策略评估的算法一般称为 <strong>TD 控制算法</strong>或者<strong>时序差分控制算法</strong>。</p>
</div>
<h4 id="多步-td-target-与-tdlambda">多步 TD target 与 TD(<span class="math inline">\(\lambda\)</span>)</h4>
<p>TD target 事实上可以设置为多步的。传统的 TD target 是单步的，即仅仅考虑轨迹上的一步转移。考虑多步 TD target，不妨考虑三步，假定轨迹上有三步转移片段 <span class="math inline">\(s_t, a_t, r_{t + 1}; s_{t + 1}, a_{t + 1}, r_{t + 2}; s_{t + 2}, a_{t + 2}, r_{t + 3}; s_{t + 3}\)</span>，那么 TD target 可以定义为：</p>
<p><span class="math display">\[
{\rm TD\ target} := r_{t + 1} + \gamma r_{t + 2} + \gamma^2 r_{t + 3} + \gamma^3 V(s_{t + 3})
\]</span></p>
<p>这也可以称为三步 TD target。事实上，如果令考虑的步数趋向于无限，无穷步 TD target 也就成为了事实的累计收益，TD 方法转化为 MC 方法。</p>
<p>我们先前提到过，TD 方法对估计值初始化敏感，但是其效率较高。另一方面，MC 方法对初始值不敏感，但是效率较低。我们事实上可以使用一种方法将其两者结合，即基于介于 TD 与 MC 方法之间的多步 TD target，利用一定的权重将其组合，作为新的 TD target，这就是 TD(<span class="math inline">\(\lambda\)</span>) 方法。</p>
<p>将 <span class="math inline">\(n\)</span> 步 TD target 记为 <span class="math inline">\(G^{(n)}_t\)</span>，那么 TD(<span class="math inline">\(\lambda\)</span>) 中的 TD target 定义为：</p>
<p><span class="math display">\[
G^\lambda_t := (1 - \lambda)\sum_{n = 1}^{+\infty}\lambda^{n - 1}G_t^{(n)}
\]</span></p>
<p>传统的 TD 方法即 <span class="math inline">\(\lambda = 0\)</span> 的 TD(<span class="math inline">\(\lambda\)</span>) 方法。</p>
<h4 id="backward-view-tdlambda-eligibility-traces">Backward-view TD(<span class="math inline">\(\lambda\)</span>) &amp; Eligibility Traces</h4>
<p>然而使用 <span class="math inline">\(\lambda \neq 0\)</span> 的 TD(<span class="math inline">\(\lambda\)</span>) 方法的时候显然存在一个问题，那就是由于 <span class="math inline">\(G^\lambda_t\)</span> 的求和上限是 <span class="math inline">\(+\infty\)</span>，这就代表我们需要让智能体在环境中一直采样到轨迹终止。这样的话事实上就违背了使用 TD(<span class="math inline">\(\lambda\)</span>) 方法的初衷，我们希望在保持 TD 方法的效率的基础上引入多步实际采样来做到更精准的估计，所以要求采样到轨迹终止是不能容忍的。</p>
<p>一种简单的解决方式，那就是用 Backward-view TD(<span class="math inline">\(\lambda\)</span>) 取代上面的 Forward-view TD(<span class="math inline">\(\lambda\)</span>)。Forward-view 的含义是通过未来的 <span class="math inline">\(s_{t + 1}, s_{t + 2}, \cdots\)</span> 来更新 <span class="math inline">\(V(s_t)\)</span> 的估计。而 Backward-view 与之相反，其通过之前经历的状态和轨迹确定价值函数估计。</p>
<p>一种直观的解释是，Backward-view 就是反思先前的哪一个决策是导致到达当前状态的核心原因。</p>
<p>我们定义<strong>效用函数（Eligibility）</strong>为 <span class="math inline">\(E_t(s), s \in \S\)</span>，这里下标 <span class="math inline">\(t\)</span> 表示不同的时间的效用函数都是不同的。我们引入效用函数的目的是将其作为权重引入价值函数更新流程中，表示 TD error 以多大程度影响价值函数估计：</p>
<p><span class="math display">\[
V(s_t) \leftarrow V(s_t) + \alpha\delta_tE_t(s_t) = V(s_t) + \alpha E_t(s_t)(r_{t + 1} + \gamma V(s_{t + 1}) - V(s_t))
\]</span></p>
<p>然后，定义效用函数的初始化和更新原则：</p>
<p><span class="math display">\[
\begin{aligned}
&amp;E_0(s) = 0, \forall s \in \S \\
&amp;E_t(s) = \gamma\lambda E_{t - 1}(s) + \b{1}(s_t = s), \forall t \in \mathbb{N}^+, s \in \S \\
\end{aligned}
\]</span></p>
<p>现在简单从数学角度说明使用效用函数的 Backward-view TD(<span class="math inline">\(\lambda\)</span>) 与 Forward-view TD(<span class="math inline">\(\lambda\)</span>) 方法等价。为了简化，这里假设轨迹中经历过的状态后续不在经过。假设我们在 <span class="math inline">\(k\)</span> 时刻到达了状态 <span class="math inline">\(s_k\)</span>，那么：</p>
<p><span class="math display">\[
E_t(s_k) = \begin{cases}
0 &amp; t &lt; k \\
(\gamma\lambda)^{t - k} &amp; t \geq k \\
\end{cases}
\]</span></p>
<p>那么使用效用函数时，我们在迭代终止时得到的状态 <span class="math inline">\(s_k\)</span> 的价值函数估计为（价值函数上标 <span class="math inline">\({\rm B}\)</span> 表示后向视角）：</p>
<p><span class="math display">\[
V_{+\infty}^{\rm B}(s_k) = V_0(s_k) + \sum_{t = 1}^{+\infty} \alpha\delta_tE_t(s_k) = V_0(s_k) + \alpha\sum_{t = k}^{+\infty} (\gamma\lambda)^{t - k}\delta_t = V_0(s_k) + \alpha\sum_{t = 0}^{+\infty} (\gamma\lambda)^t\delta_{t + k}
\]</span></p>
<p>考虑下述计算：</p>
<p><span class="math display">\[
\begin{aligned}
G_t^\lambda - V_t(s_t) &amp;= -V_t(s_t) + (1 - \lambda)\sum_{n = 1}^{+\infty}\lambda^{n - 1}G_t^{(n)} \\
&amp;= -V_t(s_t) + (1 - \lambda)\sum_{n = 1}^{+\infty}\lambda^{n - 1}\left(\gamma^nV_t(s_{t + n}) + \sum_{i = 1}^n \gamma^{i - 1}r_{t + i}\right) \\
&amp;= -V_t(s_t) + \sum_{n = 1}^{+\infty}\lambda^{n - 1}\left(\gamma^nV_t(s_{t + n}) + \sum_{i = 1}^n \gamma^{i - 1}r_{t + i}\right) -  \sum_{n = 1}^{+\infty}\lambda^n\left(\gamma^nV_t(s_{t + n}) + \sum_{i = 1}^n \gamma^{i - 1}r_{t + i}\right) \\
&amp;= -V_t(s_t) + \sum_{n = 0}^{+\infty}\lambda^n\left(\gamma^{n + 1}V_t(s_{t + n + 1}) + \sum_{i = 1}^{n + 1} \gamma^{i - 1}r_{t + i}\right) -  \sum_{n = 1}^{+\infty}\lambda^n\left(\gamma^nV_t(s_{t + n}) + \sum_{i = 1}^n \gamma^{i - 1}r_{t + i}\right) \\
&amp;= -V_t(s_t) + \gamma V_t(s_{t + 1}) + r_{t + 1} + \sum_{n = 1}^{+\infty} \lambda^n\left(\gamma^{n + 1}V_t(s_{t + n + 1}) - \gamma^nV_t(s_{t + n}) + \gamma^nr_{t + n + 1}\right) \\
&amp;= \sum_{n = 0}^{+\infty} \lambda^n\left(\gamma^{n + 1}V_t(s_{t + n + 1}) - \gamma^nV_t(s_{t + n}) + \gamma^nr_{t + n + 1}\right) \\
&amp;= \sum_{n = 0}^{+\infty} \gamma^n\lambda^n\left(\gamma V_t(s_{t + n + 1}) - V_t(s_{t + n}) + r_{t + n + 1}\right) \\
&amp;\approx \sum_{n = 0}^{+\infty} \gamma^n\lambda^n\left(\gamma V_{t + n}(s_{t + n + 1}) - V_{t + n}(s_{t + n}) + r_{t + n + 1}\right) \\
&amp;= \sum_{n = 0}^{+\infty} (\gamma\lambda)^n\delta_{t + n}
\end{aligned}
\]</span></p>
<p>这就说明通过效用函数，我们最终得到的状态 <span class="math inline">\(s_k\)</span> 的价值函数估计为：</p>
<p><span class="math display">\[
V_{+\infty}^{\rm B}(s_k) = V_0(s_k) + \alpha(G_k^\lambda - V_k(s_k))
\]</span></p>
<p>而如果通过 Forward-view TD(<span class="math inline">\(\lambda\)</span>)，由于后续不在经历 <span class="math inline">\(s_k\)</span>，那么 <span class="math inline">\(s_k\)</span> 的价值函数只会在时刻 <span class="math inline">\(k\)</span> 发生一步更新，其余时刻并不更新。而在时刻 <span class="math inline">\(k\)</span>，TD target 为 <span class="math inline">\(G_k^\lambda\)</span>，即可得到最终的价值函数估计（价值函数上标 <span class="math inline">\({\rm F}\)</span> 表示前向视角）：</p>
<p><span class="math display">\[
V_{+\infty}^{\rm F}(s_k) = V_0(s_k) + \alpha(G_k^\lambda - V_k(s_k))
\]</span></p>
<p>从而证明完毕。事实上，如果去掉状态不重复的假设，也只是把每次经过该状态时的状态函数值变化拆开，每一部分都可以通过上面的过程证明两个视角的等价性，从而求和后依然是等价的。</p>
<div class="note info no-icon"><p>这里简单说明 RL 中<strong>在线（Online）</strong>和<strong>离线（Offline）</strong>的区别。在线指的是智能体通过策略收集环境信息和通过这些信息更新策略是同时的，而离线则指的是通过某个固定策略在环境中收集信息，然后离线通过这些数据更新策略。</p>
<p>在线算法的特征在于价值函数估计在每个时间步都会发生更新。而离线算法由于已经事先采集若干条轨迹，所以更新的粒度是整条轨迹，即完整考虑一条轨迹，每次完整读取完一条轨迹后做一次价值函数估计更新。</p>
<p>上述推理中使用了约等于，这仅限于在线场景，而对于离线场景，价值函数 <span class="math inline">\(V_t\)</span> 中下标 <span class="math inline">\(t\)</span> 即失效（不再每个时间步更新），此时约等于变为严格等于。这也就说明两个视角在离线条件下是严格等价，在在线条件下是大致等价。</p>
</div>
<h2 id="策略改进">策略改进</h2>
<p>策略改进的方式实际上不拘一格，这和策略评估相对手段有限形成对比的原因是，策略评估严格要求价值函数收敛到当前策略（采用异轨方法时是当前目标策略）的真实价值函数，而策略改进仅仅是要求贪心地生成，并没有严格要求生成方式。</p>
<p>常见的策略改进之一就是<strong>完全贪心策略（Greedy Policy）</strong>，最优 Bellman 方程 <span class="math inline">\(V^\star(s) = \max_{a \in \A} Q^\star(s, a)\)</span> 保证了完全贪心策略依然可以收敛到最优策略：</p>
<p><span class="math display">\[
\pi(a \mid s) := \begin{cases}
1 &amp; a = \argmax_{a&#39; \in \A} Q(s, a&#39;) \\
0 &amp; {\rm otherwise} \\
\end{cases}
\]</span></p>
<p>然而这种方法并不一定在实际上优越，这是因为一个策略往往需要同时考虑下面两种相对立的优化目标：</p>
<ul>
<li>（Exploration）能够充分探索未知的状态和行为，增加对环境的认识</li>
<li>（Exploitation）能够充分利用已经掌握的环境信息，获取尽可能高的累计收益</li>
</ul>
<p>Exploration 会要求行为策略更加激进而 Exploitation 会要求目标策略更加保守，如何平衡这两者以获取高收益也就是这一类 RL 方法的核心。</p>
<p>一种简单的方式是 <strong><span class="math inline">\(\varepsilon\)</span> 贪心策略（<span class="math inline">\(\varepsilon\)</span> Greedy Policy）</strong>，其表现为：</p>
<p><span class="math display">\[
\pi(a \mid s) := \begin{cases}
\varepsilon / |\A| + 1 - \varepsilon &amp; a = \argmax_{a&#39; \in \A} Q(s, a&#39;) \\
\varepsilon / |\A| &amp; {\rm otherwise} \\
\end{cases}
\]</span></p>
<p>当然，还会有类似 <strong>Boltzman 探索</strong>之类的并不常见的策略改进：</p>
<p><span class="math display">\[
\pi(a \mid s) := \frac{\exp(Q(s, a) / T)}{\sum_{a&#39; \in \A} \exp(Q(s, a&#39;) / T)}
\]</span></p>
<div class="note success no-icon"><p><strong>Definition 3.01 (GLIE)</strong> 假设第 <span class="math inline">\(k\)</span> 时刻策略改进得到策略 <span class="math inline">\(\pi_k\)</span>，状态行为价值函数 <span class="math inline">\(Q_k\)</span>，记经过 <span class="math inline">\(k\)</span> 时长后策略在状态 <span class="math inline">\(s\)</span> 处做出决策 <span class="math inline">\(a\)</span> 的次数为 <span class="math inline">\(N(s, a)\)</span>。我们称该策略改进是 <strong>Greedy in the Limit with Infinite Exploration (GLIE)</strong>，如果：</p>
<ul>
<li>充分长时间后所有状态决策对均探索无穷次</li>
</ul>
<p><span class="math display">\[
\lim_{k \to +\infty} N_k(s, a) = +\infty, \forall s \in \S, a \in \A
\]</span></p>
<ul>
<li>策略最终收敛到贪心策略</li>
</ul>
<p><span class="math display">\[
\lim_{k \to +\infty} \pi_k(a \mid s) = \b{1}\left(a = \argmax_{a&#39; \in \A} Q_k(s, a&#39;)\right), \forall s \in \S, a \in \A
\]</span></p>
</div>
<p>Boltzman 探索是 GLIE，而 <span class="math inline">\(\varepsilon\)</span> 贪心策略则可能不是 GLIE。</p>
<h1 id="时序差分控制">时序差分控制</h1>
<p>时序差分控制的一般流程如下所示：</p>
<ul>
<li>初始化目标策略 <span class="math inline">\(\pi\)</span> 以及价值函数 <span class="math inline">\(Q^\pi\)</span></li>
<li>循环执行下述步骤直到收敛
<ul>
<li>【策略改进】利用 <span class="math inline">\(Q^\pi\)</span> 生成<strong>行为策略</strong>并执行<strong>一次</strong>，采样得到<strong>一步状态转移</strong></li>
<li>【策略评估】根据行为策略得到的一步状态转移更新<strong>目标策略</strong> <span class="math inline">\(\pi\)</span> 的价值函数 <span class="math inline">\(Q^\pi\)</span></li>
</ul></li>
</ul>
<h2 id="同轨时序差分控制sarsa">同轨时序差分控制（SARSA）</h2>
<p>在同轨时序差分控制之中使用 <span class="math inline">\(\varepsilon\)</span> 贪心策略即可得到 SARSA 算法。若令时刻 <span class="math inline">\(t\)</span> 智能体位于状态 <span class="math inline">\(s_t\)</span>，做出决策 <span class="math inline">\(a_t\)</span> 转移到状态 <span class="math inline">\(s_{t + 1}\)</span> 并获得收益 <span class="math inline">\(r_{t + 1}\)</span>，如果其下一时刻做出决策 <span class="math inline">\(a_{t + 1}\)</span>，可以得到其策略评估阶段所采用的更新为：</p>
<p><span class="math display">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha[r_{t + 1} + \gamma Q(s_{t + 1}, a_{t + 1}) - Q(s_t, a_t)]
\]</span></p>
<h2 id="异轨时序差分控制q-learning">异轨时序差分控制（Q learning）</h2>
<p>在异轨时序差分控制中，令目标策略为完全贪心策略，行为策略为 <span class="math inline">\(\varepsilon\)</span> 贪心策略即可得到 Q learning 算法。若令时刻 <span class="math inline">\(t\)</span> 智能体位于状态 <span class="math inline">\(s_t\)</span>，做出决策 <span class="math inline">\(a_t\)</span> 转移到状态 <span class="math inline">\(s_{t + 1}\)</span> 并获得收益 <span class="math inline">\(r_{t + 1}\)</span>，可以得到其策略评估阶段所采用的更新为：</p>
<p><span class="math display">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha\left[r_{t + 1} + \gamma\max_{a \in \A} Q(s_{t + 1}, a) - Q(s_t, a_t)\right]
\]</span></p>
<h1 id="蒙特卡洛控制">蒙特卡洛控制</h1>
<p>蒙特卡洛控制的一般流程如下所示：</p>
<ul>
<li>初始化目标策略 <span class="math inline">\(\pi\)</span> 以及价值函数 <span class="math inline">\(Q^\pi\)</span></li>
<li>循环执行下述步骤直到收敛
<ul>
<li>【策略改进】利用 <span class="math inline">\(Q^\pi\)</span> 生成<strong>行为策略</strong>并执行<strong>至终止</strong>，采样得到<strong>一条完整轨迹</strong></li>
<li>【策略评估】根据行为策略得到的轨迹更新<strong>目标策略</strong> <span class="math inline">\(\pi\)</span> 的价值函数 <span class="math inline">\(Q^\pi\)</span></li>
</ul></li>
</ul>
<h2 id="同轨蒙特卡洛控制">同轨蒙特卡洛控制</h2>
<p>若在采样得到的轨迹 <span class="math inline">\(\tau\)</span> 中，时刻 <span class="math inline">\(t\)</span> 智能体位于状态 <span class="math inline">\(s_t\)</span>，做出决策 <span class="math inline">\(a_t\)</span> 转移到状态 <span class="math inline">\(s_{t + 1}\)</span> 并获得收益 <span class="math inline">\(r_{t + 1}\)</span>，可以得到其策略评估阶段所采用的更新为：</p>
<p><span class="math display">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha[G_t(\tau) - Q(s_t, a_t)]
\]</span></p>
<h2 id="异轨蒙特卡洛控制">异轨蒙特卡洛控制</h2>
<p>现在我们需要考虑异轨带来的一个重要问题，我们通过行为策略采样得到的数据直接更新目标策略的价值函数估计是否合理。在时序差分控制中我们并没有思考这一问题，仅仅是由于时序差分控制中，行为策略和目标策略的更新每步都在进行，两者可以认为相对接近，从而可以忽略这一问题，并不是说明时序差分控制中不存在这一问题。例如，Q learning 的更新公式中：</p>
<p><span class="math display">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha\left[r_{t + 1} + \gamma{\color{red} \max_{a \in \A} Q(s_{t + 1}, a)} - Q(s_t, a_t)\right]
\]</span></p>
<p>这里 <span class="math inline">\(s_{t + 1}\)</span> 是通过行为策略确定的，然而在更新的时候则直接使用其计算目标策略（完全贪心策略）价值函数的 TD target。</p>
<p>然而在蒙特卡洛控制中这一问题则较为明显，因为行为策略会直接采样得到一整条轨迹，其会导致目标策略价值函数的大批量更新。</p>
<p>该问题可以形式化定义为，已知行为策略 <span class="math inline">\(b\)</span> 与目标策略 <span class="math inline">\(\pi\)</span>，以及使用行为策略 <span class="math inline">\(b\)</span> 采样得到的轨迹 <span class="math inline">\(\tau\)</span>，我们应当如何拟合计算目标策略的价值函数 <span class="math inline">\(Q^\pi\)</span>。</p>
<p>此时一般会使用<strong>重要性采样</strong>方法。我们考虑随机变量 <span class="math inline">\(X\)</span> 以及其概率密度函数 <span class="math inline">\(f_X(x) := \P(X = x)\)</span>，随机变量 <span class="math inline">\(X\)</span> 的期望显然定义为：</p>
<p><span class="math display">\[
\Eop_{x \sim X}[x] := \int_{\mathbb R} xf_X(x) \d x
\]</span></p>
<p>并且对于任何实数集上的函数 <span class="math inline">\(\varphi\)</span> 都有：</p>
<p><span class="math display">\[
\Eop_{x \sim X}[\varphi(x)] := \int_{\mathbb R} \varphi(x)f_X(x) \d x
\]</span></p>
<p>为了估算 <span class="math inline">\(\E[\varphi(X)]\)</span>，我们可以使用采样并利用样本均值作为数学期望的估计值。例如我们采样获取了 <span class="math inline">\(X\)</span> 的 <span class="math inline">\(N\)</span> 个值 <span class="math inline">\(x_1, x_2, \cdots, x_N\)</span>，那么：</p>
<p><span class="math display">\[
\Eop_{x \sim X}[\varphi(x)] \approx \frac{1}{N}\sum_{k = 1}^N \varphi(x_k)
\]</span></p>
<p>然而如果分布 <span class="math inline">\(f_X\)</span> 并不容易采样，那么这一条路线就有可能出现问题。此时，如果我们有一个容易采样的分布 <span class="math inline">\(f_Y\)</span>，则我们考虑：</p>
<p><span class="math display">\[
\Eop_{x \sim X}[\varphi(x)] := \int_{\mathbb R} \varphi(x)f_X(x) \d x = \int_{\mathbb R} \varphi(x)\frac{f_X(x)}{f_Y(x)} f_Y(x) \d x = \Eop_{y \sim Y}\left[\frac{f_X(y)}{f_Y(y)}\varphi(y)\right]
\]</span></p>
<p>这样，我们不需要直接采样 <span class="math inline">\(f_X\)</span>，仅需要得知 <span class="math inline">\(f_X\)</span> 的表达，即可通过采样 <span class="math inline">\(f_Y\)</span> 得到样本 <span class="math inline">\(y_1, y_2, \cdots, y_M\)</span> 即可作出下述估算：</p>
<p><span class="math display">\[
\Eop_{x \sim X}[\varphi(x)] = \Eop_{y \sim Y}\left[\frac{f_X(y)}{f_Y(y)}\varphi(y)\right] \approx \frac{1}{M}\sum_{k = 1}^M \frac{f_X(y_k)}{f_Y(y_k)}\varphi(y_k)
\]</span></p>
<p>回到原先的问题，我们首先得到：</p>
<p><span class="math display">\[
\frac{\P_\pi(\tau)}{\P_b(\tau)} = \frac{\prod_{(s, a, s&#39;) \in \tau} \pi(a \mid s)\Pe(s&#39; \mid s, a)}{\prod_{(s, a, s&#39;) \in \tau} b(a \mid s)\Pe(s&#39; \mid s, a)} = \prod_{(s, a) \in \tau} \frac{\pi(a \mid s)}{b(a \mid s)}
\]</span></p>
<p>那么对于任意的 <span class="math inline">\(s_{\rm param} \in \mathcal{S}, a_{\rm param} \in \mathcal{A}\)</span>，我们作下述计算：</p>
<p><span class="math display">\[
\begin{aligned}
Q^\pi(s_{\rm param}, a_{\rm param}) &amp;= \Eop_{\tau \sim \pi}[G(\tau) \mid s_0 = s_{\rm param}, a_0 = a_{\rm param}] \\
&amp;= \Eop_{\tau \sim b}\left[\frac{\P_\pi(\tau)}{\P_b(\tau)}G(\tau) \middle| s_0 = s_{\rm param}, a_0 = a_{\rm param}\right] \\
&amp;= \Eop_{\tau \sim b}\left[G(\tau)\prod_{(s, a) \in \tau} \frac{\pi(a \mid s)}{b(a \mid s)} \middle| s_0 = s_{\rm param}, a_0 = a_{\rm param}\right] \\
\end{aligned}
\]</span></p>
<p>那么，若在利用行为策略 <span class="math inline">\(b\)</span> 采样得到的轨迹 <span class="math inline">\(\tau\)</span> 中，时刻 <span class="math inline">\(t\)</span> 智能体位于状态 <span class="math inline">\(s_t\)</span>，做出决策 <span class="math inline">\(a_t\)</span> 转移到状态 <span class="math inline">\(s_{t + 1}\)</span> 并获得收益 <span class="math inline">\(r_{t + 1}\)</span>，可以得到其策略评估阶段所采用的更新为：</p>
<p><span class="math display">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha\left[G_t(\tau)\prod_{k = t}^{+\infty} \frac{\pi(a_k \mid s_k)}{b(a_k \mid s_k)} - Q(s_t, a_t)\right]
\]</span></p>
<p>我们也很容易发现若 <span class="math inline">\(\pi = b\)</span>，则异轨蒙特卡洛控制退化为同轨蒙特卡洛控制。</p>
<h1 id="函数估计与深度学习">函数估计与深度学习</h1>
<p>我们可以注意到上述介绍的所有算法，如果直接运用的话可能面临一些困难，这是因为现实问题的状态空间和决策空间都是巨大的，我们很难如实记录每一次迭代时价值函数在每个状态、每个行为上的值，这会需要大量的存储空间。</p>
<p>解决这一问题的最常用方案就是构造函数估计。例如，我们需要拟合一个难以直接计算的函数 <span class="math inline">\(f\)</span>，我们可以使用一系列较小的参数 <span class="math inline">\(\b w\)</span> 构造一个相对容易计算的函数 <span class="math inline">\(f_{\b w}\)</span>，尝试通过某种方式调整参数 <span class="math inline">\(\b w\)</span> 以令 <span class="math inline">\(f_{\b w} \approx f\)</span>。这样就可以使用 <span class="math inline">\(f_{\b w}\)</span> 来代替 <span class="math inline">\(f\)</span> 使用。</p>
<p>函数估计的方式是多样的，可行的方式包括线性拟合、决策树、最近邻等。当然，目前最为普遍的方式是使用神经网络，而调整参数的方式一般是梯度下降算法。</p>
<p>假设我们现在需要使用神经网络学习策略 <span class="math inline">\(\pi\)</span> 的状态价值函数 <span class="math inline">\(V^\pi\)</span>，我们使用参数 <span class="math inline">\(\b w\)</span> 构建一个函数 <span class="math inline">\(V_{\b w}\)</span>。我们可以利用 MSE 来构建一个简单的衡量拟合效果的损失函数：</p>
<p><span class="math display">\[
\mathcal{J}(\b w) := \Eop_{\tau \sim \pi} \left[\frac{1}{|\tau|} \sum_{s \in \tau} (V^\pi(s) - V_{\b w}(s))^2\right]
\]</span></p>
<p>之后我们计算其梯度：</p>
<p><span class="math display">\[
\begin{aligned}
\nabla_{\b w}\mathcal{J}(\b w) &amp;= \nabla_{\b w}\Eop_{\tau \sim \pi} \left[\frac{1}{|\tau|} \sum_{s \in \tau} (V^\pi(s) - V_{\b w}(s))^2\right] \\
&amp;= \Eop_{\tau \sim \pi} \left[\frac{1}{|\tau|} \sum_{s \in \tau} \nabla_{\b w}(V^\pi(s) - V_{\b w}(s))^2\right] \\
&amp;= -2\Eop_{\tau \sim \pi} \left[\frac{1}{|\tau|} \sum_{s \in \tau} (V^\pi(s) - V_{\b w}(s))\nabla_{\b w}V_{\b w}(s)\right]
\end{aligned}
\]</span></p>
<p>设定学习率为 <span class="math inline">\(-1/2 \cdot \alpha\)</span>，即可得到参数 <span class="math inline">\(\b w\)</span> 的更新方式：</p>
<p><span class="math display">\[
\b w \leftarrow \b w - \frac{1}{2}\alpha\nabla_{\b w}\mathcal{J}(\b w) = \b w + \alpha\Eop_{\tau \sim \pi} \left[\frac{1}{|\tau|} \sum_{s \in \tau} ({\color{red} V^\pi(s)} - V_{\b w}(s))\nabla_{\b w}V_{\b w}(s)\right]
\]</span></p>
<p>这里注意上述策略中标红项，显然这里我们需要得到真实值，或者可靠的真实值估计才能更新参数。而由于强化学习非监督学习，这里不存在标签，所以只能通过采样或者估计的方式处理这一项。</p>
<p>如果采用 MC 方法，这一项就可以使用轨迹累计收益：</p>
<p><span class="math display">\[
\b w \leftarrow \b w + \alpha\Eop_{\tau \sim \pi} \left[\frac{1}{|\tau|} \sum_{s \in \tau} ({\color{red} G_t(\tau)} - V_{\b w}(s))\nabla_{\b w}V_{\b w}(s)\right]
\]</span></p>
<p>如果采用 TD 方法，这一项就可以使用 bootstrap 的 TD target：</p>
<p><span class="math display">\[
\b w \leftarrow \b w + \alpha\Eop_{\tau \sim \pi} \left[\frac{1}{|\tau|} \sum_{(s, r, s&#39;) \in \tau} ({\color{red} r + \gamma V_{\b w}(s&#39;)} - V_{\b w}(s))\nabla_{\b w}V_{\b w}(s)\right]
\]</span></p>
<p>当然如果使用 TD(<span class="math inline">\(\lambda\)</span>)，这一项就是相应的 TD target。</p>
<p>目前基于通过神经网络实现的函数估计，我们可以将 Q learning 转换为深度学习算法 DQN。</p>
<h2 id="deep-q-network-dqn">Deep Q Network (DQN)</h2>
<p>使用神经网络后，Q learning 的基本算法框架不变，依然与上一节中所示一致，仅需要将价值函数 <span class="math inline">\(Q^\pi\)</span> 替换为一个神经网络 <span class="math inline">\(f_{\b w}: \mathcal{S} \to \mathbb{R}^{|\mathcal{A}|}\)</span>。该神经网络结构的语义为，其接受当前的状态 <span class="math inline">\(s \in \mathcal{S}\)</span>，输出这个状态下所有可能的行为带来的价值函数估计，即 <span class="math inline">\(f_{\b w}(s) = [Q_{\b w}(s, a_1), Q_{\b w}(s, a_2), \cdots, Q_{\b w}(s, a_{|\mathcal{A}|})]\)</span>。</p>
<p>此外，基于上述神经网络设计，策略改进过程中利用价值函数生成行为策略的方式在代码上是显然的，只需要将神经网络输出的数组求最大值后直接应用 <span class="math inline">\(\varepsilon\)</span> 贪心方式即可，所以不需要另开辟内存存储策略本身。</p>
<p>而策略评估过程则为上一部分所介绍的梯度下降算法：</p>
<p><span class="math display">\[
\b w \leftarrow \b w + \alpha\Eop_{\tau \sim \pi} \left[\frac{1}{|\tau|} \sum_{(s, a) \in \tau} ({\color{red} Q^\pi(s, a)} - Q_{\b w}(s, a))\nabla_{\b w}Q_{\b w}(s, a)\right]
\]</span></p>
<p>故基本的 DQN 算法流程为：</p>
<ul>
<li>初始化估计价值函数的神经网络 <span class="math inline">\(f_{\b w}\)</span></li>
<li>循环执行下述步骤直到收敛
<ul>
<li>【策略改进】求取 <span class="math inline">\(a_{\rm max} = \argmax f_{\b w}(s)\)</span> 并利用 <span class="math inline">\(\varepsilon\)</span> 贪心方式生成行为策略决策执行</li>
<li>【策略评估】利用梯度更新神经网络参数</li>
</ul></li>
</ul>
<p>至于如何求解梯度，我们</p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/02/20/2023-spring-record/" rel="prev" title="2023 春季学期记录">
                  <i class="fa fa-chevron-left"></i> 2023 春季学期记录
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/04/05/note-of-adv-jp/" rel="next" title="《现代日语中高级语法教程》学习笔记">
                  《现代日语中高级语法教程》学习笔记 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments"><div id="twikoo-comments"></div></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ashitemaru</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">523k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">7:56</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://unpkg.com/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{&quot;enable&quot;:true,&quot;theme&quot;:&quot;default&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;unpkg.com&#x2F;mermaid@8.9.3&#x2F;dist&#x2F;mermaid.min.js&quot;,&quot;integrity&quot;:&quot;sha256-OyJHvRcZHaRR6Ig73ppxF4QXk8HzvfgTprRWkulCkfY&#x3D;&quot;}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;unpkg.com&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;,&quot;integrity&quot;:&quot;sha256-ncNI9OXOS5Ek4tzVYiOMmN&#x2F;KKCPZ6V0Cpv2P&#x2F;zHntiA&#x3D;&quot;}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="twikoo" type="application/json">{&quot;enable&quot;:true,&quot;visitor&quot;:true,&quot;envId&quot;:&quot;https:&#x2F;&#x2F;vercel-deploy-two.vercel.app&quot;,&quot;el&quot;:&quot;#twikoo-comments&quot;}</script>
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.twikoo.el)
    .then(() => NexT.utils.getScript(
      'https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js',
      { condition: window.twikoo }
    ))
    .then(() => {
      twikoo.init(CONFIG.twikoo);
    });
});
</script>

</body>
</html>
